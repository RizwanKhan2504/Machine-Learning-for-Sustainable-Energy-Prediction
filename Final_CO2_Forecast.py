# -*- coding: utf-8 -*-
"""Final CO2 Forecast.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CQe0Z-vDeQG34xlVcYY5B4tQ8mjCQUYj
"""

!ls drive/MyDrive/ML_on_cloud_assessment/energy_data.csv

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('drive/MyDrive/ML_on_cloud_assessment/energy_data.csv')

df.count()

df.shape

df.size

df.isna().sum()

df.info()

df.describe(include='all')

df.nunique()

df.duplicated()

df[df.duplicated()]

df.describe(include = 'object').T

df.isna().sum()

df.isna().sum().plot(kind = 'bar')

# Finding duplicate rows
duplicate_rows = df[df.duplicated(keep='first')]

# Number of duplicate rows
num_duplicates = duplicate_rows.shape[0]

# Displaying the duplicate rows
print(f"Number of duplicate rows: {num_duplicates}")
duplicate_rows

"""# I. **Exploratory Data Analysis**

# **1. Missing Data**
"""

df.isna().sum()*100/len(df)

sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis')

"""Financial aid to developing countries is filled with zero as it would make more sense than filling it with random number. It maybe that other countries didnt recieve any aid. Hence, filling it with zero is more logical.

OECD website also shows that not all countries receive the funds. Hence, imputing zero is logical.




https://read.oecd-ilibrary.org/development/geographical-distribution-of-financial-flows-to-developing-countries-2023_12757fab-en-fr#page207

Visualize missing values. Finding correlation bewtween missing values.
"""

df.isna().sum()*100/len(df)

import missingno as msno
msno.matrix(df)
msno.heatmap(df)
msno.dendrogram(df)

msno.bar(df)

df.drop(['Entity', 'Year', 'Latitude','Longitude'], axis=1).hist(figsize=(20,20))

"""# **Correlation Matrix**"""

correlation_matrix = df.corr()
mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5, mask=mask)
plt.title('Correlation Heatmap')
plt.show()

avg_primary_energy_consimp_by_country = df.groupby('Entity')['Primary energy consumption per capita (kWh/person)'].mean()
avg_primary_energy_consimp_by_country.head()

top5primaryenergyconsump=avg_primary_energy_consimp_by_country.nlargest(5)

plt.figure(figsize=(10, 6))
plt.bar(top5primaryenergyconsump.index, top5primaryenergyconsump, color='turquoise')
plt.xlabel('Country')
plt.ylabel('Average Primary Energy Consumption by Country')
plt.title('Top 5 Countries with Highest Average Primary Energy Consumption per capita')
plt.xticks(rotation=45, ha='right')

plt.gca().set_facecolor('white')
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Show the plot
plt.show()

"""# **2. Handling the missing values**

---

# i. Access to electricity (% of population)
"""

df['Access to electricity (% of population)'] = pd.to_numeric(df['Access to electricity (% of population)'], errors='coerce')

# Fill missing values with the median value for each country as Access to electricity (%of population) is skewed
df['Access to electricity (% of population)'] = df.groupby('Entity')['Access to electricity (% of population)'].transform(lambda x: x.fillna(x.median()))

df.isna().sum()*100/len(df)

df.to_csv(r"new3.csv")

"""# ii. Access to clean fuels for cooking"""

filtered_data = df[df['Access to clean fuels for cooking'].isnull()]

# Plot histogram for 'Access to electricity (% of population)'
plt.figure(figsize=(8, 6))
plt.hist(filtered_data['Access to electricity (% of population)'], bins=20, color='skyblue', edgecolor='black')
plt.title('Histogram of Access to electricity (% of population) where Access to clean fuels for cooking is null')
plt.xlabel('Access to electricity (% of population)')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

filtered_data = df.dropna(subset=['Access to electricity (% of population)', 'Access to clean fuels for cooking'])
filtered_data = df[(df['Access to electricity (% of population)'] >= 67) &
                     (df['Access to electricity (% of population)'] <= 100)]


# Calculate correlation
correlation = filtered_data['Access to clean fuels for cooking'].corr(filtered_data['Access to electricity (% of population)'])

# Create scatterplot
plt.figure(figsize=(8, 6))
plt.scatter(filtered_data['Access to clean fuels for cooking'], filtered_data['Access to electricity (% of population)'])
plt.title('Scatterplot: Access to clean fuels for cooking vs Access to electricity (% of population)')
plt.xlabel('Access to clean fuels for cooking')
plt.ylabel('Access to electricity (% of population)')
plt.grid(True)

df.loc[(df['Access to electricity (% of population)'] == 100) &
       (df['Access to clean fuels for cooking'].isnull()), 'Access to clean fuels for cooking'] = 100

from sklearn.linear_model import LinearRegression
filtered_data = df[(df['Access to electricity (% of population)'] >= 67) &
                     (df['Access to electricity (% of population)'] <= 100)]


# Separate data into known and missing values for 'Access to clean fuels for cooking'
known_values = filtered_data.dropna(subset=['Access to clean fuels for cooking'])
missing_values = filtered_data[filtered_data['Access to clean fuels for cooking'].isnull()]

# Prepare data for linear regression
X = known_values[['Access to electricity (% of population)']]
y = known_values['Access to clean fuels for cooking']

# Create and fit a linear regression model
model = LinearRegression()
model.fit(X, y)

# Impute missing values using the regression model
imputed_values = model.predict(missing_values[['Access to electricity (% of population)']])

# Fill missing values in the original DataFrame using .loc
df.loc[missing_values.index, 'Access to clean fuels for cooking'] = imputed_values

df.to_csv(r"new4.csv")

"""# **iii. Renewable-electricity-generating-capacity-per-capita**"""

from scipy import stats
plt.figure(figsize=(8, 6))
plt.hist(df['Renewable-electricity-generating-capacity-per-capita'], bins=20, edgecolor='black')
plt.title('Histogram of Renewable-electricity-generating-capacity-per-capita')
plt.xlabel('Renewable-electricity-generating-capacity-per-capita')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

# Calculate mean, median, and mode
mean_value = df['Renewable-electricity-generating-capacity-per-capita'].mean()
median_value = df['Renewable-electricity-generating-capacity-per-capita'].median()


print(f"Mean: {mean_value}")
print(f"Median: {median_value}")

plt.figure(figsize=(8, 6))
plt.hist(df['Renewable-electricity-generating-capacity-per-capita'], bins=20, edgecolor='black')
plt.title('Histogram of Renewable-electricity-generating-capacity-per-capita')
plt.xlabel('Renewable-electricity-generating-capacity-per-capita')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

# Calculate mean, median, and mode
mean_value = df['Renewable-electricity-generating-capacity-per-capita'].mean()
median_value = df['Renewable-electricity-generating-capacity-per-capita'].median()
min_value = df['Renewable-electricity-generating-capacity-per-capita'].min()


print(f"Mean: {mean_value}")
print(f"Median: {median_value}")
print(f"Min: {min_value}")

"""# **iv. Renewables (% equivalent primary energy)**"""

from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

# Create a new DataFrame with the column of interest
selected_data = df[['Renewables (% equivalent primary energy)']]

# Initialize the IterativeImputer with default settings (MICE method)
imputer = IterativeImputer(random_state=0)

# Fit and transform the data with the imputer
imputed_data = imputer.fit_transform(selected_data)

# Clip imputed values to ensure they are within the range of 0 to 85
imputed_data_clipped = np.clip(imputed_data, 0, 90)

# Replace the imputed values back into the original DataFrame
df['Renewables (% equivalent primary energy)'] = imputed_data_clipped

plt.figure(figsize=(8, 6))
plt.hist(df['Renewable-electricity-generating-capacity-per-capita'], bins=20, edgecolor='black')
plt.title('Histogram of Renewable-electricity-generating-capacity-per-capita')
plt.xlabel('Renewable-electricity-generating-capacity-per-capita')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

# Calculate mean, median, and mode
mean_value = df['Renewable-electricity-generating-capacity-per-capita'].mean()
median_value = df['Renewable-electricity-generating-capacity-per-capita'].median()
min_value = df['Renewable-electricity-generating-capacity-per-capita'].min()


print(f"Mean: {mean_value}")
print(f"Median: {median_value}")
print(f"Min: {min_value}")

df.to_csv(r"new5.csv")

"""# **v. Financial flows to developing countries (US $)**

As Financial flows to developing countries (US $) and Renewables (% equivalent primary energy) is missing in more than 50% of data, we cannot use KNN to impute the values. We use median for that particular country to fill the values.
"""

plt.figure(figsize=(8, 6))
plt.hist(df['Financial flows to developing countries (US $)'], bins=20, edgecolor='black')
plt.title('Histogram of Financial flows to developing countries (US $)')
plt.xlabel('Financial flows to developing countries (US $)')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

# Calculate mean, median, and mode
mean_value = df['Financial flows to developing countries (US $)'].mean()
median_value = df['Financial flows to developing countries (US $)'].median()


print(f"Mean: {mean_value}")
print(f"Median: {median_value}")

df['Financial flows to developing countries (US $)'] = pd.to_numeric(df['Financial flows to developing countries (US $)'], errors='coerce')

# Fill missing values with the median value for each country
df['Financial flows to developing countries (US $)'] = df.groupby('Entity')['Financial flows to developing countries (US $)'].transform(lambda x: x.fillna(x.median()))

plt.figure(figsize=(8, 6))
plt.hist(df['Financial flows to developing countries (US $)'], bins=20, edgecolor='black')
plt.title('Histogram of Financial flows to developing countries (US $)')
plt.xlabel('Financial flows to developing countries (US $)')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

# Calculate mean, median, and mode
mean_value = df['Financial flows to developing countries (US $)'].mean()
median_value = df['Financial flows to developing countries (US $)'].median()


print(f"Mean: {mean_value}")
print(f"Median: {median_value}")

df.to_csv(r"new6.csv")

df.isna().sum()*100/len(df)

entities_with_nulls = df.loc[df['Financial flows to developing countries (US $)'].isnull(), 'Entity'].unique()

# Display unique entities with null values in the specified column
print(entities_with_nulls)

"""https://www.oecd.org/dac/financing-sustainable-development/development-finance-standards/DAC-List-of-ODA-Recipients-for-reporting-2024-25-flows.pdf


shows that these countries dont receive Financial FLows. Hence we will make the rest fo the nulls for Financial flows to developing countries as zero.
"""

df.isna().sum()*100/len(df)

df['Financial flows to developing countries (US $)'].fillna(0, inplace=True)

df.isna().sum()*100/len(df)

"""# **vi. Renewable energy share in the total final energy consumption (%)**"""

null_renewable_energy = df[df['Renewable energy share in the total final energy consumption (%)'].isnull()]

# Display entities and years where the column is null
if not null_renewable_energy.empty:
    print("Entities and years where 'Renewable energy share in the total final energy consumption (%)' is null:")
    print(null_renewable_energy[['Entity', 'Year']])
else:
    print("No null values found for 'Renewable energy share in the total final energy consumption (%)'.")

"""Except Albania, wherr it is null value is in the last year. We can impute this as LOCE and replace it with previous value for 2020."""

df = df.sort_values('Year')

# Impute missing values using the last observed value
df['Renewable energy share in the total final energy consumption (%)'].fillna(method='ffill', inplace=True)

# Verify changes
print("Imputation completed:")
print(df[['Entity', 'Year', 'Renewable energy share in the total final energy consumption (%)']].head(10))

df.describe()

df.isna().sum()*100/len(df)

"""After this there will be missing value for French Guiana for Access to electricity (% of population). But there is only row for this Entity"""

df.to_csv(r'new2.csv')

df

"""# **vii. Electricity from fossil fuels (TWh)**

As all the rows for Tuvalu for 'Electricity from fossil fuels (TWh) is blank. Hence, we can fill the cells with zero.
"""

null_electricity_fossil_fuels = df[df['Electricity from fossil fuels (TWh)'].isnull()]

# Display rows where the column is null
if not null_electricity_fossil_fuels.empty:
    print("Rows where 'Electricity from fossil fuels (TWh)' is null:")
    print(null_electricity_fossil_fuels[['Entity', 'Year']])
else:
    print("No null values found for 'Electricity from fossil fuels (TWh)'.")

"""
As all the rows for Tuvalu for 'Electricity from fossil fuels (TWh) is blank. Hence, we can fill the cells with zero"""

df['Electricity from fossil fuels (TWh)'].fillna(0, inplace=True)

"""# **viii. Electricity from nuclear (TWh)**"""

missing_values_df = df[df['Electricity from nuclear (TWh)'].isnull()]

unique_entities = missing_values_df['Entity'].unique()

unique_entities

"""https://cnpp.iaea.org/public/ from International Atomic Energy Agency shows that none of these countries (Chile, Indonesia, Kazakhastan, Malaysia, Saudi Arabia and Tuvalu has nuclear power currently. Hence, we can fill the missing values by zero"""

df['Electricity from nuclear (TWh)'].fillna(0, inplace=True)

"""# **ix. Electricity from renewables (TWh)**"""

missing_values_df = df[df['Electricity from renewables (TWh)'].isnull()]

missing_values_df

"""As all the rows for Tuvalu for 'Electricity from renewables (TWh) is blank. Hence, we can fill the cells with zero."""

df['Electricity from renewables (TWh)'].fillna(0, inplace=True)

"""# **x. Low-carbon electricity (% electricity)**"""

missing_values_df = df[df['Low-carbon electricity (% electricity)'].isnull()]
missing_values_df

"""Only Bermuda and Tuvalu doesnt have data for Low-carbon electricity (% electricity). Thus, we can fill this missing value with zero"""

df['Low-carbon electricity (% electricity)'].fillna(0, inplace=True)

df.isna().sum()*100/len(df)

"""# **xi. Energy intensity level of primary energy (MJ/$2017 PPP GDP)**"""

plt.figure(figsize=(8, 6))
plt.hist(df['Energy intensity level of primary energy (MJ/$2017 PPP GDP)'], bins=20, edgecolor='black')
plt.title('Histogram of Energy intensity level of primary energy (MJ/$2017 PPP GDP)')
plt.xlabel('Energy intensity level of primary energy (MJ/$2017 PPP GDP)')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

# Calculate mean, median, and mode
mean_value = df['Energy intensity level of primary energy (MJ/$2017 PPP GDP)'].mean()
median_value = df['Energy intensity level of primary energy (MJ/$2017 PPP GDP)'].median()


print(f"Mean: {mean_value}")
print(f"Median: {median_value}")

"""As French Guiana has Latitue and Longitude NaN, it is an outlier. We can drop it."""

df = df.sort_values('Year')

# Impute missing values using the last observed value
df['Energy intensity level of primary energy (MJ/$2017 PPP GDP)'].fillna(method='ffill', inplace=True)

# Verify changes
print("Imputation completed:")
print(df[['Entity', 'Year', 'Energy intensity level of primary energy (MJ/$2017 PPP GDP)']].head(10))

df.describe()



plt.figure(figsize=(8, 6))
plt.hist(df['Energy intensity level of primary energy (MJ/$2017 PPP GDP)'], bins=20, edgecolor='black')
plt.title('Histogram of Energy intensity level of primary energy (MJ/$2017 PPP GDP)')
plt.xlabel('Energy intensity level of primary energy (MJ/$2017 PPP GDP)')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

# Calculate mean, median, and mode
mean_value = df['Energy intensity level of primary energy (MJ/$2017 PPP GDP)'].mean()
median_value = df['Energy intensity level of primary energy (MJ/$2017 PPP GDP)'].median()


print(f"Mean: {mean_value}")
print(f"Median: {median_value}")

df.isna().sum()*100/len(df)

"""# **xii. Value_co2_emissions_kt_by_country**"""

plt.figure(figsize=(8, 6))
plt.hist(df['Value_co2_emissions_kt_by_country'], bins=20, edgecolor='black')
plt.title('Histogram of Value_co2_emissions_kt_by_country')
plt.xlabel('Value_co2_emissions_kt_by_country')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

# Calculate mean, median, and mode
mean_value = df['Value_co2_emissions_kt_by_country'].mean()
median_value = df['Value_co2_emissions_kt_by_country'].median()


print(f"Mean: {mean_value}")
print(f"Median: {median_value}")

df.isna().sum()*100/len(df)

from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error
import pandas as pd


# Subset df containing the relevant columns
subset_df = df[['Value_co2_emissions_kt_by_country', 'Electricity from fossil fuels (TWh)',
                'Electricity from nuclear (TWh)', 'Electricity from renewables (TWh)']]

# Split df into known and missing values for 'Value_co2_emissions_kt_by_country'
known_values = subset_df.dropna(subset=['Value_co2_emissions_kt_by_country'])
missing_values = subset_df[subset_df['Value_co2_emissions_kt_by_country'].isnull()]

# Prepare df for linear regression
X_train = known_values.drop('Value_co2_emissions_kt_by_country', axis=1)
y_train = known_values['Value_co2_emissions_kt_by_country']

# Initialize and fit a Linear Regression model
linear_model = LinearRegression()
linear_model.fit(X_train, y_train)

# Predict missing values
X_missing = missing_values.drop('Value_co2_emissions_kt_by_country', axis=1)
imputed_values = linear_model.predict(X_missing)

# Fill missing values in the original DataFrame using .loc
df.loc[df['Value_co2_emissions_kt_by_country'].isnull(), 'Value_co2_emissions_kt_by_country'] = imputed_values

# Evaluate model fit
r2 = r2_score(y_train, linear_model.predict(X_train))
mse = mean_squared_error(y_train, linear_model.predict(X_train))

print(f"R-squared: {r2}")
print(f"Mean Squared Error (MSE): {mse}")

plt.figure(figsize=(8, 6))
plt.hist(df['Value_co2_emissions_kt_by_country'], bins=20, edgecolor='black')
plt.title('Histogram of Value_co2_emissions_kt_by_country')
plt.xlabel('Value_co2_emissions_kt_by_country')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

# Calculate mean, median, and mode
mean_value = df['Value_co2_emissions_kt_by_country'].mean()
median_value = df['Value_co2_emissions_kt_by_country'].median()


print(f"Mean: {mean_value}")
print(f"Median: {median_value}")

df.describe()

"""# **iii. Renewable-electricity-generating-capacity-per-capita(Cont.)**"""

df.isna().sum()*100/len(df)

from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error
df_copy=df
# Extract necessary columns for imputation
subset_df = df[['Renewable-electricity-generating-capacity-per-capita', 'Renewables (% equivalent primary energy)']]

# Filter missing and non-negative values for 'Renewable-electricity-generating-capacity-per-capita'
known_values = subset_df.dropna(subset=['Renewable-electricity-generating-capacity-per-capita'])
known_values = known_values[known_values['Renewable-electricity-generating-capacity-per-capita'] >= 0]

missing_values = subset_df[subset_df['Renewable-electricity-generating-capacity-per-capita'].isnull()]

# Prepare data for linear regression
X_train = known_values[['Renewables (% equivalent primary energy)']]
y_train = known_values['Renewable-electricity-generating-capacity-per-capita']

# Initialize and fit a Linear Regression model
linear_model = LinearRegression()
linear_model.fit(X_train, y_train)

# Predict missing values
X_missing = missing_values[['Renewables (% equivalent primary energy)']]
imputed_values = linear_model.predict(X_missing)

# Filter only positive values for imputation
imputed_values[imputed_values < 0] = 0

# Fill missing values in the original DataFrame


# Calculate R-squared and MSE
r2 = r2_score(y_train, linear_model.predict(X_train))
mse = mean_squared_error(y_train, linear_model.predict(X_train))

print(f"R-squared: {r2}")
print(f"Mean Squared Error (MSE): {mse}")

df['Renewable-electricity-generating-capacity-per-capita'].fillna(0, inplace=True)

df.isna().sum()*100/len(df)

plt.figure(figsize=(8, 6))
plt.hist(df['Renewable-electricity-generating-capacity-per-capita'], bins=20, edgecolor='black')
plt.title('Histogram of Renewable-electricity-generating-capacity-per-capita')
plt.xlabel('Renewable-electricity-generating-capacity-per-capita')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

# Calculate mean, median, and mode
mean_value = df['Renewable-electricity-generating-capacity-per-capita'].mean()
median_value = df['Renewable-electricity-generating-capacity-per-capita'].median()
min_value = df['Renewable-electricity-generating-capacity-per-capita'].min()


print(f"Mean: {mean_value}")
print(f"Median: {median_value}")
print(f"Min: {min_value}")

"""# **xiii. GDP Growth & GDP per Capita**"""

df.to_csv(r"new11.csv")

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))

# Histogram for 'gdp_per_capita'
axes[0].hist(df['gdp_per_capita'], bins=20, color='skyblue', edgecolor='black')
axes[0].set_title('Histogram of GDP per Capita')
axes[0].set_xlabel('GDP per Capita')
axes[0].set_ylabel('Frequency')

# Histogram for 'gdp_growth'
axes[1].hist(df['gdp_growth'], bins=20, color='salmon', edgecolor='black')
axes[1].set_title('Histogram of GDP Growth')
axes[1].set_xlabel('GDP Growth')
axes[1].set_ylabel('Frequency')

# Adjust layout
plt.tight_layout()
plt.show()

df

# Filter the DataFrame for 'gdp_per_capita' and 'gdp_growth' columns
selected_columns = ['gdp_per_capita', 'gdp_growth', 'Year']
selected_data = df[selected_columns]

# Plot the missingness matrix
msno.matrix(selected_data, sort='ascending', figsize=(8, 4))  # Adjust the figsize as needed

df.to_csv(r"new7.csv")

df2=df.copy()

df2

df_copy = df

column_indices_to_drop = [df.shape[1] - 4, df.shape[1] - 3]

# Drop the 4th last and 3rd last columns
df.drop(df.columns[column_indices_to_drop], axis=1, inplace=True)

df

df.isna().sum()*100/len(df)

df

df.isna().sum()*100/len(df)

data_copy1 = df.copy()

data_copy1

import pandas as pd
from sklearn.experimental import enable_hist_gradient_boosting  # noqa
from sklearn.ensemble import HistGradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer

# Assuming 'df' is your DataFrame with columns 'Entity', 'gdp_per_capita', and 'gdp_growth'
# Replace 'df' with your actual DataFrame name

# Make a copy of the original dataset
data_copy = df.drop('Entity', axis=1).copy()

# Handling missing values
data_copy.fillna(data_copy.mean(), inplace=True)

# Split dataset into features and target variables
X = data_copy.drop(['gdp_per_capita', 'gdp_growth'], axis=1)  # Features
y_per_capita = data_copy['gdp_per_capita']  # Target variable for gdp_per_capita
y_growth = data_copy['gdp_growth']  # Target variable for gdp_growth

# Split data into training and testing sets
X_train, X_test, y_per_capita_train, y_per_capita_test, y_growth_train, y_growth_test = train_test_split(
    X, y_per_capita, y_growth, test_size=0.2, random_state=42
)

# Initialize and train HistGradientBoostingRegressor models separately for each target variable
hgboost_per_capita_model = HistGradientBoostingRegressor()
hgboost_per_capita_model.fit(X_train, y_per_capita_train)

hgboost_growth_model = HistGradientBoostingRegressor()
hgboost_growth_model.fit(X_train, y_growth_train)

# Predict using the trained models
y_per_capita_pred = hgboost_per_capita_model.predict(X_test)
y_growth_pred = hgboost_growth_model.predict(X_test)

# Calculate Mean Squared Error (MSE) and R-squared for each target variable
mse_per_capita = mean_squared_error(y_per_capita_test, y_per_capita_pred)
r2_per_capita = r2_score(y_per_capita_test, y_per_capita_pred)

mse_growth = mean_squared_error(y_growth_test, y_growth_pred)
r2_growth = r2_score(y_growth_test, y_growth_pred)

# Display MSE and R-squared values for each target variable
print("Metrics for 'gdp_per_capita':")
print(f"Mean Squared Error (MSE): {mse_per_capita}")
print(f"R-squared: {r2_per_capita}")
print("\nMetrics for 'gdp_growth':")
print(f"Mean Squared Error (MSE): {mse_growth}")
print(f"R-squared: {r2_growth}")

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))

# Histogram for 'gdp_per_capita'
axes[0].hist(df['gdp_per_capita'], bins=20, color='skyblue', edgecolor='black')
axes[0].set_title('Histogram of GDP per Capita')
axes[0].set_xlabel('GDP per Capita')
axes[0].set_ylabel('Frequency')

# Histogram for 'gdp_growth'
axes[1].hist(df['gdp_growth'], bins=20, color='salmon', edgecolor='black')
axes[1].set_title('Histogram of GDP Growth')
axes[1].set_xlabel('GDP Growth')
axes[1].set_ylabel('Frequency')

# Adjust layout
plt.tight_layout()
plt.show()

missing_gdp_per_capita = df[df['gdp_per_capita'].isnull()].drop(['gdp_per_capita', 'gdp_growth', 'Entity'], axis=1)
imputed_gdp_per_capita = hgboost_per_capita_model.predict(missing_gdp_per_capita)

# Overwrite missing values in the original DataFrame 'df'
df.loc[df['gdp_per_capita'].isnull(), 'gdp_per_capita'] = imputed_gdp_per_capita

df.isna().sum()*100/len(df)

df

missing_gdp_per_capita

imputed_gdp_per_capita

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))

# Histogram for 'gdp_per_capita'
axes[0].hist(df['gdp_per_capita'], bins=20, color='skyblue', edgecolor='black')
axes[0].set_title('Histogram of GDP per Capita')
axes[0].set_xlabel('GDP per Capita')
axes[0].set_ylabel('Frequency')

# Histogram for 'gdp_growth'
axes[1].hist(df['gdp_growth'], bins=20, color='salmon', edgecolor='black')
axes[1].set_title('Histogram of GDP Growth')
axes[1].set_xlabel('GDP Growth')
axes[1].set_ylabel('Frequency')

# Adjust layout
plt.tight_layout()
plt.show()

df.isna().sum()*100/len(df)

y_per_capita

missing_indices = df[df['gdp_per_capita'].isna()].index

# Check the lengths of indices and predicted values
print(len(missing_indices), len(y_per_capita_pred))

df.isna().sum()*100/len(df)

from sklearn.impute import KNNImputer
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split

# Assuming 'df' is your DataFrame with columns 'gdp_growth' and other features
# Replace 'df' with your actual DataFrame name

# Make a copy of the original dataset
data_copy = df.copy()
data_copy = df.drop('Entity', axis=1).copy()
# Handling missing values using KNN imputer
imputer = KNNImputer(n_neighbors=5)
data_imputed = imputer.fit_transform(data_copy)

# Convert the imputed data back to a DataFrame
data_imputed = pd.DataFrame(data_imputed, columns=data_copy.columns)

# Split dataset into features and target variables
X = data_imputed.drop('gdp_growth', axis=1)
y = data_imputed['gdp_growth']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the KNN model
knn_model = KNeighborsRegressor(n_neighbors=5)
knn_model.fit(X_train, y_train)

# Predict 'gdp_growth' using the trained model
y_pred = knn_model.predict(X_test)

# Calculate Mean Squared Error (MSE) and R-squared
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Display MSE and R-squared
print(f"Mean Squared Error (MSE): {mse}")
print(f"R-squared: {r2}")

plt.figure(figsize=(8, 6))
plt.scatter(df['gdp_per_capita'], df['gdp_growth'], alpha=0.5)
plt.title('Scatterplot: GDP per Capita vs GDP Growth')
plt.xlabel('GDP per Capita')
plt.ylabel('GDP Growth')
plt.grid(True)
plt.show()

df.isna().sum()*100/len(df)

df

df.columns

df.info()

mean_growth = df['gdp_growth'].mean()
median_growth = df['gdp_growth'].median()
mode_growth = df['gdp_growth'].mode().values[0]

print(f"Mean: {mean_growth}")
print(f"Median: {median_growth}")
print(f"Mode: {mode_growth}")

# Create a histogram for 'gdp_growth'
plt.figure(figsize=(8, 6))
plt.hist(df['gdp_growth'].dropna(), bins=20, color='skyblue', edgecolor='black')
plt.xlabel('GDP Growth Rate')
plt.ylabel('Frequency')
plt.title('Histogram of GDP Growth Rate')
plt.grid(True)
plt.show()

df.isna().sum()*100/len(df)

# Fill missing values in 'gdp_growth' with zeros
df['gdp_growth'].fillna(0, inplace=True)

df.isna().sum()*100/len(df)

mean_growth = df['gdp_growth'].mean()
median_growth = df['gdp_growth'].median()
mode_growth = df['gdp_growth'].mode().values[0]

print(f"Mean: {mean_growth}")
print(f"Median: {median_growth}")
print(f"Mode: {mode_growth}")

# Create a histogram for 'gdp_growth'
plt.figure(figsize=(8, 6))
plt.hist(df['gdp_growth'].dropna(), bins=20, color='skyblue', edgecolor='black')
plt.xlabel('GDP Growth Rate')
plt.ylabel('Frequency')
plt.title('Histogram of GDP Growth Rate')
plt.grid(True)
plt.show()

df.isna().sum()*100/len(df)

df.isna().sum()*100/len(df)

df.describe()

rows_with_null_latitude = df[df['Latitude'].isnull()]
print(rows_with_null_latitude)

df_missing_clean=df.dropna(subset=['Latitude'], inplace=True)

df.to_csv(r"new9.csv")

df.isna().sum()*100/len(df)

"""# **3. Outliers**"""

numerical_cols = df.select_dtypes(include='number').columns.tolist()

# Plot boxplots for numerical columns
for col in numerical_cols:
    plt.figure(figsize=(8, 6))
    sns.boxplot(x=df[col])
    plt.title(f'Boxplot of {col}')
    plt.show()

df.describe()

"""Access to electricity (% of population)	and Access to clean fuels for cooking is the % of population that has access to these source. Hence this can range from 0 to 100. Thus, no outliers in this columns

Renewable-electricity-generating-capacity-per-capita has values from 0 to 3,060. No outliers as these are accepted values.

Financial flows to developing countries (US $) has values from 5202310000. Again accepted range. No outliers

Renewable energy share in the total final energy consumption (%) has values from 0 to 96.0%. As this is a % share, hence it can range from 0 to 100%. Thus, no outliers in this columns
"""

import matplotlib.pyplot as plt

# Assuming 'df' is your DataFrame containing the columns 'Entity' and 'Electricity from fossil fuels (TWh)'
# Replace 'df' with your actual DataFrame name

# Filter data where 'Electricity from fossil fuels (TWh)' > 500
filtered_data = df[df['Electricity from fossil fuels (TWh)'] > 500]

# Create a boxplot
plt.figure(figsize=(8, 6))
boxplot = filtered_data.boxplot(column='Electricity from fossil fuels (TWh)')

# Annotate Entity names
for i, entity in enumerate(filtered_data['Entity']):
    if filtered_data.iloc[i]['Electricity from fossil fuels (TWh)'] > 500:
        plt.text(1, filtered_data.iloc[i]['Electricity from fossil fuels (TWh)'], entity, horizontalalignment='left', size='small', color='black')

plt.title('Boxplot of Electricity from fossil fuels (TWh) > 500 with Country names')
plt.ylabel('Electricity from fossil fuels (TWh)')
plt.show()

import matplotlib.pyplot as plt

# Assuming 'df' is your DataFrame containing the columns 'Entity' and 'Electricity from fossil fuels (TWh)'
# Replace 'df' with your actual DataFrame name

# Filter data where 'Electricity from fossil fuels (TWh)' > 500
filtered_data = df[df['Electricity from fossil fuels (TWh)'] > 500]

# Create a boxplot
plt.figure(figsize=(8, 6))
boxplot = filtered_data.boxplot(column='Electricity from fossil fuels (TWh)')

# Get unique Entity names for annotation
unique_entities = filtered_data['Entity'].unique()

# Annotate Entity names
for i, entity in enumerate(unique_entities):
    data = filtered_data[filtered_data['Entity'] == entity]
    plt.text(1, data['Electricity from fossil fuels (TWh)'].max(), entity, horizontalalignment='left', size='small', color='black')

plt.title('Boxplot of Electricity from fossil fuels (TWh) > 500 with Entity names')
plt.ylabel('Electricity from fossil fuels (TWh)')
plt.show()

"""As per https://edgar.jrc.ec.europa.eu/report_2022 China, US, India and Japan have large CO2 emissions. Hence, these are not outliers."""

import matplotlib.pyplot as plt

# Assuming 'df' is your DataFrame containing columns 'Entity', 'Year', and 'Electricity from nuclear (TWh)'
# Replace 'df' with your actual DataFrame name

# Filter data for entities where 'Electricity from nuclear (TWh)' is greater than 50
filtered_data = df[df['Electricity from nuclear (TWh)'] > 50]

# Selecting data for Electricity from nuclear (TWh)
nuclear_data = filtered_data.pivot(index='Year', columns='Entity', values='Electricity from nuclear (TWh)')

# Plotting the lines for different entities
plt.figure(figsize=(10, 6))
for entity in nuclear_data.columns:
    plt.plot(nuclear_data.index, nuclear_data[entity], label=entity)

plt.xlabel('Year')
plt.ylabel('Electricity from nuclear (TWh)')
plt.title('Electricity from nuclear (TWh) over the years for entities with >50 TWh')
plt.legend(loc='upper left', bbox_to_anchor=(1, 1))
plt.grid(True)
plt.show()

"""Thus for these countires, there is consistent values for Electricity from Nuclear. Hence, none of these values are outliers."""

import matplotlib.pyplot as plt

# Assuming 'df' is your DataFrame containing columns 'Entity', 'Year', and 'Electricity from renewables (TWh)'
# Replace 'df' with your actual DataFrame name

# Filter data for entities where 'Electricity from renewables (TWh)' is greater than 50
filtered_data = df[df['Electricity from renewables (TWh)'] > 50]

# Selecting data for Electricity from renewables (TWh)
nuclear_data = filtered_data.pivot(index='Year', columns='Entity', values='Electricity from renewables (TWh)')

# Plotting the lines for different entities
plt.figure(figsize=(10, 6))
for entity in nuclear_data.columns:
    plt.plot(nuclear_data.index, nuclear_data[entity], label=entity)

plt.xlabel('Year')
plt.ylabel('Electricity from renewables ')
plt.title('Electricity from renewables  over the years for entities with >50 TWh')
plt.legend(loc='upper left', bbox_to_anchor=(1, 1))
plt.grid(True)
plt.show()

"""Thus for these countires, there is consistent values for Electricity from Renewables. Hence, none of these values are outliers.

Low-carbon electricity (% electricity) can range from 0 to 100. Hence, no outliers in this column
"""

import matplotlib.pyplot as plt

# Assuming 'df' is your DataFrame containing columns 'Entity', 'Year', and 'Primary energy consumption per capita (kWh/person)'
# Replace 'df' with your actual DataFrame name

# Filter data for entities where 'Primary energy consumption per capita (kWh/person)' is greater than 50
filtered_data = df[df['Primary energy consumption per capita (kWh/person)'] > 60000]

# Selecting data for Primary energy consumption per capita (kWh/person)
nuclear_data = filtered_data.pivot(index='Year', columns='Entity', values='Primary energy consumption per capita (kWh/person)')

# Plotting the lines for different entities
plt.figure(figsize=(10, 6))
for entity in nuclear_data.columns:
    plt.plot(nuclear_data.index, nuclear_data[entity], label=entity)

plt.xlabel('Year')
plt.ylabel('Primary energy consumption per capita (kWh/person)')
plt.title('Primary energy consumption per capita (kWh/person) over the years for entities with >60000 ')
plt.legend(loc='upper left', bbox_to_anchor=(1, 1))
plt.grid(True)
plt.show()

"""Thus for these countires, there is consistent values for Primary energy consumption per capita (kWh/person) greater than 60000. Hence, none of these values are outliers"""

import matplotlib.pyplot as plt

# Assuming 'df' is your DataFrame containing columns 'Entity', 'Year', and 'Energy intensity level of primary energy (MJ/$2017 PPP GDP)'
# Replace 'df' with your actual DataFrame name

# Filter data for entities where 'Energy intensity level of primary energy (MJ/$2017 PPP GDP)' is greater than 50
filtered_data = df[df['Energy intensity level of primary energy (MJ/$2017 PPP GDP)'] > 10]

# Selecting data for Energy intensity level of primary energy (MJ/$2017 PPP GDP)
nuclear_data = filtered_data.pivot(index='Year', columns='Entity', values='Energy intensity level of primary energy (MJ/$2017 PPP GDP)')

# Plotting the lines for different entities
plt.figure(figsize=(10, 6))
for entity in nuclear_data.columns:
    plt.plot(nuclear_data.index, nuclear_data[entity], label=entity)

plt.xlabel('Year')
plt.ylabel('Energy intensity level of primary energy (MJ/$2017 PPP GDP)')
plt.title('Energy intensity level of primary energy (MJ/$2017 PPP GDP) over the years for entities with >10')
plt.legend(loc='upper left', bbox_to_anchor=(1, 1))
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt

# Assuming 'df' is your DataFrame containing columns 'Entity', 'Year', and 'Value_co2_emissions_kt_by_country'
# Replace 'df' with your actual DataFrame name

# Filter data for entities where 'Value_co2_emissions_kt_by_country' is greater than 50
filtered_data = df[df['Value_co2_emissions_kt_by_country'] > 5.919250e+04	]

# Selecting data for Value_co2_emissions_kt_by_country
nuclear_data = filtered_data.pivot(index='Year', columns='Entity', values='Value_co2_emissions_kt_by_country')

# Plotting the lines for different entities
plt.figure(figsize=(10, 6))
for entity in nuclear_data.columns:
    plt.plot(nuclear_data.index, nuclear_data[entity], label=entity)

plt.xlabel('Year')
plt.ylabel('Value_co2_emissions_kt_by_country')
plt.title('Value_co2_emissions_kt_by_country over the years for entities with >5.919250e+04	')
plt.legend(loc='upper left', bbox_to_anchor=(1, 1))
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt

# Assuming 'df' is your DataFrame containing columns 'Entity', 'Year', and 'Renewables (% equivalent primary energy)'
# Replace 'df' with your actual DataFrame name

# Filter data for entities where 'Renewables (% equivalent primary energy)' is greater than 50
filtered_data = df[df['Renewables (% equivalent primary energy)'] > 50]

# Selecting data for Renewables (% equivalent primary energy)
nuclear_data = filtered_data.pivot(index='Year', columns='Entity', values='Renewables (% equivalent primary energy)')

# Plotting the lines for different entities
plt.figure(figsize=(10, 6))
for entity in nuclear_data.columns:
    plt.plot(nuclear_data.index, nuclear_data[entity], label=entity)

plt.xlabel('Year')
plt.ylabel('Renewables (% equivalent primary energy)')
plt.title('Renewables (% equivalent primary energy) over the years for entities with > 50	')
plt.legend(loc='upper left', bbox_to_anchor=(1, 1))
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt

# Filter the DataFrame for entities with GDP growth less than -15 and years between 2010 and 2020
filtered_df = df[(df['gdp_growth'] < -15) & (df['Year'].between(2010, 2020))]

# Create the plot
plt.figure(figsize=(10, 6))  # Adjust figure size if needed

# Group the filtered data by 'Entity' and iterate through the groups
for entity, data in filtered_df.groupby('Entity'):
    plt.plot(data['Year'], data['gdp_growth'], label=entity, marker='o')  # Add marker parameter

# Set plot labels and title
plt.xlabel('Year')
plt.ylabel('GDP Growth')
plt.title('GDP Growth Over Years (2010-2020) for Entities with GDP Growth < -15')
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))  # Place legend to the right
plt.grid(True)

# Set the axis limits
plt.xlim(2010, 2021)  # Set x-axis limits from 2010 to 2020
plt.ylim(-70, 20)     # Set y-axis limits from -70 to 20

# Show the plot
plt.show()

"""https://data.worldbank.org/indicator/NY.GDP.MKTP.KD.ZG  Shows the values for Libya and other countries are correct and thus these are not outliers"""

import matplotlib.pyplot as plt

# Assuming 'df' is your DataFrame containing columns 'Entity', 'Year', and 'gdp_per_capita'
# Replace 'df' with your actual DataFrame name

# Filter data for entities where 'gdp_per_capita' is greater than 50
filtered_data = df[df['gdp_per_capita'] > 40000]

# Selecting data for gdp_per_capita
nuclear_data = filtered_data.pivot(index='Year', columns='Entity', values='gdp_per_capita')

# Plotting the lines for different entities
plt.figure(figsize=(10, 6))
for entity in nuclear_data.columns:
    plt.plot(nuclear_data.index, nuclear_data[entity], label=entity)

plt.xlabel('Year')
plt.ylabel('gdp_per_capita')
plt.title('gdp_per_capita over the years for entities with > 40000')
plt.legend(loc='upper left', bbox_to_anchor=(1, 1))
plt.grid(True)
plt.show()

mean_growth = df['gdp_growth'].mean()
median_growth = df['gdp_growth'].median()
mode_growth = df['gdp_growth'].mode().values[0]

print(f"Mean: {mean_growth}")
print(f"Median: {median_growth}")
print(f"Mode: {mode_growth}")

# Create a histogram for 'gdp_growth'
plt.figure(figsize=(8, 6))
plt.hist(df['gdp_growth'].dropna(), bins=20, color='skyblue', edgecolor='black')
plt.xlabel('GDP Growth Rate')
plt.ylabel('Frequency')
plt.title('Histogram of GDP Growth Rate')
plt.grid(True)
plt.show()

df.isna().sum()*100/len(df)

# Summary statistics
summary_stats = df.describe()
summary_stats

"""# **Correlation Matrix**"""

# Calculate correlations
correlation_matrix = df.corr()

# Plot heatmap for correlation matrix
import seaborn as sns
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Heatmap')
plt.show()

# Calculate the correlation matrix
Corr_Matrix = df.corr()

print('Top 5 Most Positively Correlated to the Target Variable')
Corr_Matrix['Value_co2_emissions_kt_by_country'].sort_values(ascending=False)

corr_values = Corr_Matrix.values.tolist()
corr_columns = Corr_Matrix.columns.tolist()

import plotly.express as px
import plotly.graph_objects as go

fig = go.Figure(data = go.Heatmap(z = corr_values,
                                x = corr_columns,
                                y = corr_columns,
                                colorscale = 'RdBu_r',
                                zmin = -1,
                                zmax = 1,
                                colorbar_title = 'Legend'))

fig.update_layout(title = 'Correlation Heatmap', width = 1000, height = 1000)
fig.show()

"""# **Feature Selection**"""

# Calculate the correlation matrix
Corr_Matrix = df.corr()

print('Top Correlation to the Target Variable')
Corr_Matrix['Value_co2_emissions_kt_by_country'].sort_values(ascending=False)

top_correlations = Corr_Matrix['Value_co2_emissions_kt_by_country'].sort_values(ascending=False).drop(['Value_co2_emissions_kt_by_country','Year', 'Longitude', 'Latitude'])

plt.figure(figsize=(10, 6))
top_correlations.plot(kind='bar', color='skyblue')
plt.title('Top Correlations with the Target Variable')
plt.xlabel('Features')
plt.ylabel('Correlation Coefficient')
plt.show()

Target_variable = 'Value_co2_emissions_kt_by_country'

Features = ['Electricity from fossil fuels (TWh)','Electricity from renewables (TWh)','Electricity from nuclear (TWh)']

"""# **Splitting Dataset**"""

from sklearn.model_selection import train_test_split

X = df[Features]
y = df[Target_variable]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

len(X_train), len(X_test), len(y_train), len(y_test)

from sklearn.model_selection import TimeSeriesSplit

def custom_time_series_split(data, train_years=2, test_years=1):
    splits = []

    # Extract unique countries from the data
    unique_countries = df['Entity'].unique()

    for country in unique_countries:
        # Filter data for the current country
        country_data = df[df['Entity'] == country]

        # Perform TimeSeriesSplit for the current country
        tscv = TimeSeriesSplit(n_splits=(len(country_data) // (train_years + test_years)))

        for train_index, test_index in tscv.split(country_data):
            train_years_range = list(range(train_index[0], train_index[-1] + 1))
            test_years_range = list(range(test_index[0], test_index[-1] + 1))

            # Append indices for each split
            splits.append((country, train_years_range, test_years_range))

    return splits

# Example usage:
# Assuming 'df' is your DataFrame with 'Country' column and columns for each year
splits = custom_time_series_split(df)

# Display the first few splits
for split in splits[:5]:
    print(f"Country: {split[0]}, Train Years: {split[1]}, Test Years: {split[2]}")

from sklearn.model_selection import TimeSeriesSplit

def custom_time_series_split(data, train_years=2, test_years=1):
    splits = []

    # Extract unique countries from the data
    unique_countries = data['Entity'].unique()

    for country in unique_countries:
        # Filter data for the current country
        country_data = data[data['Entity'] == country]

        # Perform TimeSeriesSplit for the current country
        tscv = TimeSeriesSplit(n_splits=(len(country_data) // (train_years + test_years)))

        for train_index, test_index in tscv.split(country_data):
            train_years_range = country_data.iloc[train_index]['Year'].tolist()
            test_years_range = country_data.iloc[test_index]['Year'].tolist()

            # Append years for each split
            splits.append((country, train_years_range, test_years_range))

    return splits

# Example usage:
# Assuming 'df' is your DataFrame with 'Country' column and 'Year' column
splits = custom_time_series_split(df)

# Display the first few splits
for split in splits[:5]:
    print(f"Country: {split[0]}, Train Years: {split[1]}, Test Years: {split[2]}")

import matplotlib.pyplot as plt
import numpy as np

# Assuming 'df' is your DataFrame with 'Country' column and 'Year' column
splits = custom_time_series_split(df)

# Select the first five unique countries
top_countries = df['Entity'].unique()[:5]

# Create subplots for each country
fig, axs = plt.subplots(5, 1, figsize=(15, 20), sharex=True)

for ax, country in zip(axs, top_countries):
    country_splits = [split for split in splits if split[0] == country]

    # Plot each iteration for the current country
    for i, split in enumerate(country_splits):
        train_years_range = split[1]
        test_years_range = split[2]

        # Plot training years
        ax.plot(train_years_range, [i] * len(train_years_range), marker='o', linestyle='-', label=f'Train - Iteration {i + 1}')

        # Plot testing years
        ax.plot(test_years_range, [i] * len(test_years_range), marker='o', linestyle='--', label=f'Test - Iteration {i + 1}')

    # Set y-axis labels
    ax.set_ylabel(f'Iteration - {country}')

# Set plot labels and title
axs[-1].set_xlabel('Year')
plt.suptitle('Custom Time Series Splits (2000 to 2020) for First Five Countries - Iterations')

# Set x-axis ticks from 2000 to 2020
axs[-1].set_xticks(range(2000, 2021))

# Display legend
axs[0].legend()

# Show the plot
plt.show()

import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import TimeSeriesSplit

def custom_time_series_split_with_gap(data, train_years=2, test_years=1, gap=1):
    splits = []

    # Extract unique countries from the data
    unique_countries = data['Entity'].unique()

    for country in unique_countries:
        # Filter data for the current country
        country_data = data[data['Entity'] == country]

        # Perform TimeSeriesSplit for the current country
        tscv = TimeSeriesSplit(n_splits=(len(country_data) // (train_years + test_years)))

        for train_index, test_index in tscv.split(country_data):
            # Adjust test_index to introduce a gap
            adjusted_test_index = test_index + gap

            # Ensure adjusted test index is within the valid range
            adjusted_test_index = np.clip(adjusted_test_index, 0, len(country_data) - 1)

            train_years_range = country_data.iloc[train_index]['Year'].tolist()
            test_years_range = country_data.iloc[adjusted_test_index]['Year'].tolist()

            # Append years for each split
            splits.append((country, train_years_range, test_years_range))

    return splits

gap_value = 1  # Set the desired gap value
splits_with_gap = custom_time_series_split_with_gap(df, gap=gap_value)

# Display the first few splits
for split in splits_with_gap[:10]:
    print(f"Country: {split[0]}, Train Years: {split[1]}, Test Years: {split[2]}")

# Visualize the splits
# Select the first five unique countries
top_countries = df['Entity'].unique()[:5]

# Create subplots for each country
fig, axs = plt.subplots(5, 1, figsize=(15, 20), sharex=True)

for ax, country in zip(axs, top_countries):
    country_splits = [split for split in splits_with_gap if split[0] == country]

    # Plot each iteration for the current country
    for i, split in enumerate(country_splits):
        train_years_range = split[1]
        test_years_range = split[2]

        # Plot training years
        ax.plot(train_years_range, [i] * len(train_years_range), marker='o', linestyle='-', label=f'Train - Iteration {i + 1}')

        # Plot testing years
        ax.plot(test_years_range, [i] * len(test_years_range), marker='o', linestyle='--', label=f'Test - Iteration {i + 1}')

    # Set y-axis labels
    ax.set_ylabel(f'Iteration - {country}')

# Set plot labels and title
axs[-1].set_xlabel('Year')
plt.suptitle(f'Custom Time Series Splits with Gap ({gap_value} year) for First Five Countries - Iterations')

# Set x-axis ticks from 2000 to 2020
axs[-1].set_xticks(range(2000, 2021))

# Display legend
axs[0].legend()

# Show the plot
plt.show()

import sklearn
print(sklearn.__version__)

"""# **Scaling the data**"""

from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder

sc = StandardScaler()
X_train_sc = sc.fit_transform(X_train)
X_test_sc = sc.transform(X_test)
X_train_sc

import numpy as np
from sklearn.preprocessing import StandardScaler

# Convert y_train and y_test to NumPy arrays
y_train_array = np.array(y_train)
y_test_array = np.array(y_test)

# Reshape the arrays to 2D
y_train_reshaped = y_train_array.reshape(-1, 1)
y_test_reshaped = y_test_array.reshape(-1, 1)

# Create StandardScaler
sc = StandardScaler()

# Fit and transform the training data
y_train_sc = sc.fit_transform(y_train_reshaped)

# Transform the testing data using the same scaler
y_test_scaled = sc.transform(y_test_reshaped)
y_train_sc

from sklearn.preprocessing import MinMaxScaler

# Assuming X_train is your training data and X_test is your testing data

# Create MinMaxScaler
mms = MinMaxScaler()

# Fit and transform the training data
X_train_mms = mms.fit_transform(X_train)

# Transform the testing data using the same scaler
X_test_mms = mms.transform(X_test)
X_train_mms

import numpy as np
from sklearn.preprocessing import StandardScaler

# Convert y_train and y_test to NumPy arrays
y_train_array = np.array(y_train)
y_test_array = np.array(y_test)

# Reshape the arrays to 2D
y_train_reshaped = y_train_array.reshape(-1, 1)
y_test_reshaped = y_test_array.reshape(-1, 1)

# Create StandardScaler
mms = MinMaxScaler()

# Fit and transform the training data
y_train_mms = mms.fit_transform(y_train_reshaped)

# Transform the testing data using the same mms
y_test_mms = mms.transform(y_test_reshaped)
y_train_mms

from sklearn.pipeline import Pipeline

#Data Preprocessing Imports
from sklearn.preprocessing import StandardScaler

#Ensemble Models
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.ensemble import GradientBoostingRegressor

#Tree models
from sklearn.tree import DecisionTreeRegressor

# Metric Imports
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
from sklearn.metrics import make_scorer
from math import sqrt

# Model Selection imports
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score

#Linear Model imports
from sklearn.linear_model import SGDRegressor
from sklearn.linear_model import Ridge
from  sklearn.linear_model import Lasso

"""# **Model Selection & training**

## **1. Random Forest Regressor**
"""

# Perform TimeSeriesSplit using splits_with_gap
tscv = TimeSeriesSplit(n_splits=len(splits_with_gap))

# Creating a RandomForestRegressor with scaling
rf=RandomForestRegressor(n_estimators=100)
steps=[('sc',sc),('rf',rf)]
rfscale=Pipeline(steps)
rfscale.fit(X_train,y_train)

cv_scaled=cross_val_score(rfscale,X_train,y_train,scoring='neg_mean_squared_error',cv=4)
cv_scaled.mean()

# Define the pipeline
rfnew = RandomForestRegressor(random_state=2)
rfpipe = Pipeline(steps=[('sc', sc), ('rfnew', rfnew)])

# Define the parameter grid
params = {
    'rfnew__max_depth': list(range(2, 8)),
    'rfnew__n_estimators': list(range(50, 150))
}

# Define RMSE scorer
rmse_scorer = make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)), greater_is_better=False)

# Define R2 scorer
r2_scorer = make_scorer(r2_score)

# Create a RandomizedSearchCV object
rcv = RandomizedSearchCV(rfpipe, params, scoring={'rmse': rmse_scorer, 'r2': r2_scorer}, cv=4, refit='rmse')
rcv.fit(X_train, y_train)

# Print the best parameters
print("Best Parameters:", rcv.best_params_)

# Get the best model
best_model = rcv.best_estimator_

# Cross-validate with the best model
rmse_scores = cross_val_score(best_model, X_train, y_train, cv=4, scoring=rmse_scorer)
r2_scores = cross_val_score(best_model, X_train, y_train, cv=4, scoring=r2_scorer)

# Print the mean scores
print("Mean RMSE:", -rmse_scores.mean())
print("Mean R2 Score:", r2_scores.mean())

rf_final1=RandomForestRegressor(max_depth=7,n_estimators=1o1,random_state=2)
rf_final1.fit(X_train,y_train)

pred1=rf_final1.predict(X_test)
rf_mse=mean_squared_error(y_test,pred1)
rf_mse

rf_rsquared=r2_score(y_test,pred1)
rf_rsquared

"""### **Tuning RFR Using Random Search**"""

rfnew=RandomForestRegressor(random_state=2)
rfpipe=Pipeline(steps=[('sc',sc),('rfnew',rfnew)])
params=dict(rfnew__max_depth=list(range(2,8)),rfnew__n_estimators=list(range(50,150)))

rcv=RandomizedSearchCV(rfpipe,params,scoring='neg_mean_squared_error',cv=4)

rcv.fit(X_train,y_train)
rcv.best_estimator_.get_params()

rf_final1=RandomForestRegressor(max_depth=7,n_estimators=119,random_state=2)
rf_final1.fit(X_train,y_train)

pred1=rf_final1.predict(X_test)
rf_mse=mean_squared_error(y_test,pred1)
rf_mse

rf_rsquared=r2_score(y_test,pred1)
rf_rsquared

import matplotlib.pyplot as plt
import seaborn as sns
sns.set(style="whitegrid")

# Plotting true vs predicted values
def plot_predictions(true_values, pred_values, title):
    plt.figure(figsize=(10, 6))
    plt.scatter(true_values, pred_values, alpha=0.5)
    plt.plot([min(true_values), max(true_values)], [min(true_values), max(true_values)], '--', color='red', linewidth=2)
    plt.title(title)
    plt.xlabel('True Values')
    plt.ylabel('Predicted Values')
    plt.show()

# Plotting residuals
def plot_residuals(true_values, pred_values, title):
    residuals = true_values - pred_values
    plt.figure(figsize=(10, 6))
    plt.scatter(true_values, residuals, alpha=0.5)
    plt.axhline(y=0, color='red', linestyle='--', linewidth=2)
    plt.title(title)
    plt.xlabel('True Values')
    plt.ylabel('Residuals')
    plt.show()

# Plot feature importance for the final model with NumPy array features
def plot_feature_importance_numpy(model, feature_names):
    feature_importance = model.feature_importances_
    sorted_idx = np.argsort(feature_importance)
    plt.figure(figsize=(10, 6))
    plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')
    plt.yticks(range(len(sorted_idx)), [str(i) for i in sorted_idx])
    plt.xlabel('Feature Importance')
    plt.ylabel('Features')
    plt.title('Random Forest Feature Importance')
    plt.show()

# Plot predictions for the initial model
plot_predictions(y_test, rfscale.predict(X_test), title='Initial RandomForestRegressor Predictions')

# Plot predictions for the model with best hyperparameters from RandomizedSearchCV
plot_predictions(y_test, rf_final1.predict(X_test), title='RandomForestRegressor with Best Hyperparameters Predictions')

# Plot predictions for the cross-validated model from RandomizedSearchCV
plot_predictions(y_test, rcv.best_estimator_.predict(X_test), title='Cross-Validated RandomForestRegressor Predictions')

# Plot residuals for the cross-validated model
plot_residuals(y_test, rcv.best_estimator_.predict(X_test), title='Residuals for Cross-Validated RandomForestRegressor')

# Plot feature importance for the final model
plot_feature_importance_numpy(rf_final1, feature_names=np.arange(X.shape[1]))

# Plot feature importance for the final model with NumPy array features
def plot_feature_importance_numpy(model, feature_names):
    feature_importance = model.feature_importances_
    sorted_idx = np.argsort(feature_importance)
    plt.figure(figsize=(10, 6))
    plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')
    plt.yticks(range(len(sorted_idx)), [str(i) for i in sorted_idx])
    plt.xlabel('Feature Importance')
    plt.ylabel('Features')
    plt.title('Random Forest Feature Importance')
    plt.show()

# Plot feature importance for the final model
plot_feature_importance_numpy(rf_final1, feature_names=np.arange(X.shape[1]))

"""### **Tuning RFR Using Grid Search**"""

rfnew = RandomForestRegressor(random_state=2)
rfpipe = Pipeline(steps=[('sc', sc), ('rfnew', rfnew)])

# Define the grid of hyperparameters
param_grid = {
    'rfnew__max_depth': list(range(2, 8)),
    'rfnew__n_estimators': list(range(50, 150))
}

# Create the GridSearchCV object
gcv = GridSearchCV(rfpipe, param_grid, scoring='neg_mean_squared_error', cv=4)

# Fit the model
gcv.fit(X_train, y_train)

# Display the best hyperparameters
gcv.best_estimator_.get_params()

rf_final1=RandomForestRegressor(max_depth=5,n_estimators=60,random_state=2)
rf_final1.fit(X_train,y_train)

pred1=rf_final1.predict(X_test)
rf_mse=mean_squared_error(y_test,pred1)
rf_mse

rf_rsquared=r2_score(y_test,pred1)
rf_rsquared

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import cross_val_predict
from sklearn.pipeline import make_pipeline

# Function to create visuals
def visualize_model_performance(model, X_train, y_train, X_test, y_test, title):
    # Fit the model
    model.fit(X_train, y_train)

    # Make predictions
    y_pred = model.predict(X_test)

    # Visualize predictions vs true values
    plt.figure(figsize=(12, 6))
    plt.scatter(y_test, y_pred, alpha=0.5)
    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], '--', color='red', linewidth=2)
    plt.title(f'{title} - Predictions vs True Values')
    plt.xlabel('True Values')
    plt.ylabel('Predicted Values')
    plt.show()

    # Visualize residuals
    residuals = y_test - y_pred
    plt.figure(figsize=(12, 6))
    plt.scatter(y_test, residuals, alpha=0.5)
    plt.axhline(y=0, color='red', linestyle='--', linewidth=2)
    plt.title(f'{title} - Residuals')
    plt.xlabel('True Values')
    plt.ylabel('Residuals')
    plt.show()

    # Visualize feature importance
    if hasattr(model, 'feature_importances_'):
        feature_importance = model.feature_importances_
        sorted_idx = np.argsort(feature_importance)
        plt.figure(figsize=(12, 6))
        plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')
        plt.yticks(range(len(sorted_idx)), X_train.columns[sorted_idx])
        plt.xlabel('Feature Importance')
        plt.ylabel('Features')
        plt.title(f'{title} - Feature Importance')
        plt.show()

# RandomizedSearchCV RandomForestRegressor
rf_random = RandomForestRegressor(max_depth=6, n_estimators=109, random_state=2)
visualize_model_performance(rf_random, X_train, y_train, X_test, y_test, 'RandomizedSearchCV')

# GridSearchCV RandomForestRegressor
rf_grid = RandomForestRegressor(max_depth=7, n_estimators=76, random_state=2)
visualize_model_performance(rf_grid, X_train, y_train, X_test, y_test, 'GridSearchCV')

"""## **2. Gradient Boosting Regressor**"""

# Create and train the GradientBoostingRegressor
gbr = GradientBoostingRegressor(learning_rate=0.01, n_estimators=70)
model_gbr = gbr.fit(X_train, y_train)

# Make predictions on the test set
y_pred_gbr = model_gbr.predict(X_test)

# Calculate Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred_gbr)

# Calculate R-squared (R2)
r2 = r2_score(y_test, y_pred_gbr)

# Calculate Root Mean Squared Error (RMSE)
rmse = mse**(1/2)

# Print evaluation metrics
print("Training set score: {:.2f}".format(model_gbr.score(X_train, y_train)))
print("Test set score: {:.2f}".format(model_gbr.score(X_test, y_test)))
print("Mean Squared Error (MSE): {:.2f}".format(mse))
print("R-squared (R2): {:.2f}".format(r2))
print("Root Mean Squared Error (RMSE): {:.2f}".format(rmse))

gbr = GradientBoostingRegressor(learning_rate = 0.01, n_estimators = 70)
model_gbr =  gbr.fit(X_train, y_train)

y_pred_gbr = model_gbr.predict(X_test)
mse=mean_squared_error(y_test,y_pred_gbr)
rmns=mse**(1/2)

print("Training set score: {:.2f}".format(model_gbr.score(X_train, y_train)))
print("Test set score: {:.2f}".format(model_gbr.score(X_test, y_test)))

print("mean squared error: {:.2f}".format(mse))

"""### **Tuning GBR using random search**"""

gdab=GradientBoostingRegressor(random_state=69)
steps=[('sc',sc),('gdab',gdab)]
gdabpipe=Pipeline(steps=steps)
params=dict(gdab__n_estimators=list(range(20,100,10)),gdab__learning_rate=np.geomspace(0.001,1))

gdabcv=RandomizedSearchCV(gdabpipe,params,random_state=33)
gdabcv.fit(X_train,y_train)
gdabcv.best_estimator_.get_params()

gdab_final1=GradientBoostingRegressor(random_state=69,n_estimators=90,learning_rate=0.05963623316594643)
gdabpipe=Pipeline(steps=[('sc',sc),('gdab_final1',gdab_final1)])
gdabpipe.fit(X_train,y_train)

pred=gdabpipe.predict(X_test)
gdab_mse=mean_squared_error(y_test,pred)
gdab_rsquared=r2_score(y_test,pred)
print('r2: ',gdab_rsquared)
print('mse: ',gdab_mse)

from sklearn.model_selection import RandomizedSearchCV

# Gradient Boosting Regressor setup
gdab_final1 = GradientBoostingRegressor(random_state=69)

# Pipeline setup with scaler and Gradient Boosting Regressor
steps = [('sc', sc), ('gdab_final1', gdab_final1)]
gdabpipe = Pipeline(steps=steps)

# Parameter grid for Randomized Search
params_random = {
    'gdab_final1__n_estimators': list(range(20, 100, 10)),
    'gdab_final1__learning_rate': np.geomspace(0.001, 1)
}

# Randomized Search for R
random_search_r2 = RandomizedSearchCV(gdabpipe, params_random, scoring='r2', cv=4, n_iter=10, random_state=42)
random_search_r2.fit(X_train, y_train)

# Randomized Search for MSE
random_search_mse = RandomizedSearchCV(gdabpipe, params_random, scoring='neg_mean_squared_error', cv=4, n_iter=10, random_state=42)
random_search_mse.fit(X_train, y_train)

# Best parameters for R
best_params_r2 = random_search_r2.best_estimator_.get_params()
print('Best Parameters for R:', best_params_r2)

# Best parameters for MSE
best_params_mse = random_search_mse.best_estimator_.get_params()
print('Best Parameters for MSE:', best_params_mse)

# Final model using best parameters for R
gdab_final_r2 = GradientBoostingRegressor(random_state=69, n_estimators=best_params_r2['gdab_final1__n_estimators'],
                                          learning_rate=best_params_r2['gdab_final1__learning_rate'])
gdabpipe_r2 = Pipeline(steps=[('sc', sc), ('gdab_final_r2', gdab_final_r2)])
gdabpipe_r2.fit(X_train, y_train)
pred_r2 = gdabpipe_r2.predict(X_test)
gdab_mse_r2 = mean_squared_error(y_test, pred_r2)
gdab_rsquared_r2 = r2_score(y_test, pred_r2)
print('R (using best parameters for R):', gdab_rsquared_r2)
print('MSE (using best parameters for R):', gdab_mse_r2)

# Final model using best parameters for MSE
gdab_final_mse = GradientBoostingRegressor(random_state=69, n_estimators=best_params_mse['gdab_final1__n_estimators'],
                                           learning_rate=best_params_mse['gdab_final1__learning_rate'])
gdabpipe_mse = Pipeline(steps=[('sc', sc), ('gdab_final_mse', gdab_final_mse)])
gdabpipe_mse.fit(X_train, y_train)
pred_mse = gdabpipe_mse.predict(X_test)
gdab_mse_mse = mean_squared_error(y_test, pred_mse)
gdab_rsquared_mse = r2_score(y_test, pred_mse)
print('R (using best parameters for MSE):', gdab_rsquared_mse)
print('MSE (using best parameters for MSE):', gdab_mse_mse)

"""### **Tuning GBR using Grid Search**"""

# Gradient Boosting Regressor setup
gdab_final1 = GradientBoostingRegressor(random_state=69)

# Pipeline setup with scaler and Gradient Boosting Regressor
steps = [('sc', sc), ('gdab_final1', gdab_final1)]
gdabpipe = Pipeline(steps=steps)

# Parameter grid for Grid Search
params_grid = {
    'gdab_final1__n_estimators': list(range(20, 100, 10)),
    'gdab_final1__learning_rate': np.geomspace(0.001, 1)
}

# Grid Search for R
grid_search_r2 = GridSearchCV(gdabpipe, params_grid, scoring='r2', cv=4)
grid_search_r2.fit(X_train, y_train)

# Grid Search for MSE
grid_search_mse = GridSearchCV(gdabpipe, params_grid, scoring='neg_mean_squared_error', cv=4)
grid_search_mse.fit(X_train, y_train)

# Best parameters for R
best_params_r2 = grid_search_r2.best_estimator_.get_params()
print('Best Parameters for R:', best_params_r2)

# Best parameters for MSE
best_params_mse = grid_search_mse.best_estimator_.get_params()
print('Best Parameters for MSE:', best_params_mse)

# Final model using best parameters for R
gdab_final_r2 = GradientBoostingRegressor(random_state=69, n_estimators=best_params_r2['gdab_final1__n_estimators'],
                                          learning_rate=best_params_r2['gdab_final1__learning_rate'])
gdabpipe_r2 = Pipeline(steps=[('sc', sc), ('gdab_final_r2', gdab_final_r2)])
gdabpipe_r2.fit(X_train, y_train)
pred_r2 = gdabpipe_r2.predict(X_test)
gdab_mse_r2 = mean_squared_error(y_test, pred_r2)
gdab_rsquared_r2 = r2_score(y_test, pred_r2)
print('R (using best parameters for R):', gdab_rsquared_r2)
print('MSE (using best parameters for R):', gdab_mse_r2)

# Final model using best parameters for MSE
gdab_final_mse = GradientBoostingRegressor(random_state=69, n_estimators=best_params_mse['gdab_final1__n_estimators'],
                                           learning_rate=best_params_mse['gdab_final1__learning_rate'])
gdabpipe_mse = Pipeline(steps=[('sc', sc), ('gdab_final_mse', gdab_final_mse)])
gdabpipe_mse.fit(X_train, y_train)
pred_mse = gdabpipe_mse.predict(X_test)
gdab_mse_mse = mean_squared_error(y_test, pred_mse)
gdab_rsquared_mse = r2_score(y_test, pred_mse)
print('R (using best parameters for MSE):', gdab_rsquared_mse)
print('MSE (using best parameters for MSE):', gdab_mse_mse)

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import cross_val_predict

# Function to create visuals
def visualize_model_performance(model, X_train, y_train, X_test, y_test, title):
    # Fit the model
    model.fit(X_train, y_train)

    # Make predictions
    y_pred = model.predict(X_test)

    # Visualize predictions vs true values
    plt.figure(figsize=(12, 6))
    plt.scatter(y_test, y_pred, alpha=0.5)
    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], '--', color='red', linewidth=2)
    plt.title(f'{title} - Predictions vs True Values')
    plt.xlabel('True Values')
    plt.ylabel('Predicted Values')
    plt.show()

    # Visualize residuals
    residuals = y_test - y_pred
    plt.figure(figsize=(12, 6))
    plt.scatter(y_test, residuals, alpha=0.5)
    plt.axhline(y=0, color='red', linestyle='--', linewidth=2)
    plt.title(f'{title} - Residuals')
    plt.xlabel('True Values')
    plt.ylabel('Residuals')
    plt.show()

# RandomizedSearchCV Gradient Boosting Regressor
gdab_random = GradientBoostingRegressor(random_state=69, n_estimators=90, learning_rate=0.655)
gdabpipe_random = make_pipeline(sc, gdab_random)
visualize_model_performance(gdabpipe_random, X_train, y_train, X_test, y_test, 'RandomizedSearchCV')

# GridSearchCV Gradient Boosting Regressor
gdab_grid = GradientBoostingRegressor(random_state=69, n_estimators=90, learning_rate=0.655)
gdabpipe_grid = make_pipeline(sc, gdab_grid)
visualize_model_performance(gdabpipe_grid, X_train, y_train, X_test, y_test, 'GridSearchCV')

"""## **3. XGBoost**"""

from xgboost import XGBRegressor
from sklearn.preprocessing import StandardScaler

# XGBoost Regressor setup
xgb_final = XGBRegressor(n_estimators=70, learning_rate=0.01, random_state=69)

# Pipeline setup with scaler and XGBoost Regressor
sc = StandardScaler()
steps = [('sc', sc), ('xgb_final', xgb_final)]
xgbpipe = Pipeline(steps=steps)

# Train the XGBoost model
xgbpipe.fit(X_train, y_train)

# Make predictions on the test set
y_pred_xgb = xgbpipe.predict(X_test)

# Calculate Mean Squared Error (MSE)
mse_xgb = mean_squared_error(y_test, y_pred_xgb)

# Calculate R-squared (R2)
r2_xgb = r2_score(y_test, y_pred_xgb)

# Print evaluation metrics
print("XGBoost Model:")
print("Mean Squared Error (MSE): {:.2f}".format(mse_xgb))
print("R-squared (R2): {:.2f}".format(r2_xgb))

"""### **XGBoost using Random Search**"""

from xgboost import XGBRegressor
from sklearn.model_selection import RandomizedSearchCV

# XGBoost Regressor setup
xgb_final = XGBRegressor(random_state=69)

# Pipeline setup with scaler and XGBoost Regressor
steps = [('sc', sc), ('xgb_final', xgb_final)]
xgbpipe = Pipeline(steps=steps)

# Parameter grid for Randomized Search
params_random_xgb = {
    'xgb_final__n_estimators': list(range(20, 100, 10)),
    'xgb_final__learning_rate': np.geomspace(0.001, 1)
}

# Randomized Search for R
random_search_r2_xgb = RandomizedSearchCV(xgbpipe, params_random_xgb, scoring='r2', cv=4, n_iter=10, random_state=42)
random_search_r2_xgb.fit(X_train, y_train)

# Randomized Search for MSE
random_search_mse_xgb = RandomizedSearchCV(xgbpipe, params_random_xgb, scoring='neg_mean_squared_error', cv=4, n_iter=10, random_state=42)
random_search_mse_xgb.fit(X_train, y_train)

# Best parameters for R
best_params_r2_xgb = random_search_r2_xgb.best_estimator_.get_params()
print('Best Parameters for R (XGBoost):', best_params_r2_xgb)

# Best parameters for MSE
best_params_mse_xgb = random_search_mse_xgb.best_estimator_.get_params()
print('Best Parameters for MSE (XGBoost):', best_params_mse_xgb)

# Final model using best parameters for R
xgb_final_r2 = XGBRegressor(random_state=69, n_estimators=best_params_r2_xgb['xgb_final__n_estimators'],
                             learning_rate=best_params_r2_xgb['xgb_final__learning_rate'])
xgbpipe_r2 = Pipeline(steps=[('sc', sc), ('xgb_final_r2', xgb_final_r2)])
xgbpipe_r2.fit(X_train, y_train)
pred_r2_xgb = xgbpipe_r2.predict(X_test)
xgb_mse_r2 = mean_squared_error(y_test, pred_r2_xgb)
xgb_rsquared_r2 = r2_score(y_test, pred_r2_xgb)
print('R (using best parameters for R - XGBoost):', xgb_rsquared_r2)
print('MSE (using best parameters for R - XGBoost):', xgb_mse_r2)

# Final model using best parameters for MSE
xgb_final_mse = XGBRegressor(random_state=69, n_estimators=best_params_mse_xgb['xgb_final__n_estimators'],
                              learning_rate=best_params_mse_xgb['xgb_final__learning_rate'])
xgbpipe_mse = Pipeline(steps=[('sc', sc), ('xgb_final_mse', xgb_final_mse)])
xgbpipe_mse.fit(X_train, y_train)
pred_mse_xgb = xgbpipe_mse.predict(X_test)
xgb_mse_mse = mean_squared_error(y_test, pred_mse_xgb)
xgb_rsquared_mse = r2_score(y_test, pred_mse_xgb)
print('R (using best parameters for MSE - XGBoost):', xgb_rsquared_mse)
print('MSE (using best parameters for MSE - XGBoost):', xgb_mse_mse)

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_squared_error, r2_score

# Function to create visualizations
def visualize_xgboost_performance(model, X_test, y_test, title):
    # Make predictions
    y_pred = model.predict(X_test)

    # Visualize predictions vs true values
    plt.figure(figsize=(12, 6))
    plt.scatter(y_test, y_pred, alpha=0.5)
    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], '--', color='red', linewidth=2)
    plt.title(f'{title} - Predictions vs True Values')
    plt.xlabel('True Values')
    plt.ylabel('Predicted Values')
    plt.show()

    # Visualize residuals
    residuals = y_test - y_pred
    plt.figure(figsize=(12, 6))
    plt.scatter(y_test, residuals, alpha=0.5)
    plt.axhline(y=0, color='red', linestyle='--', linewidth=2)
    plt.title(f'{title} - Residuals')
    plt.xlabel('True Values')
    plt.ylabel('Residuals')
    plt.show()

# Visualize using best parameters for R
visualize_xgboost_performance(xgbpipe_r2, X_test, y_test, 'XGBoost (Best Parameters for R)')

# Visualize using best parameters for MSE
visualize_xgboost_performance(xgbpipe_mse, X_test, y_test, 'XGBoost (Best Parameters for MSE)')

"""### **XGBoost using Grid Search**"""

from sklearn.model_selection import GridSearchCV
from xgboost import XGBRegressor

# XGBoost Regressor setup
xgb_final = XGBRegressor(random_state=69)

# Pipeline setup with scaler and XGBoost Regressor
steps = [('sc', sc), ('xgb_final', xgb_final)]
xgbpipe = Pipeline(steps=steps)

# Parameter grid for Grid Search
params_grid_xgb = {
    'xgb_final__n_estimators': list(range(20, 100, 10)),
    'xgb_final__learning_rate': np.geomspace(0.001, 1)
}

# Grid Search for R
grid_search_r2_xgb = GridSearchCV(xgbpipe, params_grid_xgb, scoring='r2', cv=4)
grid_search_r2_xgb.fit(X_train, y_train)

# Grid Search for MSE
grid_search_mse_xgb = GridSearchCV(xgbpipe, params_grid_xgb, scoring='neg_mean_squared_error', cv=4)
grid_search_mse_xgb.fit(X_train, y_train)

# Best parameters for R
best_params_r2_xgb = grid_search_r2_xgb.best_estimator_.get_params()
print('Best Parameters for R (XGBoost):', best_params_r2_xgb)

# Best parameters for MSE
best_params_mse_xgb = grid_search_mse_xgb.best_estimator_.get_params()
print('Best Parameters for MSE (XGBoost):', best_params_mse_xgb)

# Final model using best parameters for R
xgb_final_r2 = XGBRegressor(random_state=69, n_estimators=best_params_r2_xgb['xgb_final__n_estimators'],
                             learning_rate=best_params_r2_xgb['xgb_final__learning_rate'])
xgbpipe_r2 = Pipeline(steps=[('sc', sc), ('xgb_final_r2', xgb_final_r2)])
xgbpipe_r2.fit(X_train, y_train)
pred_r2_xgb = xgbpipe_r2.predict(X_test)
xgb_mse_r2 = mean_squared_error(y_test, pred_r2_xgb)
xgb_rsquared_r2 = r2_score(y_test, pred_r2_xgb)
print('R (using best parameters for R - XGBoost):', xgb_rsquared_r2)
print('MSE (using best parameters for R - XGBoost):', xgb_mse_r2)

# Final model using best parameters for MSE
xgb_final_mse = XGBRegressor(random_state=69, n_estimators=best_params_mse_xgb['xgb_final__n_estimators'],
                              learning_rate=best_params_mse_xgb['xgb_final__learning_rate'])
xgbpipe_mse = Pipeline(steps=[('sc', sc), ('xgb_final_mse', xgb_final_mse)])
xgbpipe_mse.fit(X_train, y_train)
pred_mse_xgb = xgbpipe_mse.predict(X_test)
xgb_mse_mse = mean_squared_error(y_test, pred_mse_xgb)
xgb_rsquared_mse = r2_score(y_test, pred_mse_xgb)
print('R (using best parameters for MSE - XGBoost):', xgb_rsquared_mse)
print('MSE (using best parameters for MSE - XGBoost):', xgb_mse_mse)

# Visualize using best parameters for R
visualize_xgboost_performance(xgbpipe_r2, X_test, y_test, 'XGBoost (Best Parameters for R - Grid Search)')

# Visualize using best parameters for MSE
visualize_xgboost_performance(xgbpipe_mse, X_test, y_test, 'XGBoost (Best Parameters for MSE - Grid Search)')

"""## **4. Linear Regression**"""

# Linear Regression setup
lr_final = LinearRegression()

# Pipeline setup with scaler and Linear Regression
lr_pipe = Pipeline(steps=[('sc', StandardScaler()), ('lr_final', lr_final)])

# Train the Linear Regression model
lr_pipe.fit(X_train, y_train)

# Make predictions on the test set
y_pred_lr = lr_pipe.predict(X_test)

# Calculate Mean Squared Error (MSE)
mse_lr = mean_squared_error(y_test, y_pred_lr)

# Calculate R-squared (R2)
r2_lr = r2_score(y_test, y_pred_lr)

# Print evaluation metrics
print("Linear Regression Model:")
print("Mean Squared Error (MSE): {:.2f}".format(mse_lr))
print("R-squared (R2): {:.2f}".format(r2_lr))

"""### **Linear Regression using Grid Search**"""

# Linear Regression setup
lr_final = LinearRegression()

# Pipeline setup with scaler and Linear Regression
steps = [('sc', StandardScaler()), ('lr_final', lr_final)]
lr_pipe = Pipeline(steps=steps)

# Parameter grid for Grid Search
params_grid_lr = {
    'lr_final__fit_intercept': [True, False],
    'lr_final__positive': [True, False],
    'lr_final__copy_X': [True, False],
    'lr_final__n_jobs': [-1, None]  # Adjust the values based on your preferences
}

# Grid Search for R
grid_search_r2_lr = GridSearchCV(lr_pipe, params_grid_lr, scoring='r2', cv=4)
grid_search_r2_lr.fit(X_train, y_train)

# Grid Search for MSE
grid_search_mse_lr = GridSearchCV(lr_pipe, params_grid_lr, scoring='neg_mean_squared_error', cv=4)
grid_search_mse_lr.fit(X_train, y_train)

# Best parameters for R
best_params_r2_lr = grid_search_r2_lr.best_estimator_.get_params()
print('Best Parameters for R (Linear Regression):', best_params_r2_lr)

# Best parameters for MSE
best_params_mse_lr = grid_search_mse_lr.best_estimator_.get_params()
print('Best Parameters for MSE (Linear Regression):', best_params_mse_lr)

# Final model using best parameters for R
lr_final_r2 = LinearRegression()
lr_pipe_r2 = Pipeline(steps=[('sc', StandardScaler()), ('lr_final_r2', lr_final_r2)])
lr_pipe_r2.fit(X_train, y_train)

# Final model using best parameters for MSE
lr_final_mse = LinearRegression()
lr_pipe_mse = Pipeline(steps=[('sc', StandardScaler()), ('lr_final_mse', lr_final_mse)])
lr_pipe_mse.fit(X_train, y_train)

# Predictions and evaluation for R
pred_r2_lr = lr_pipe_r2.predict(X_test)
lr_mse_r2 = mean_squared_error(y_test, pred_r2_lr)
lr_rsquared_r2 = r2_score(y_test, pred_r2_lr)
print('R (using best parameters for R - Linear Regression):', lr_rsquared_r2)
print('MSE (using best parameters for R - Linear Regression):', lr_mse_r2)

# Predictions and evaluation for MSE
pred_mse_lr = lr_pipe_mse.predict(X_test)
lr_mse_mse = mean_squared_error(y_test, pred_mse_lr)
lr_rsquared_mse = r2_score(y_test, pred_mse_lr)
print('R (using best parameters for MSE - Linear Regression):', lr_rsquared_mse)
print('MSE (using best parameters for MSE - Linear Regression):', lr_mse_mse)

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_squared_error, r2_score

def visualize_linear_regression_performance(model, X_test, y_test, title):
    # Make predictions
    y_pred = model.predict(X_test)

    # Scatter plot for actual vs predicted values
    plt.figure(figsize=(10, 6))
    sns.scatterplot(x=y_test, y=y_pred)
    plt.title(title)
    plt.xlabel('Actual Values')
    plt.ylabel('Predicted Values')
    plt.show()

    # Residual plot
    residuals = y_test - y_pred
    plt.figure(figsize=(10, 6))
    sns.scatterplot(x=y_pred, y=residuals)
    plt.axhline(y=0, color='r', linestyle='--')
    plt.title('Residual Plot')
    plt.xlabel('Predicted Values')
    plt.ylabel('Residuals')
    plt.show()

    # Distribution of residuals
    plt.figure(figsize=(10, 6))
    sns.histplot(residuals, kde=True)
    plt.title('Distribution of Residuals')
    plt.xlabel('Residuals')
    plt.ylabel('Frequency')
    plt.show()

    # Print evaluation metrics
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    print(f'Mean Squared Error: {mse:.4f}')
    print(f'R Score: {r2:.4f}')

# Visualize using best parameters for R
visualize_linear_regression_performance(lr_pipe_r2, X_test, y_test, 'Linear Regression (Best Parameters for R - Grid Search)')

# Visualize using best parameters for MSE
visualize_linear_regression_performance(lr_pipe_mse, X_test, y_test, 'Linear Regression (Best Parameters for MSE - Grid Search)')

"""### **Linear Regression using Random Search**"""

# Random Search for R
random_search_r2_lr = RandomizedSearchCV(lr_pipe, params_grid_lr, scoring='r2', cv=4, n_iter=10)
random_search_r2_lr.fit(X_train, y_train)

# Random Search for MSE
random_search_mse_lr = RandomizedSearchCV(lr_pipe, params_grid_lr, scoring='neg_mean_squared_error', cv=4, n_iter=10)
random_search_mse_lr.fit(X_train, y_train)

# Best parameters for R
best_params_r2_lr = random_search_r2_lr.best_estimator_.get_params()
print('Best Parameters for R (Linear Regression):', best_params_r2_lr)

# Best parameters for MSE
best_params_mse_lr = random_search_mse_lr.best_estimator_.get_params()
print('Best Parameters for MSE (Linear Regression):', best_params_mse_lr)

# Final model using best parameters for R
lr_final_r2 = LinearRegression()
lr_pipe_r2 = Pipeline(steps=[('sc', StandardScaler()), ('lr_final_r2', lr_final_r2)])
lr_pipe_r2.set_params(**best_params_r2_lr)
lr_pipe_r2.fit(X_train, y_train)

# Final model using best parameters for MSE
lr_final_mse = LinearRegression()
lr_pipe_mse = Pipeline(steps=[('sc', StandardScaler()), ('lr_final_mse', lr_final_mse)])
lr_pipe_mse.set_params(**best_params_mse_lr)
lr_pipe_mse.fit(X_train, y_train)

# Predictions and evaluation for R
pred_r2_lr = lr_pipe_r2.predict(X_test)
lr_mse_r2 = mean_squared_error(y_test, pred_r2_lr)
lr_rsquared_r2 = r2_score(y_test, pred_r2_lr)
print('R (using best parameters for R - Linear Regression):', lr_rsquared_r2)
print('MSE (using best parameters for R - Linear Regression):', lr_mse_r2)

# Predictions and evaluation for MSE
pred_mse_lr = lr_pipe_mse.predict(X_test)
lr_mse_mse = mean_squared_error(y_test, pred_mse_lr)
lr_rsquared_mse = r2_score(y_test, pred_mse_lr)
print('R (using best parameters for MSE - Linear Regression):', lr_rsquared_mse)
print('MSE (using best parameters for MSE - Linear Regression):', lr_mse_mse)

# Visualize using best parameters for R
visualize_linear_regression_performance(lr_pipe_r2, X_test, y_test, 'Linear Regression (Best Parameters for R - Randomized Search)')

# Visualize using best parameters for MSE
visualize_linear_regression_performance(lr_pipe_mse, X_test, y_test, 'Linear Regression (Best Parameters for MSE - Randomized Search)')

"""## **5. Support Vector Machine (SVM)**"""

from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

# Define the pipeline for SVM with scaling
svm_pipe = Pipeline([
    ('sc', StandardScaler()),
    ('svm', SVR())
])

# Train the SVM model
svm_pipe.fit(X_train, y_train)

# Make predictions on the test set
y_pred_svm = svm_pipe.predict(X_test)

# Calculate Mean Squared Error (MSE)
mse_svm = mean_squared_error(y_test, y_pred_svm)

# Calculate R-squared (R2)
r2_svm = r2_score(y_test, y_pred_svm)

# Print evaluation metrics
print("Support Vector Machine (SVM) Model:")
print("Mean Squared Error (MSE): {:.2f}".format(mse_svm))
print("R-squared (R2): {:.2f}".format(r2_svm))

"""### **Support Vector Machine (SVM) using Random Search**"""

from sklearn.svm import SVR
from scipy.stats import uniform

# Define the pipeline for SVM with scaling
svm_pipe = Pipeline([
    ('sc', StandardScaler()),
    ('svm', SVR())
])

# Define the parameter grid for random search
params_grid_svm = {
    'svm__C': uniform(0.1, 10),
    'svm__kernel': ['linear', 'rbf', 'poly'],
    'svm__degree': [2, 3, 4],
    'svm__gamma': ['scale', 'auto']
}

# Random Search for R
random_search_r2_svm = RandomizedSearchCV(svm_pipe, params_grid_svm, scoring='r2', cv=4, n_iter=10)
random_search_r2_svm.fit(X_train, y_train)

# Random Search for MSE
random_search_mse_svm = RandomizedSearchCV(svm_pipe, params_grid_svm, scoring='neg_mean_squared_error', cv=4, n_iter=10)
random_search_mse_svm.fit(X_train, y_train)

# Best parameters for R
best_params_r2_svm = random_search_r2_svm.best_estimator_.get_params()
print('Best Parameters for R (SVM):', best_params_r2_svm)

# Best parameters for MSE
best_params_mse_svm = random_search_mse_svm.best_estimator_.get_params()
print('Best Parameters for MSE (SVM):', best_params_mse_svm)

# Final model using best parameters for R
svm_final_r2 = SVR()
svm_pipe_r2 = Pipeline(steps=[('sc', StandardScaler()), ('svm_final_r2', svm_final_r2)])
svm_pipe_r2.set_params(**best_params_r2_svm)
svm_pipe_r2.fit(X_train, y_train)

# Final model using best parameters for MSE
svm_final_mse = SVR()
svm_pipe_mse = Pipeline(steps=[('sc', StandardScaler()), ('svm_final_mse', svm_final_mse)])
svm_pipe_mse.set_params(**best_params_mse_svm)
svm_pipe_mse.fit(X_train, y_train)

# Predictions and evaluation for R
pred_r2_svm = svm_pipe_r2.predict(X_test)
svm_mse_r2 = mean_squared_error(y_test, pred_r2_svm)
svm_rsquared_r2 = r2_score(y_test, pred_r2_svm)
print('R (using best parameters for R - SVM):', svm_rsquared_r2)
print('MSE (using best parameters for R - SVM):', svm_mse_r2)

# Predictions and evaluation for MSE
pred_mse_svm = svm_pipe_mse.predict(X_test)
svm_mse_mse = mean_squared_error(y_test, pred_mse_svm)
svm_rsquared_mse = r2_score(y_test, pred_mse_svm)
print('R (using best parameters for MSE - SVM):', svm_rsquared_mse)
print('MSE (using best parameters for MSE - SVM):', svm_mse_mse)

import matplotlib.pyplot as plt
import seaborn as sns

def visualize_svm_performance(model, X_test, y_test, title):
    # Scatter plot of predicted vs actual values
    plt.figure(figsize=(10, 6))
    sns.scatterplot(x=y_test, y=model.predict(X_test))
    plt.title(f'{title}\nPredicted vs Actual Values')
    plt.xlabel('Actual Values')
    plt.ylabel('Predicted Values')
    plt.show()

    # Residual plot
    plt.figure(figsize=(10, 6))
    sns.scatterplot(x=model.predict(X_test), y=y_test - model.predict(X_test))
    plt.title(f'{title}\nResidual Plot')
    plt.xlabel('Predicted Values')
    plt.ylabel('Residuals')
    plt.axhline(y=0, color='r', linestyle='--')
    plt.show()

    # Distribution plot of residuals
    plt.figure(figsize=(10, 6))
    sns.histplot(y_test - model.predict(X_test), kde=True)
    plt.title(f'{title}\nDistribution of Residuals')
    plt.xlabel('Residuals')
    plt.ylabel('Frequency')
    plt.show()

    # Evaluation metrics
    print('Mean Squared Error:', mean_squared_error(y_test, model.predict(X_test)))
    print('R Score:', r2_score(y_test, model.predict(X_test)))

# Visualize using best parameters for R
visualize_svm_performance(svm_pipe_r2, X_test, y_test, 'SVM (Best Parameters for R - Randomized Search)')

# Visualize using best parameters for MSE
visualize_svm_performance(svm_pipe_mse, X_test, y_test, 'SVM (Best Parameters for MSE - Randomized Search)')

"""### **Support Vector Machine (SVM) using Grid Search**"""

# Define the pipeline for SVM with scaling
svm_pipe = Pipeline([
    ('sc', StandardScaler()),
    ('svm', SVR())
])

# Define the parameter grid for grid search
params_grid_svm = {
    'svm__C': [0.1, 1, 10],
    'svm__kernel': ['linear', 'rbf', 'poly'],
    'svm__degree': [2, 3, 4],
    'svm__gamma': ['scale', 'auto']
}

# Grid Search for R
grid_search_r2_svm = GridSearchCV(svm_pipe, params_grid_svm, scoring='r2', cv=4)
grid_search_r2_svm.fit(X_train, y_train)

# Grid Search for MSE
grid_search_mse_svm = GridSearchCV(svm_pipe, params_grid_svm, scoring='neg_mean_squared_error', cv=4)
grid_search_mse_svm.fit(X_train, y_train)

# Best parameters for R
best_params_r2_svm = grid_search_r2_svm.best_estimator_.get_params()
print('Best Parameters for R (SVM):', best_params_r2_svm)

# Best parameters for MSE
best_params_mse_svm = grid_search_mse_svm.best_estimator_.get_params()
print('Best Parameters for MSE (SVM):', best_params_mse_svm)

# Final model using best parameters for R
svm_final_r2 = SVR()
svm_pipe_r2 = Pipeline(steps=[('sc', StandardScaler()), ('svm_final_r2', svm_final_r2)])
svm_pipe_r2.set_params(**best_params_r2_svm)
svm_pipe_r2.fit(X_train, y_train)

# Final model using best parameters for MSE
svm_final_mse = SVR()
svm_pipe_mse = Pipeline(steps=[('sc', StandardScaler()), ('svm_final_mse', svm_final_mse)])
svm_pipe_mse.set_params(**best_params_mse_svm)
svm_pipe_mse.fit(X_train, y_train)

# Predictions and evaluation for R
pred_r2_svm = svm_pipe_r2.predict(X_test)
svm_mse_r2 = mean_squared_error(y_test, pred_r2_svm)
svm_rsquared_r2 = r2_score(y_test, pred_r2_svm)
print('R (using best parameters for R - SVM):', svm_rsquared_r2)
print('MSE (using best parameters for R - SVM):', svm_mse_r2)

# Predictions and evaluation for MSE
pred_mse_svm = svm_pipe_mse.predict(X_test)
svm_mse_mse = mean_squared_error(y_test, pred_mse_svm)
svm_rsquared_mse = r2_score(y_test, pred_mse_svm)
print('R (using best parameters for MSE - SVM):', svm_rsquared_mse)
print('MSE (using best parameters for MSE - SVM):', svm_mse_mse)

import matplotlib.pyplot as plt
import seaborn as sns

def visualize_svm_performance(model, X_test, y_test, title):
    # Scatter plot of predicted vs actual values
    plt.figure(figsize=(10, 6))
    sns.scatterplot(x=y_test, y=model.predict(X_test))
    plt.title(f'{title}\nPredicted vs Actual Values')
    plt.xlabel('Actual Values')
    plt.ylabel('Predicted Values')
    plt.show()

    # Residual plot
    plt.figure(figsize=(10, 6))
    sns.scatterplot(x=model.predict(X_test), y=y_test - model.predict(X_test))
    plt.title(f'{title}\nResidual Plot')
    plt.xlabel('Predicted Values')
    plt.ylabel('Residuals')
    plt.axhline(y=0, color='r', linestyle='--')
    plt.show()

    # Distribution plot of residuals
    plt.figure(figsize=(10, 6))
    sns.histplot(y_test - model.predict(X_test), kde=True)
    plt.title(f'{title}\nDistribution of Residuals')
    plt.xlabel('Residuals')
    plt.ylabel('Frequency')
    plt.show()

    # Evaluation metrics
    print('Mean Squared Error:', mean_squared_error(y_test, model.predict(X_test)))
    print('R Score:', r2_score(y_test, model.predict(X_test)))

# Visualize using best parameters for R
visualize_svm_performance(svm_pipe_r2, X_test, y_test, 'SVM (Best Parameters for R - Grid Search)')

# Visualize using best parameters for MSE
visualize_svm_performance(svm_pipe_mse, X_test, y_test, 'SVM (Best Parameters for MSE - Grid Search)')

"""## **6. Lasso**"""

from sklearn.linear_model import Lasso
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split

# Assuming you have X_train, X_test, y_train, y_test defined

# Lasso Regressor setup
lasso_final = Lasso(random_state=69)

# Pipeline setup with scaler and Lasso Regressor
lasso_pipe = Pipeline(steps=[('sc', StandardScaler()), ('lasso_final', lasso_final)])

# Train the Lasso model
lasso_pipe.fit(X_train, y_train)

# Make predictions on the test set
y_pred_lasso = lasso_pipe.predict(X_test)

# Calculate Mean Squared Error (MSE)
mse_lasso = mean_squared_error(y_test, y_pred_lasso)

# Calculate R-squared (R2)
r2_lasso = r2_score(y_test, y_pred_lasso)

# Print evaluation metrics
print("Lasso Regression Model:")
print("Mean Squared Error (MSE): {:.2f}".format(mse_lasso))
print("R-squared (R2): {:.2f}".format(r2_lasso))

"""### **Lasso using random search**"""

from sklearn.linear_model import Lasso

# Lasso Regressor setup
lasso_final = Lasso(random_state=69)

# Pipeline setup with scaler and Lasso Regressor
steps = [('sc', StandardScaler()), ('lasso_final', lasso_final)]
lasso_pipe = Pipeline(steps=steps)

# Parameter grid for Randomized Search
params_random_lasso = {
    'lasso_final__alpha': np.logspace(-4, 4, 9),  # Example range for alpha (adjust as needed)
}

# Randomized Search for R
random_search_r2_lasso = RandomizedSearchCV(lasso_pipe, params_random_lasso, scoring='r2', cv=4, n_iter=10, random_state=42)
random_search_r2_lasso.fit(X_train, y_train)

# Randomized Search for MSE
random_search_mse_lasso = RandomizedSearchCV(lasso_pipe, params_random_lasso, scoring='neg_mean_squared_error', cv=4, n_iter=10, random_state=42)
random_search_mse_lasso.fit(X_train, y_train)

# Best parameters for R
best_params_r2_lasso = random_search_r2_lasso.best_estimator_.get_params()
print('Best Parameters for R (Lasso):', best_params_r2_lasso)

# Best parameters for MSE
best_params_mse_lasso = random_search_mse_lasso.best_estimator_.get_params()
print('Best Parameters for MSE (Lasso):', best_params_mse_lasso)

# Final model using best parameters for R
lasso_final_r2 = Lasso(random_state=69, alpha=best_params_r2_lasso['lasso_final__alpha'])
lasso_pipe_r2 = Pipeline(steps=[('sc', StandardScaler()), ('lasso_final_r2', lasso_final_r2)])
lasso_pipe_r2.fit(X_train, y_train)
pred_r2_lasso = lasso_pipe_r2.predict(X_test)
lasso_mse_r2 = mean_squared_error(y_test, pred_r2_lasso)
lasso_rsquared_r2 = r2_score(y_test, pred_r2_lasso)
print('R (using best parameters for R - Lasso):', lasso_rsquared_r2)
print('MSE (using best parameters for R - Lasso):', lasso_mse_r2)

# Final model using best parameters for MSE
lasso_final_mse = Lasso(random_state=69, alpha=best_params_mse_lasso['lasso_final__alpha'])
lasso_pipe_mse = Pipeline(steps=[('sc', StandardScaler()), ('lasso_final_mse', lasso_final_mse)])
lasso_pipe_mse.fit(X_train, y_train)
pred_mse_lasso = lasso_pipe_mse.predict(X_test)
lasso_mse_mse = mean_squared_error(y_test, pred_mse_lasso)
lasso_rsquared_mse = r2_score(y_test, pred_mse_lasso)
print('R (using best parameters for MSE - Lasso):', lasso_rsquared_mse)
print('MSE (using best parameters for MSE - Lasso):', lasso_mse_mse)

import matplotlib.pyplot as plt
import seaborn as sns

def visualize_lasso_performance(model, X_test, y_test, title):
    # Scatter plot of predicted vs actual values
    plt.figure(figsize=(10, 6))
    sns.scatterplot(x=y_test, y=model.predict(X_test))
    plt.title(f'{title}\nPredicted vs Actual Values')
    plt.xlabel('Actual Values')
    plt.ylabel('Predicted Values')
    plt.show()

    # Residual plot
    plt.figure(figsize=(10, 6))
    sns.scatterplot(x=model.predict(X_test), y=y_test - model.predict(X_test))
    plt.title(f'{title}\nResidual Plot')
    plt.xlabel('Predicted Values')
    plt.ylabel('Residuals')
    plt.axhline(y=0, color='r', linestyle='--')
    plt.show()

    # Distribution plot of residuals
    plt.figure(figsize=(10, 6))
    sns.histplot(y_test - model.predict(X_test), kde=True)
    plt.title(f'{title}\nDistribution of Residuals')
    plt.xlabel('Residuals')
    plt.ylabel('Frequency')
    plt.show()

    # Evaluation metrics
    print('Mean Squared Error:', mean_squared_error(y_test, model.predict(X_test)))
    print('R Score:', r2_score(y_test, model.predict(X_test)))

# Visualize using best parameters for R
visualize_lasso_performance(lasso_pipe_r2, X_test, y_test, 'Lasso (Best Parameters for R - Randomized Search)')

# Visualize using best parameters for MSE
visualize_lasso_performance(lasso_pipe_mse, X_test, y_test, 'Lasso (Best Parameters for MSE - Randomized Search)')

"""### **Lasso using Grid Search**"""

# Lasso Regressor setup
lasso_final = Lasso(random_state=69)

# Pipeline setup with scaler and Lasso Regressor
steps = [('sc', StandardScaler()), ('lasso_final', lasso_final)]
lasso_pipe = Pipeline(steps=steps)

# Parameter grid for Grid Search
params_grid_lasso = {
    'lasso_final__alpha': np.logspace(-4, 4, 9),  # Example range for alpha (adjust as needed)
}

# Grid Search for R
grid_search_r2_lasso = GridSearchCV(lasso_pipe, params_grid_lasso, scoring='r2', cv=4)
grid_search_r2_lasso.fit(X_train, y_train)

# Grid Search for MSE
grid_search_mse_lasso = GridSearchCV(lasso_pipe, params_grid_lasso, scoring='neg_mean_squared_error', cv=4)
grid_search_mse_lasso.fit(X_train, y_train)

# Best parameters for R
best_params_r2_lasso = grid_search_r2_lasso.best_estimator_.get_params()
print('Best Parameters for R (Lasso):', best_params_r2_lasso)

# Best parameters for MSE
best_params_mse_lasso = grid_search_mse_lasso.best_estimator_.get_params()
print('Best Parameters for MSE (Lasso):', best_params_mse_lasso)

# Final model using best parameters for R
lasso_final_r2 = Lasso(random_state=69, alpha=best_params_r2_lasso['lasso_final__alpha'])
lasso_pipe_r2 = Pipeline(steps=[('sc', StandardScaler()), ('lasso_final_r2', lasso_final_r2)])
lasso_pipe_r2.fit(X_train, y_train)
pred_r2_lasso = lasso_pipe_r2.predict(X_test)
lasso_mse_r2 = mean_squared_error(y_test, pred_r2_lasso)
lasso_rsquared_r2 = r2_score(y_test, pred_r2_lasso)
print('R (using best parameters for R - Lasso):', lasso_rsquared_r2)
print('MSE (using best parameters for R - Lasso):', lasso_mse_r2)

# Final model using best parameters for MSE
lasso_final_mse = Lasso(random_state=69, alpha=best_params_mse_lasso['lasso_final__alpha'])
lasso_pipe_mse = Pipeline(steps=[('sc', StandardScaler()), ('lasso_final_mse', lasso_final_mse)])
lasso_pipe_mse.fit(X_train, y_train)
pred_mse_lasso = lasso_pipe_mse.predict(X_test)
lasso_mse_mse = mean_squared_error(y_test, pred_mse_lasso)
lasso_rsquared_mse = r2_score(y_test, pred_mse_lasso)
print('R (using best parameters for MSE - Lasso):', lasso_rsquared_mse)
print('MSE (using best parameters for MSE - Lasso):', lasso_mse_mse)

# Visualize using best parameters for R
visualize_lasso_performance(lasso_pipe_r2, X_test, y_test, 'Lasso (Best Parameters for R - Grid Search)')

# Visualize using best parameters for MSE
visualize_lasso_performance(lasso_pipe_mse, X_test, y_test, 'Lasso (Best Parameters for MSE - Grid Search)')

"""## **7. Decision TreeRegressor**"""

dt=DecisionTreeRegressor(random_state=59)
dtpipe=Pipeline(steps=[('sc',sc),('dt',dt)])

dtpipe.fit(X_train,y_train)

pred=dtpipe.predict(X_test)
print('r2: ',r2_score(y_test,pred))
print('mse: ',mean_squared_error(y_test,pred))

"""### **Tuning Parameters using Random Search**"""

dt=DecisionTreeRegressor(random_state=69)
steps=[('sc',sc),('dt',dt)]
dtpipe=Pipeline(steps=steps)
params=dict(dt__max_depth=list(range(2,10)),dt__criterion=["squared_error", "friedman_mse", "absolute_error","poisson"])

dtcv=RandomizedSearchCV(dtpipe,params,random_state=33)
dtcv.fit(X_train,y_train)
dtcv.best_estimator_.get_params()

dt_final1=DecisionTreeRegressor(max_depth=6,criterion='squared_error')
dtpipe=Pipeline(steps=[('sc',sc),('dt_final1',dt_final1)])
dtpipe.fit(X_train,y_train)

pred=dtpipe.predict(X_test)
dt_mse=mean_squared_error(y_test,pred)
dt_rsquared=r2_score(y_test,pred)
print('r2: ',dt_rsquared)
print('mse: ',dt_mse)

from sklearn.tree import DecisionTreeRegressor
from sklearn.pipeline import Pipeline
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import r2_score, mean_squared_error

# Decision Tree Regressor setup
dt = DecisionTreeRegressor(random_state=59)
dtpipe = Pipeline(steps=[('sc', sc), ('dt', dt)])
dtpipe.fit(X_train, y_train)
pred = dtpipe.predict(X_test)
print('r2:', r2_score(y_test, pred))
print('mse:', mean_squared_error(y_test, pred))

# Randomized Search for Hyperparameter Tuning
dt = DecisionTreeRegressor(random_state=69)
steps = [('sc', sc), ('dt', dt)]
dtpipe = Pipeline(steps=steps)
params = {
    'dt__max_depth': list(range(2, 10)),
    'dt__criterion': ["mse", "friedman_mse", "mae", "poisson"]
}
dtcv = RandomizedSearchCV(dtpipe, params, random_state=33)
dtcv.fit(X_train, y_train)

# Best parameters
best_params_dt = dtcv.best_estimator_.get_params()
print('Best Parameters for Decision Tree:', best_params_dt)

# Final model using best parameters
dt_final = DecisionTreeRegressor(max_depth=best_params_dt['dt__max_depth'],
                                 criterion=best_params_dt['dt__criterion'])
dtpipe_final = Pipeline(steps=[('sc', sc), ('dt_final', dt_final)])
dtpipe_final.fit(X_train, y_train)
pred_final = dtpipe_final.predict(X_test)
dt_mse_final = mean_squared_error(y_test, pred_final)
dt_rsquared_final = r2_score(y_test, pred_final)
print('r2 (using best parameters):', dt_rsquared_final)
print('mse (using best parameters):', dt_mse_final)

import matplotlib.pyplot as plt

# Scatter plot for Decision Tree Regressor
plt.figure(figsize=(10, 6))

# Line plot for actual data
plt.plot(y_test, label='Actual', marker='o', linestyle='None', markersize=8, color='blue')

# Line plot for predicted data
plt.plot(pred, label='Predicted', marker='o', linestyle='None', markersize=8, color='red')

plt.title('Decision Tree Regressor - Actual vs. Predicted')
plt.xlabel('Sample Index')
plt.ylabel('Target Variable (y)')
plt.legend()
plt.show()

import matplotlib.pyplot as plt

# Scatter plot for predicted vs actual data (R)
plt.scatter(y_test, pred, color='red', label='Predicted (Original)')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='blue', label='Actual')
plt.title('Decision Tree Regressor - Actual vs Predicted (Original)')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values (Original)')
plt.legend()
plt.show()

# Scatter plot for predicted vs actual data (R - Random Search)
plt.scatter(y_test, pred_final, color='orange', label='Predicted (Random Search - R)')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='blue', label='Actual')
plt.title('Decision Tree Regressor - Actual vs Predicted (Random Search - R)')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values (Random Search - R)')
plt.legend()
plt.show()

# Scatter plot for predicted vs actual data (MSE - Random Search)
plt.scatter(y_test, pred_final, color='green', label='Predicted (Random Search - MSE)')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='blue', label='Actual')
plt.title('Decision Tree Regressor - Actual vs Predicted (Random Search - MSE)')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values (Random Search - MSE)')
plt.legend()
plt.show()

"""### **Fine tuning using Grid Search**"""

# Decision Tree Regressor setup
dt_final = DecisionTreeRegressor(random_state=69)

# Pipeline setup with scaler and Decision Tree Regressor
steps = [('sc', StandardScaler()), ('dt_final', dt_final)]
dtpipe = Pipeline(steps=steps)

# Parameter grid for Grid Search
params_grid_dt = {
    'dt_final__max_depth': list(range(2, 10)),
    'dt_final__criterion': ["squared_error", "friedman_mse", "absolute_error", "poisson"]
}

# Grid Search for R
grid_search_r2_dt = GridSearchCV(dtpipe, params_grid_dt, scoring='r2', cv=4)
grid_search_r2_dt.fit(X_train, y_train)

# Grid Search for MSE
grid_search_mse_dt = GridSearchCV(dtpipe, params_grid_dt, scoring='neg_mean_squared_error', cv=4)
grid_search_mse_dt.fit(X_train, y_train)

# Best parameters for R
best_params_r2_dt = grid_search_r2_dt.best_estimator_.get_params()
print('Best Parameters for R (Decision Tree):', best_params_r2_dt)

# Best parameters for MSE
best_params_mse_dt = grid_search_mse_dt.best_estimator_.get_params()
print('Best Parameters for MSE (Decision Tree):', best_params_mse_dt)

# Final model using best parameters for R
dt_final_r2 = DecisionTreeRegressor(random_state=69, max_depth=best_params_r2_dt['dt_final__max_depth'],
                                    criterion=best_params_r2_dt['dt_final__criterion'])
dtpipe_r2 = Pipeline(steps=[('sc', StandardScaler()), ('dt_final_r2', dt_final_r2)])
dtpipe_r2.fit(X_train, y_train)
pred_r2_dt = dtpipe_r2.predict(X_test)
dt_mse_r2 = mean_squared_error(y_test, pred_r2_dt)
dt_rsquared_r2 = r2_score(y_test, pred_r2_dt)
print('R (using best parameters for R - Decision Tree):', dt_rsquared_r2)
print('MSE (using best parameters for R - Decision Tree):', dt_mse_r2)

# Final model using best parameters for MSE
dt_final_mse = DecisionTreeRegressor(random_state=69, max_depth=best_params_mse_dt['dt_final__max_depth'],
                                     criterion=best_params_mse_dt['dt_final__criterion'])
dtpipe_mse = Pipeline(steps=[('sc', StandardScaler()), ('dt_final_mse', dt_final_mse)])
dtpipe_mse.fit(X_train, y_train)
pred_mse_dt = dtpipe_mse.predict(X_test)
dt_mse_mse = mean_squared_error(y_test, pred_mse_dt)
dt_rsquared_mse = r2_score(y_test, pred_mse_dt)
print('R (using best parameters for MSE - Decision Tree):', dt_rsquared_mse)
print('MSE (using best parameters for MSE - Decision Tree):', dt_mse_mse)

# Scatter plot for predicted vs actual data (Original)
plt.scatter(y_test, pred_final, color='green', label='Predicted (Original)')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='blue', label='Actual')
plt.title('Decision Tree Regressor - Actual vs Predicted (Original)')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values (Original)')
plt.legend()
plt.show()

# Scatter plot for predicted vs actual data (Original - R)
plt.scatter(y_test, pred_r2_dt, color='purple', label='Predicted (Original - R)')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='blue', label='Actual')
plt.title('Decision Tree Regressor - Actual vs Predicted (Original - R)')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values (Original - R)')
plt.legend()
plt.show()

# Scatter plot for predicted vs actual data (Original - MSE)
plt.scatter(y_test, pred_mse_dt, color='pink', label='Predicted (Original - MSE)')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='blue', label='Actual')
plt.title('Decision Tree Regressor - Actual vs Predicted (Original - MSE)')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values (Original - MSE)')
plt.legend()
plt.show()

"""## **Selecting best model based on R2 AND MSE**"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.linear_model import LinearRegression, Lasso
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor

# List of models
models = ['RandomForestRegressor', 'GradientBoostingRegressor', 'XGBoost', 'LinearRegression', 'SVM', 'Lasso', 'DecisionTreeRegressor']

# Generate random evaluation results for illustration
evaluation_results = {
    'Mean Squared Error': [5883476246.153057, 150263982836.01, 151599339717.96, 10011779634.92, 602860551847.33, 10011745301.55, 9002121252.414045],
    'R-squared': [0.9898825800892396, 0.74, 0.74, 0.98, -0.04, 0.98, 0.9845196552195132],

}

# Add results from grid search and random search for each model
grid_search_mse_results = {'RandomForestRegressor': 4861439109.697738, 'GradientBoostingRegressor': 3525222653.0657, 'XGBoost': 9199959196.926035, 'LinearRegression': 10011779634.915705, 'SVM': 109535695802.29001, 'Lasso': 10011745301.546024, 'DecisionTreeRegressor': 11442974919.31688}
grid_search_r2_results = {'RandomForestRegressor': 0.9916401088768625, 'GradientBoostingRegressor': 0.9940209946435846, 'XGBoost': 0.9827834133456946, 'LinearRegression': 0.9827834133456946, 'SVM': 0.811638802761599, 'Lasso': 0.9827834133520559, 'DecisionTreeRegressor': 0.9803222827044253}

random_search_mse_results = {'RandomForestRegressor': 4876139980.313386, 'GradientBoostingRegressor': 4028428597.039006, 'XGBoost': 8666361096.873606, 'LinearRegression': 10011779634.915705, 'SVM': 114097382772.68053, 'Lasso': 10011745301.546024, 'DecisionTreeRegressor': 9002121252.414045}
random_search_r2_results = {'RandomForestRegressor': 0.9916148287746153, 'GradientBoostingRegressor': 0.9930725812442247, 'XGBoost': 0.985097039463246, 'LinearRegression': 0.9827834133456946, 'SVM': 0.8116480768645798, 'Lasso': 0.9827834133520559, 'DecisionTreeRegressor': 0.9845196552195132}

# Combine the results
for model in models:
    evaluation_results['GridSearch_MSE_' + model] = grid_search_mse_results[model]
    evaluation_results['GridSearch_R2_' + model] = grid_search_r2_results[model]
    evaluation_results['RandomSearch_MSE_' + model] = random_search_mse_results[model]
    evaluation_results['RandomSearch_R2_' + model] = random_search_r2_results[model]

# Create a DataFrame from the results
results_df = pd.DataFrame(evaluation_results, index=models)

# Plot a bar chart to compare models based on MSE
plt.figure(figsize=(10, 6))
plt.bar(models, results_df['Mean Squared Error'], color='blue', label='Mean Squared Error')
plt.xlabel('Model')
plt.ylabel('Mean Squared Error (MSE)')
plt.title('Model Comparison Based on MSE')
plt.legend()
plt.xticks(rotation=45, ha='right')
plt.show()

# Plot a bar chart to compare models based on R-squared
plt.figure(figsize=(10, 6))
plt.bar(models, results_df['R-squared'], color='orange', label='R-squared')
plt.xlabel('Model')
plt.ylabel('R-squared')
plt.title('Model Comparison Based on R-squared')
plt.legend()
plt.xticks(rotation=45, ha='right')
plt.show()

# Identify the best model based on the minimum MSE or maximum R-squared
best_mse_model = results_df['Mean Squared Error'].idxmin()
best_r2_model = results_df['R-squared'].idxmax()

print(f"Best Model based on MSE: {best_mse_model} (MSE: {results_df.loc[best_mse_model, 'Mean Squared Error']})")
print(f"Best Model based on R-squared: {best_r2_model} (R-squared: {results_df.loc[best_r2_model, 'R-squared']})")

import pandas as pd
import matplotlib.pyplot as plt

# Sample data
data = {
    'Model': ['RandomForestRegressor', 'GradientBoostingRegressor', 'XGBoost', 'LinearRegression', 'SVM', 'Lasso', 'DecisionTreeRegressor'],
    'Mean Squared Error': [5883476246.15, 150263982836.01, 151599339717.96, 10011779634.92, 602860551847.33, 10011745301.55, 9002121252.41],
    'R-squared': [0.9899, 0.7400, 0.7400, 0.9800, -0.0400, 0.9800, 0.9845]
}

df = pd.DataFrame(data)

# Create two subplots side by side
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Plot for R-squared
axes[0].bar(df['Model'], df['R-squared'], color='orange')
axes[0].set_title('R-squared Comparison')
axes[0].set_xlabel('Model')
axes[0].set_ylabel('R-squared')

# Plot for Mean Squared Error
axes[1].bar(df['Model'], df['Mean Squared Error'], color='blue')
axes[1].set_title('Mean Squared Error Comparison')
axes[1].set_xlabel('Model')
axes[1].set_ylabel('Mean Squared Error')

# Rotate x-axis labels for better readability
for ax in axes:
    ax.tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()

from IPython.display import display, HTML

# Assuming you have the models and results_df defined

# Create an HTML table
html_table = f"""
<table style="width:50%; border: 1px solid black; text-align:center">
  <tr style="background-color: #f2f2f2">
    <th>Model</th>
    <th>Mean Squared Error (MSE)</th>
    <th>R-squared (R2)</th>
  </tr>
"""

# Populate the HTML table with data
for model in models:
    mse_value = results_df.loc[model, 'Mean Squared Error']
    r2_value = results_df.loc[model, 'R-squared']
    background_color = "#d9edf7" if model == "XGBoost" else "white"  # Highlight XGBoost row
    html_table += f"""
  <tr style="background-color: {background_color}">
    <td>{model}</td>
    <td>{mse_value:.2f}</td>
    <td>{r2_value:.4f}</td>
  </tr>
"""

# Close the HTML table
html_table += "</table>"

# Display the HTML table
display(HTML(html_table))

from IPython.display import display, HTML

# Assuming you have the models and results_df defined

# Sort models based on MSE (ascending) and R2 (descending)
sorted_models_mse = results_df.sort_values('Mean Squared Error').index
sorted_models_r2 = results_df.sort_values('R-squared', ascending=False).index

# Select the top 3 models from both MSE and R2
top3_mse = sorted_models_mse[:3]
top3_r2 = sorted_models_r2[:3]

# Create an HTML table
html_table = f"""
<table style="width:60%; border: 1px solid black; text-align:center">
  <tr style="background-color: #f2f2f2">
    <th>Rank</th>
    <th>Model</th>
    <th>Mean Squared Error (MSE)</th>
    <th>R-squared (R2)</th>
  </tr>
"""

# Populate the HTML table with data
for idx, model in enumerate(models):
    mse_value = results_df.loc[model, 'Mean Squared Error']
    r2_value = results_df.loc[model, 'R-squared']
    background_color = "#d9edf7" if model in top3_mse or model in top3_r2 else "white"
    rank = top3_mse.get_loc(model) + 1 if model in top3_mse else (top3_r2.get_loc(model) + 1) if model in top3_r2 else ""
    html_table += f"""
  <tr style="background-color: {background_color}">
    <td>{rank}</td>
    <td>{model}</td>
    <td>{mse_value:.2f}</td>
    <td>{r2_value:.4f}</td>
  </tr>
"""

# Close the HTML table
html_table += "</table>"

# Display the HTML table
display(HTML(html_table))

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.linear_model import Lasso
from sklearn.tree import DecisionTreeRegressor
from xgboost import XGBRegressor

# Assuming you have X_train, X_test, y_train, y_test defined

# Define the models
models = {
    'Linear Regression': LinearRegression(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'Support Vector Machine (SVM)': SVR(),
    'Lasso': Lasso(),
    'Decision Tree': DecisionTreeRegressor(),
    'XGBoost': XGBRegressor()
}

# Create subplots for each model
fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(15, 20))
fig.suptitle('Actual vs Predicted Values for Different Models', fontsize=16)

# Iterate through each model and plot actual vs predicted values
for (model_name, model), ax in zip(models.items(), axes.flatten()):
    # Fit the model
    model.fit(X_train, y_train)

    # Predictions
    y_pred = model.predict(X_test)

    # Scatter plot
    sns.scatterplot(x=y_test, y=y_pred, ax=ax)
    ax.set_title(f'{model_name} - Actual vs Predicted')
    ax.set_xlabel('Actual Values')
    ax.set_ylabel('Predicted Values')

    # Calculate R-squared and MSE
    r2 = r2_score(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)

    # Display R-squared and MSE as text in the plot
    ax.text(0.1, 0.9, f'R2 = {r2:.4f}\nMSE = {mse:.2f}', transform=ax.transAxes, bbox=dict(facecolor='white', alpha=0.5))

# Adjust layout
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.linear_model import Lasso
from sklearn.tree import DecisionTreeRegressor
from xgboost import XGBRegressor

# Assuming you have X_train, X_test, y_train, y_test defined

# Define the models
models = {
    'Linear Regression': LinearRegression(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'Support Vector Machine (SVM)': SVR(),
    'Lasso': Lasso(),
    'Decision Tree': DecisionTreeRegressor(),
    'XGBoost': XGBRegressor()
}

# Create subplots for each model
fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(15, 20))
fig.suptitle('Actual vs Predicted Values for Different Models', fontsize=16)

# Define colors for predicted and actual values
predicted_color = 'red'
actual_color = 'green'

# Iterate through each model and plot actual vs predicted values
for (model_name, model), ax in zip(models.items(), axes.flatten()):
    # Fit the model
    model.fit(X_train, y_train)

    # Predictions
    y_pred = model.predict(X_test)

    # Scatter plot for predicted values in red
    sns.scatterplot(x=y_test, y=y_pred, ax=ax, color=predicted_color, label='Predicted')

    # Scatter plot for actual values in green
    sns.scatterplot(x=y_test, y=y_test, ax=ax, color=actual_color, label='Actual')

    ax.set_title(f'{model_name} - Actual vs Predicted')
    ax.set_xlabel('Values')
    ax.set_ylabel('Values')

    # Calculate R-squared and MSE
    r2 = r2_score(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)

    # Display R-squared and MSE as text in the plot
    ax.text(0.1, 0.9, f'R2 = {r2:.4f}\nMSE = {mse:.2f}', transform=ax.transAxes, bbox=dict(facecolor='white', alpha=0.5))

    # Add legend to top-right
    ax.legend(loc='center right')

# Adjust layout
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

"""## **Selected Model: Random Forest Regressor**"""

from sklearn.model_selection import RandomizedSearchCV

# Define the parameter grid
param_grid = {
    'n_estimators': [int(x) for x in np.linspace(start=50, stop=200, num=4)],
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_depth': [int(x) for x in np.linspace(10, 110, num=11)],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

# Create a Random Forest Regressor
rf_model = RandomForestRegressor()

# Initialize RandomizedSearchCV
random_search = RandomizedSearchCV(
    rf_model,
    param_distributions=param_grid,
    n_iter=10,
    cv=5,
    scoring={'r2': 'r2', 'neg_mean_squared_error': 'neg_mean_squared_error'},
    refit='r2',  # Specify the metric to use for refitting
    random_state=42,
    n_jobs=-1
)

# Perform random search
random_search.fit(X_train, y_train)

# Get the best parameters
best_params = random_search.best_params_
best_model = random_search.best_estimator_

# Continue with the best model for further analysis

from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestRegressor
import numpy as np

# Define the parameter grid
param_grid = {
    'n_estimators': [int(x) for x in np.linspace(start=50, stop=200, num=4)],
    'max_features': ['auto', 'sqrt', 'log2', None],  # Update max_features values
    'max_depth': [int(x) for x in np.linspace(10, 110, num=11)],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

# Create a Random Forest Regressor
rf_model = RandomForestRegressor()

# Initialize RandomizedSearchCV
random_search = RandomizedSearchCV(
    rf_model,
    param_distributions=param_grid,
    n_iter=10,
    cv=5,
    scoring={'r2': 'r2', 'neg_mean_squared_error': 'neg_mean_squared_error'},
    refit='r2',
    random_state=42,
    n_jobs=-1
)

# Perform random search
random_search.fit(X_train, y_train)

# Get the best parameters
best_params = random_search.best_params_
best_model = random_search.best_estimator_

from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestRegressor
import numpy as np

# Define the parameter grid
param_grid = {
    'n_estimators': [int(x) for x in np.linspace(start=50, stop=200, num=4)],
    'max_features': [int(x) for x in np.linspace(1, X_train.shape[1], num=5)],  # Specify as an integer range
    'max_depth': [int(x) for x in np.linspace(10, 110, num=11)],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

# Create a Random Forest Regressor
rf_model = RandomForestRegressor()

# Initialize RandomizedSearchCV
random_search = RandomizedSearchCV(
    rf_model,
    param_distributions=param_grid,
    n_iter=10,
    cv=5,
    scoring={'r2': 'r2', 'neg_mean_squared_error': 'neg_mean_squared_error'},
    refit='r2',
    random_state=42,
    n_jobs=-1
)

# Perform random search
random_search.fit(X_train, y_train)

# Get the best parameters
best_params = random_search.best_params_
best_model = random_search.best_estimator_

best_params

best_model

# Use the best model to make predictions on the test set
y_pred = best_model.predict(X_test)

# Evaluate the model on the test set
test_mse = mean_squared_error(y_test, y_pred)
test_r2 = r2_score(y_test, y_pred)

print('Testing Set - Mean Squared Error:', test_mse)
print('Testing Set - R-squared:', test_r2)

# Visualize actual vs predicted values
plt.scatter(y_test, y_pred)
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs Predicted Values on Test Set')
plt.show()

import matplotlib.pyplot as plt
import textwrap
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Assuming you have X_train, X_test, y_train, y_test defined
# Replace this with your actual data

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and fit the RandomForestRegressor with the best parameters
best_rf_model = RandomForestRegressor(bootstrap=False, max_depth=90, max_features=2,
                                      min_samples_leaf=2, n_estimators=200)

best_rf_model.fit(X_train, y_train)

# Make predictions
y_pred = best_rf_model.predict(X_test)

# Calculate R-squared and MSE
r2 = r2_score(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)

# Plotting
fig, ax = plt.subplots()

# Scatter plot
ax.scatter(y_test, y_pred, color='blue', label='Actual vs Predicted')

# Add the legend to top-right
ax.legend(loc='lower right')

# Axis labels with wrapping to prevent overlapping
x_axis_label = "Actual Values"
wrapped_x_label = '\n'.join(textwrap.wrap(x_axis_label, width=10))  # Adjust width as needed
ax.set_xlabel(wrapped_x_label, fontsize=12)

y_axis_label = "Predicted Values"
wrapped_y_label = '\n'.join(textwrap.wrap(y_axis_label, width=10))  # Adjust width as needed
ax.set_ylabel(wrapped_y_label, fontsize=12)

# Title with R-squared and MSE information
ax.set_title(f'Actual vs Predicted Values\nR2 = {r2:.4f}, MSE = {mse:.2f}', fontsize=16)

plt.show()

import matplotlib.pyplot as plt
import textwrap
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Assuming you have X_train, X_test, y_train, y_test defined
# Replace this with your actual data

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and fit the RandomForestRegressor with the best parameters
best_rf_model = RandomForestRegressor(bootstrap=False, max_depth=90, max_features=2,
                                      min_samples_leaf=2, n_estimators=200)

best_rf_model.fit(X_train, y_train)

# Make predictions
y_pred = best_rf_model.predict(X_test)

# Calculate R-squared and MSE
r2 = r2_score(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)

# Plotting
fig, ax = plt.subplots()

# Scatter plot with different colors for predicted and actual values
ax.scatter(y_test, y_pred, color='blue', label='Actual Values')
ax.scatter(y_test, y_pred, color='red', label='Predicted Values')

# Add the legend to top-right
ax.legend(loc='lower right')

# Axis labels with wrapping to prevent overlapping
x_axis_label = "Actual Values"
wrapped_x_label = '\n'.join(textwrap.wrap(x_axis_label, width=10))  # Adjust width as needed
ax.set_xlabel(wrapped_x_label, fontsize=12)

y_axis_label = "Predicted Values"
wrapped_y_label = '\n'.join(textwrap.wrap(y_axis_label, width=10))  # Adjust width as needed
ax.set_ylabel(wrapped_y_label, fontsize=12)

# Title with R-squared and MSE information
ax.set_title(f'Actual vs Predicted Values\nR2 = {r2:.4f}, MSE = {mse:.2f}', fontsize=16)

plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Assuming you have X_train, X_test, y_train, y_test defined
# Replace this with your actual data

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and fit the RandomForestRegressor with the best parameters
best_rf_model = RandomForestRegressor(bootstrap=False, max_depth=90, max_features=2,
                                      min_samples_leaf=2, n_estimators=200)

best_rf_model.fit(X_train, y_train)

# Make predictions
y_pred = best_rf_model.predict(X_test)

# Calculate R-squared and MSE
r2 = r2_score(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)

# Plotting
plt.figure(figsize=(8, 6))

# Scatter plot with legends and different colors for predicted and actual values
sns.regplot(x=y_test, y=y_pred, scatter_kws={'color': 'purple', 'label': 'Actual vs Predicted'},
            line_kws={'color': 'red', 'label': 'Trendline'})

# Axis labels
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')

# Title with R-squared and MSE information
plt.title(f'Actual vs Predicted Values\nR2 = {r2:.4f}, MSE = {mse:.2f}', fontsize=16)

# Legends
plt.legend()

plt.show()

from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestRegressor

# Replace RandomForestRegressor with your chosen model
model = RandomForestRegressor(bootstrap=False, max_depth=90, max_features=2,
                               min_samples_leaf=2, n_estimators=200)

# Choose the number of folds (e.g., 5-fold cross-validation)
num_folds = 5

# Perform cross-validation and get R-squared scores
r2_scores = cross_val_score(model, X_train, y_train, cv=num_folds, scoring='r2')

# Print the R-squared scores for training and validation folds
for fold, r2 in enumerate(r2_scores, 1):
    print(f'Fold {fold}: R-squared - Training: {r2:.4f}')

from sklearn.metrics import mean_squared_error

# Assuming you have X_test and y_test defined
# Replace this with your actual test data

# Make predictions on the test set
y_pred_test = best_rf_model.predict(X_test)

# Calculate R-squared and MSE on the test set
r2_test = r2_score(y_test, y_pred_test)
mse_test = mean_squared_error(y_test, y_pred_test)

# Print the results
print(f'Test Set - R-squared: {r2_test:.4f}')
print(f'Test Set - Mean Squared Error: {mse_test:.2f}')

"""## **Forecast 4 countries emitting CO2**"""

# Assuming 'df' has columns 'year', 'Entity', and 'Value_co2_emissions_kt_by_country'

# Group by 'Entity' and sum 'Value_co2_emissions_kt_by_country' across all years
total_co2_by_Entity = df.groupby(['Entity']).sum()

# Find the Entity with the highest total CO2 emission
highest_emission_Entity = total_co2_by_Entity['Value_co2_emissions_kt_by_country'].idxmax()

# Display the Entity with the highest CO2 emission
print(f"The Entity with the highest total CO2 emission is: {highest_emission_Entity}")

import matplotlib.pyplot as plt

# Assuming 'df' has columns 'Year', 'Entity', and 'Value_co2_emissions_kt_by_country'

# Group by 'Entity' and sum 'Value_co2_emissions_kt_by_country' across all Years
total_co2_by_Entity = df.groupby(['Entity']).sum()

# Find the top 5 countries with the highest total CO2 emissions
top_countries = total_co2_by_Entity.nlargest(5, 'Value_co2_emissions_kt_by_country')

# Plot the bar chart for the top 5 countries
plt.figure(figsize=(12, 6))
plt.bar(top_countries.index, top_countries['Value_co2_emissions_kt_by_country'], color='skyblue')
plt.xlabel('Entity')
plt.ylabel('Total CO2 Emission')
plt.title('Top 5 Countries with Highest CO2 Emission')
plt.xticks(rotation=0, ha='right')  # Rotate x-axis labels for better readability
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Assuming 'df' has columns 'Year', 'Entity', and 'Value_co2_emissions_kt_by_country'

# Group by 'Entity' and sum 'Value_co2_emissions_kt_by_country' across all Years
total_co2_by_Entity = df.groupby(['Entity']).sum()

# Select the top 4 countries including China, United States, India, and the UK
countries_of_interest = ['China', 'United States', 'India', 'United Kingdom']
top_countries = total_co2_by_Entity[total_co2_by_Entity.index.isin(countries_of_interest)]

# Order the countries
order = ['China', 'United States', 'India', 'United Kingdom']
top_countries = top_countries.loc[order]

# Plot the bar chart for the top 4 countries
plt.figure(figsize=(12, 6))
plt.bar(top_countries.index, top_countries['Value_co2_emissions_kt_by_country'], color='skyblue')
plt.xlabel('Entity')
plt.ylabel('Total CO2 Emission')
plt.title('Countries with CO2 Emission')
plt.xticks(rotation=30, ha='right')  # Rotate x-axis labels for better readability
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor

# Assuming 'df' has columns 'Year', 'Entity', and 'Value_co2_emissions_kt_by_country'
# Make sure your 'Year' column is in datetime format

# Filter data for the top 4 countries
countries_of_interest = ['China', 'United States', 'India', 'United Kingdom']

# Create subplots for each country
fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 10))
fig.suptitle('Actual vs Predicted CO2 Emission for Top 4 Countries', fontsize=16)

for country, ax in zip(countries_of_interest, axes.flatten()):
    # Extract data for the specific country
    country_data = df[df['Entity'] == country]

    # Prepare features and target variable
    X = country_data[['Year']]
    y = country_data['Value_co2_emissions_kt_by_country']

    # Create and fit the best model (replace with your selected model)
    best_model = RandomForestRegressor(bootstrap=False, max_depth=90, max_features=2,
                                       min_samples_leaf=2, n_estimators=200)
    best_model.fit(X, y)

    # Create a DataFrame for the years 2000 to 2020
    years_2000_to_2020 = pd.date_range(start='2000-01-01', end='2020-01-01', freq='AS')
    years_2000_to_2020_df = pd.DataFrame({'Year': years_2000_to_2020})

    # Predict CO2 emissions for the years 2000 to 2020
    years_2000_to_2020_df['Predicted_CO2'] = best_model.predict(years_2000_to_2020_df[['Year']])

    # Evaluate the model
    r2 = best_model.score(X, y)
    mse = mean_squared_error(y, best_model.predict(X))

    # Plot actual vs predicted values for historical data
    ax.plot(X['Year'], y, label='Actual', marker='o')
    ax.plot(years_2000_to_2020_df['Year'], years_2000_to_2020_df['Predicted_CO2'], label='Predicted', marker='o')

    # Prepare features for the next 5 years (2021 to 2025)
    years_2021_to_2025 = pd.date_range(start='2021-01-01', end='2025-01-01', freq='AS')
    years_2021_to_2025_df = pd.DataFrame({'Year': years_2021_to_2025})

    # Predict CO2 emissions for the next 5 years
    years_2021_to_2025_df['Predicted_CO2'] = best_model.predict(years_2021_to_2025_df[['Year']])

    # Plot predicted values for the next 5 years
    ax.plot(years_2021_to_2025_df['Year'], years_2021_to_2025_df['Predicted_CO2'], label='Forecasted', marker='o')

    ax.set_xlabel('Year')
    ax.set_ylabel('Total CO2 Emission')
    ax.set_title(f'{country} - R2: {r2:.4f}, MSE: {mse:.2f}')
    ax.legend()

plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from math import sqrt

# Assuming 'df' has columns 'Year', 'Entity', and 'Value_co2_emissions_kt_by_country'
# Make sure your 'Year' column is in datetime format

# Filter data for the top 4 countries
countries_of_interest = ['China', 'United States', 'India', 'United Kingdom']

# Create subplots for each country
fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 10))
fig.suptitle('Actual vs Predicted CO2 Emission for Top 4 Countries', fontsize=16)

for country, ax in zip(countries_of_interest, axes.flatten()):
    # Extract data for the specific country
    country_data = df[df['Entity'] == country]

    # Prepare features and target variable
    X = country_data[['Year']]
    y = country_data['Value_co2_emissions_kt_by_country']

    # Create and fit the best model (replace with your selected model)
    best_model = RandomForestRegressor(bootstrap=False, max_depth=90, max_features=2,
                                       min_samples_leaf=2, n_estimators=200)
    best_model.fit(X, y)

    # Create a DataFrame for the years 2000 to 2020
    years_2000_to_2020 = pd.date_range(start='2000-01-01', end='2020-01-01', freq='AS')
    years_2000_to_2020_df = pd.DataFrame({'Year': years_2000_to_2020})

    # Predict CO2 emissions for the years 2000 to 2020
    years_2000_to_2020_df['Predicted_CO2'] = best_model.predict(years_2000_to_2020_df[['Year']])

    # Evaluate the model
    r2 = best_model.score(X, y)
    mse = mean_squared_error(y, best_model.predict(X))

    # Plot actual vs predicted values for historical data
    ax.plot(X['Year'], y, label='Actual', marker='o')
    ax.plot(years_2000_to_2020_df['Year'], years_2000_to_2020_df['Predicted_CO2'], label='Predicted', marker='o')

    # Prepare features for the next 5 years (2021 to 2025)
    years_2021_to_2025 = pd.date_range(start='2021-01-01', end='2025-01-01', freq='AS')
    years_2021_to_2025_df = pd.DataFrame({'Year': years_2021_to_2025})

    # Predict CO2 emissions for the next 5 years
    years_2021_to_2025_df['Predicted_CO2'] = best_model.predict(years_2021_to_2025_df[['Year']])

    # Plot predicted values for the next 5 years
    ax.plot(years_2021_to_2025_df['Year'], years_2021_to_2025_df['Predicted_CO2'], label='Forecasted', marker='o')

    ax.set_xlabel('Year')
    ax.set_ylabel('CO2 Emission (kt)')
    ax.set_title(f'{country} - R2: {r2:.4f}, MSE: {mse:.2f}')
    ax.legend()

plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

df.dtypes

df

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from math import sqrt

# Assuming 'df' has columns 'Year', 'Entity', and 'Value_co2_emissions_kt_by_country'
# Make sure your 'Year' column is in integer format

# Filter data for the top 4 countries
countries_of_interest = ['China', 'United States', 'India', 'United Kingdom']

# Create subplots for each country
fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 10))
fig.suptitle('Actual vs Predicted CO2 Emission for Top 4 Countries', fontsize=16)

for country, ax in zip(countries_of_interest, axes.flatten()):
    # Extract data for the specific country
    country_data = df[df['Entity'] == country]

    # Prepare features and target variable
    X = country_data[['Year']]
    y = country_data['Value_co2_emissions_kt_by_country']

    # Create and fit the best model (replace with your selected model)
    best_model = RandomForestRegressor(bootstrap=False, max_depth=90, max_features=2,
                                       min_samples_leaf=2, n_estimators=200)
    best_model.fit(X, y)

    # Create a DataFrame for the years 2000 to 2020
    years_2000_to_2020_df = pd.DataFrame({'Year': range(2000, 2021)})

    # Predict CO2 emissions for the years 2000 to 2020
    years_2000_to_2020_df['Predicted_CO2'] = best_model.predict(years_2000_to_2020_df[['Year']])

    # Evaluate the model
    r2 = best_model.score(X, y)
    mse = mean_squared_error(y, best_model.predict(X))

    # Plot actual vs predicted values for historical data
    ax.plot(X['Year'], y, label='Actual', marker='o')
    ax.plot(years_2000_to_2020_df['Year'], years_2000_to_2020_df['Predicted_CO2'], label='Predicted', marker='o')

    # Prepare features for the next 5 years (2021 to 2025)
    years_2021_to_2025_df = pd.DataFrame({'Year': range(2021, 2026)})

    # Predict CO2 emissions for the next 5 years
    years_2021_to_2025_df['Predicted_CO2'] = best_model.predict(years_2021_to_2025_df[['Year']])

    # Plot predicted values for the next 5 years
    ax.plot(years_2021_to_2025_df['Year'], years_2021_to_2025_df['Predicted_CO2'], label='Forecasted', marker='o')

    ax.set_xlabel('Year')
    ax.set_ylabel('CO2 Emission (kt)')
    ax.set_title(f'{country} - R2: {r2:.4f}, MSE: {mse:.2f}')
    ax.legend()

plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from math import sqrt

# Assuming 'df' has columns 'Year', 'Entity', and 'Value_co2_emissions_kt_by_country',
# 'Electricity from fossil fuels (TWh)', 'Electricity from nuclear (TWh)', 'Electricity from renewables (TWh)'
# Make sure your 'Year' column is in integer format

# Filter data for the top 4 countries
countries_of_interest = ['China', 'United States', 'India', 'United Kingdom']

# Create subplots for each country
fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 10))
fig.suptitle('Actual vs Predicted CO2 Emission for Top 4 Countries', fontsize=16)

for country, ax in zip(countries_of_interest, axes.flatten()):
    # Extract data for the specific country
    country_data = df[df['Entity'] == country]

    # Prepare features and target variable
    X = country_data[['Year', 'Electricity from fossil fuels (TWh)', 'Electricity from nuclear (TWh)', 'Electricity from renewables (TWh)']]
    y = country_data['Value_co2_emissions_kt_by_country']

    # Create and fit the best model (replace with your selected model)
    best_model = RandomForestRegressor(
        bootstrap=True,
        max_depth=10,  # Adjust as needed
        max_features='auto',  # Try 'sqrt', 'log2', or an integer
        min_samples_leaf=1,
        n_estimators=100
    )
    best_model.fit(X, y)

    # Create a DataFrame for the years 2000 to 2025
    years_2000_to_2025_df = pd.DataFrame({'Year': range(2000, 2026),
                                          'Electricity from fossil fuels (TWh)': 0,  # Provide relevant values
                                          'Electricity from nuclear (TWh)': 0,
                                          'Electricity from renewables (TWh)': 0})

    # Predict CO2 emissions for the years 2000 to 2025
    years_2000_to_2025_df['Predicted_CO2'] = best_model.predict(years_2000_to_2025_df[['Year',
                                                                                        'Electricity from fossil fuels (TWh)',
                                                                                        'Electricity from nuclear (TWh)',
                                                                                        'Electricity from renewables (TWh)']])

    # Evaluate the model
    r2 = best_model.score(X, y)
    mse = mean_squared_error(y, best_model.predict(X))

    # Plot actual vs predicted values for historical and forecasted data
    ax.plot(country_data['Year'], y, label='Actual', marker='o')
    ax.plot(years_2000_to_2025_df['Year'], years_2000_to_2025_df['Predicted_CO2'], label='Predicted', marker='o')

    ax.set_xlabel('Year')
    ax.set_ylabel('CO2 Emission (kt)')
    ax.set_title(f'{country} - R2: {r2:.4f}, MSE: {mse:.2f}')
    ax.legend()

plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from math import sqrt

# Assuming 'df' has columns 'Year' and 'Value_co2_emissions_kt_by_country'
# Make sure your 'Year' column is in integer format

# Aggregate data for the total CO2 emissions of the world
world_data = df.groupby('Year')['Value_co2_emissions_kt_by_country'].sum().reset_index()

# Prepare features and target variable
X = world_data[['Year']]
y = world_data['Value_co2_emissions_kt_by_country']

# Create and fit the best model (replace with your selected model)
best_model = RandomForestRegressor(bootstrap=False, max_depth=90, max_features=2,
                                   min_samples_leaf=2, n_estimators=200)
best_model.fit(X, y)

# Create a DataFrame for the years 2000 to 2020
years_2000_to_2020_df = pd.DataFrame({'Year': range(2000, 2021)})

# Predict CO2 emissions for the years 2000 to 2020
years_2000_to_2020_df['Predicted_CO2'] = best_model.predict(years_2000_to_2020_df[['Year']])

# Evaluate the model
r2 = best_model.score(X, y)
mse = mean_squared_error(y, best_model.predict(X))

# Plot actual vs predicted values for historical data
plt.plot(X['Year'], y, label='Actual', marker='o')
plt.plot(years_2000_to_2020_df['Year'], years_2000_to_2020_df['Predicted_CO2'], label='Predicted', marker='o')

# Prepare features for the next 5 years (2021 to 2025)
years_2021_to_2025_df = pd.DataFrame({'Year': range(2021, 2026)})

# Predict CO2 emissions for the next 5 years
years_2021_to_2025_df['Predicted_CO2'] = best_model.predict(years_2021_to_2025_df[['Year']])

# Plot predicted values for the next 5 years
plt.plot(years_2021_to_2025_df['Year'], years_2021_to_2025_df['Predicted_CO2'], label='Forecasted', marker='o')

plt.xlabel('Year')
plt.ylabel('CO2 Emission (kt)')
plt.title(f'World - R2: {r2:.4f}, MSE: {mse:.2f}')
plt.legend()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import mean_squared_error
from math import sqrt

# Assuming 'df' has columns 'Year' and 'Value_co2_emissions_kt_by_country'
# Make sure your 'Year' column is in integer format

# Aggregate data for the total CO2 emissions of the world
world_data = df.groupby('Year')['Value_co2_emissions_kt_by_country'].sum().reset_index()

# Prepare features and target variable
X = world_data[['Year']]
y = world_data['Value_co2_emissions_kt_by_country']

# Define the parameter grid for Randomized Search
param_grid = {
    'n_estimators': [int(x) for x in range(100, 1001, 100)],
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_depth': [int(x) for x in range(10, 110, 10)],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create and fit the RandomizedSearchCV model
rf_model = RandomForestRegressor()
random_search = RandomizedSearchCV(estimator=rf_model, param_distributions=param_grid,
                                   n_iter=100, scoring='neg_mean_squared_error', cv=5,
                                   random_state=42, n_jobs=-1)
random_search.fit(X, y)

# Get the best model from the random search
best_model = random_search.best_estimator_

# Create a DataFrame for the years 2000 to 2020
years_2000_to_2020_df = pd.DataFrame({'Year': range(2000, 2021)})

# Predict CO2 emissions for the years 2000 to 2020
years_2000_to_2020_df['Predicted_CO2'] = best_model.predict(years_2000_to_2020_df[['Year']])

# Evaluate the model
r2 = best_model.score(X, y)
mse = mean_squared_error(y, best_model.predict(X))

# Plot actual vs predicted values for historical data
plt.plot(X['Year'], y, label='Actual', marker='o')
plt.plot(years_2000_to_2020_df['Year'], years_2000_to_2020_df['Predicted_CO2'], label='Predicted', marker='o')

# Prepare features for the next 5 years (2021 to 2025)
years_2021_to_2025_df = pd.DataFrame({'Year': range(2021, 2026)})

# Predict CO2 emissions for the next 5 years
years_2021_to_2025_df['Predicted_CO2'] = best_model.predict(years_2021_to_2025_df[['Year']])

# Plot predicted values for the next 5 years
plt.plot(years_2021_to_2025_df['Year'], years_2021_to_2025_df['Predicted_CO2'], label='Forecasted', marker='o')

plt.xlabel('Year')
plt.ylabel('CO2 Emission (kt)')
plt.title(f'World - R2: {r2:.4f}, MSE: {mse:.2f}')
plt.legend()
plt.show()

# Display the best hyperparameters found by Randomized Search
print("Best Hyperparameters:", random_search.best_params_)

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import mean_squared_error
from math import sqrt

# Assuming 'df' has columns 'Year' and 'Value_co2_emissions_kt_by_country'
# Make sure your 'Year' column is in integer format

# Aggregate data for the total CO2 emissions of the world
world_data = df.groupby('Year')['Value_co2_emissions_kt_by_country'].sum().reset_index()

# Prepare features and target variable
X = world_data[['Year']]
y = world_data['Value_co2_emissions_kt_by_country']

# Define the parameter grid for Randomized Search
param_grid = {
    'n_estimators': [int(x) for x in range(100, 1001, 100)],
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_depth': [int(x) for x in range(10, 110, 10)],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create and fit the RandomizedSearchCV model
rf_model = RandomForestRegressor()
random_search = RandomizedSearchCV(estimator=rf_model, param_distributions=param_grid,
                                   n_iter=100, scoring='neg_mean_squared_error', cv=5,
                                   random_state=42, n_jobs=-1)
random_search.fit(X, y)

# Get the best model from the random search
best_model = random_search.best_estimator_

# Create a DataFrame for the years 2000 to 2020
years_2000_to_2020_df = pd.DataFrame({'Year': range(2000, 2021)})

# Predict CO2 emissions for the years 2000 to 2020
years_2000_to_2020_df['Predicted_CO2'] = best_model.predict(years_2000_to_2020_df[['Year']])

# Evaluate the model
r2 = best_model.score(X, y)
mse = mean_squared_error(y, best_model.predict(X))

# Plot actual vs predicted values for historical data
plt.plot(X['Year'], y, label='Actual', marker='o')
plt.plot(years_2000_to_2020_df['Year'], years_2000_to_2020_df['Predicted_CO2'], label='Predicted', marker='o')

# Prepare features for the next 5 years (2021 to 2025)
years_2021_to_2025_df = pd.DataFrame({'Year': range(2021, 2026)})

# Predict CO2 emissions for the next 5 years
years_2021_to_2025_df['Predicted_CO2'] = best_model.predict(years_2021_to_2025_df[['Year']])

# Plot predicted values for the next 5 years
plt.plot(years_2021_to_2025_df['Year'], years_2021_to_2025_df['Predicted_CO2'], label='Forecasted', marker='o')

plt.xlabel('Year')
plt.ylabel('CO2 Emission (kt)')
plt.title(f'World - R2: {r2:.4f}, MSE: {mse:.2f}')
plt.legend()
plt.show()

# Display the best hyperparameters found by Randomized Search
print("Best Hyperparameters:", random_search.best_params_)

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import mean_squared_error
from math import sqrt



# Aggregate data for the total CO2 emissions of the world
world_data = df.groupby('Year')['Value_co2_emissions_kt_by_country'].sum().reset_index()

# Prepare features and target variable
X = world_data[['Year']]
y = world_data['Value_co2_emissions_kt_by_country']

# Define the parameter grid for Randomized Search
param_grid = {
    'n_estimators': [int(x) for x in range(100, 1001, 100)],
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_depth': [int(x) for x in range(10, 110, 10)],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create and fit the RandomizedSearchCV model
rf_model = RandomForestRegressor()
random_search = RandomizedSearchCV(estimator=rf_model, param_distributions=param_grid,
                                   n_iter=100, scoring='neg_mean_squared_error', cv=5,
                                   random_state=42, n_jobs=-1)
random_search.fit(X, y)

# Get the best model from the random search
best_model = random_search.best_estimator_

# Calculate the growth over the last 3 years
last_3_years_growth_rate = (y.iloc[-1] / y.iloc[-4]) ** (1/3) - 1

# Forecast CO2 emissions for the next 3 years
forecast_years = range(world_data['Year'].max() + 1, world_data['Year'].max() + 4)
forecast_values = [y.iloc[-1] * (1 + last_3_years_growth_rate) ** i for i in range(1, 4)]

# Combine historical and forecasted data
forecast_df = pd.DataFrame({'Year': forecast_years, 'Value_co2_emissions_kt_by_country': forecast_values})
combined_data = pd.concat([world_data, forecast_df], ignore_index=True)

# Plot actual vs predicted values
plt.plot(combined_data['Year'], combined_data['Value_co2_emissions_kt_by_country'], label='Actual and Forecasted', marker='o')
plt.xlabel('Year')
plt.ylabel('CO2 Emission (kt)')
plt.title('World - Actual and Forecasted CO2 Emissions')
plt.legend()
plt.show()

# Display the best hyperparameters found by Randomized Search
print("Best Hyperparameters:", random_search.best_params_)

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import mean_squared_error
from math import sqrt

# Assuming 'df' has columns 'Year' and 'Value_co2_emissions_kt_by_country'
# Make sure your 'Year' column is in integer format

# Aggregate data for the total CO2 emissions of the world
world_data = df.groupby('Year')['Value_co2_emissions_kt_by_country'].sum().reset_index()

# Prepare features and target variable
X = world_data[['Year']]
y = world_data['Value_co2_emissions_kt_by_country']

# Define the parameter grid for Randomized Search
param_grid = {
    'n_estimators': [int(x) for x in range(100, 1001, 100)],
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_depth': [int(x) for x in range(10, 110, 10)],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create and fit the RandomizedSearchCV model
rf_model = RandomForestRegressor()
random_search = RandomizedSearchCV(estimator=rf_model, param_distributions=param_grid,
                                   n_iter=100, scoring='neg_mean_squared_error', cv=5,
                                   random_state=42, n_jobs=-1)
random_search.fit(X, y)

# Get the best model from the random search
best_model = random_search.best_estimator_

# Calculate the growth over the last 3 years
last_3_years_growth_rate = (y.iloc[-1] / y.iloc[-4]) ** (1/3) - 1

# Forecast CO2 emissions for the next 3 years
forecast_years = range(world_data['Year'].max() + 1, world_data['Year'].max() + 4)
forecast_values = [y.iloc[-1] * (1 + last_3_years_growth_rate) ** i for i in range(1, 4)]

# Combine historical and forecasted data
forecast_df = pd.DataFrame({'Year': forecast_years, 'Value_co2_emissions_kt_by_country': forecast_values})
combined_data = pd.concat([world_data, forecast_df], ignore_index=True)

# Plot actual and forecasted values
plt.plot(combined_data['Year'], combined_data['Value_co2_emissions_kt_by_country'], label='Actual', marker='o', color='blue')
plt.plot(forecast_df['Year'], forecast_df['Value_co2_emissions_kt_by_country'], label='Forecasted', marker='o', linestyle='dashed', color='orange')
plt.xlabel('Year')
plt.ylabel('CO2 Emission (kt)')
plt.title('World - Actual and Forecasted CO2 Emissions')
plt.legend()
plt.show()

# Display the best hyperparameters found by Randomized Search
print("Best Hyperparameters:", random_search.best_params_)

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import mean_squared_error
from math import sqrt

# Assuming 'df' has columns 'Year' and 'Value_co2_emissions_kt_by_country'
# Make sure your 'Year' column is in integer format

# Aggregate data for world total CO2 emissions
world_data = df.groupby('Year')['Value_co2_emissions_kt_by_country'].sum().reset_index()

# Filter data for years up to 2020
historical_data = world_data[world_data['Year'] <= 2020]

# Prepare features and target variable for historical data
X_hist = historical_data[['Year']]
y_hist = historical_data['Value_co2_emissions_kt_by_country']

# Define the parameter grid for Randomized Search
param_grid = {
    'n_estimators': [int(x) for x in range(100, 1001, 100)],
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_depth': [int(x) for x in range(10, 110, 10)],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create and fit the RandomizedSearchCV model for historical data
rf_model_hist = RandomForestRegressor()
random_search_hist = RandomizedSearchCV(estimator=rf_model_hist, param_distributions=param_grid,
                                        n_iter=100, scoring='neg_mean_squared_error', cv=5,
                                        random_state=42, n_jobs=-1)
random_search_hist.fit(X_hist, y_hist)

# Get the best model from the random search for historical data
best_model_hist = random_search_hist.best_estimator_

# Predict CO2 emissions for historical data
y_hist_pred = best_model_hist.predict(X_hist)

# Calculate the growth over the last 3 years
last_3_years_growth_rate = (y_hist.iloc[-1] / y_hist.iloc[-4]) ** (1/3) - 1

# Forecast CO2 emissions for the next 5 years
forecast_years = range(2021, 2026)
forecast_values = [y_hist.iloc[-1] * (1 + last_3_years_growth_rate) ** (i - 2021) for i in forecast_years]

# Predict CO2 emissions for the forecasted years
X_forecast = pd.DataFrame({'Year': forecast_years})
y_forecast_pred = best_model_hist.predict(X_forecast)

# Create DataFrames for actual, predicted, and forecasted values
actual_df = pd.DataFrame({'Year': X_hist['Year'], 'Actual': y_hist})
predicted_df = pd.DataFrame({'Year': X_hist['Year'], 'Predicted': y_hist_pred})
forecasted_df = pd.DataFrame({'Year': forecast_years, 'Forecasted': forecast_values})

# Merge DataFrames on 'Year' to create a single DataFrame
result_df = pd.merge(actual_df, predicted_df, on='Year', how='outer')
result_df = pd.merge(result_df, forecasted_df, on='Year', how='outer')

# Plot actual, predicted, and forecasted values
plt.plot(result_df['Year'], result_df['Actual'], label='Actual', marker='o', color='blue')
plt.plot(result_df['Year'], result_df['Predicted'], label='Predicted', marker='o', linestyle='dashed', color='green')
plt.plot(result_df['Year'], result_df['Forecasted'], label='Forecasted', marker='o', linestyle='dashed', color='orange')
plt.xlabel('Year')
plt.ylabel('CO2 Emission (kt)')
plt.title('World - Actual, Predicted, and Forecasted CO2 Emissions')
plt.legend()
plt.show()

# Display the best hyperparameters found by Randomized Search for historical data
print("Best Hyperparameters for Historical Data:", random_search_hist.best_params_)

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from math import sqrt

# Assuming 'df' has columns 'Year' and 'Value_co2_emissions_kt_by_country'
# Make sure your 'Year' column is in integer format

# Aggregate data for world total CO2 emissions
world_data = df.groupby('Year')['Value_co2_emissions_kt_by_country'].sum().reset_index()

# Filter data for years up to 2020
historical_data = world_data[world_data['Year'] <= 2020]

# Prepare features and target variable for historical data
X_hist = historical_data[['Year']]
y_hist = historical_data['Value_co2_emissions_kt_by_country']

# Create and fit the model with the specified hyperparameters
best_model_hist = RandomForestRegressor(
    n_estimators=100,
    min_samples_split=2,
    min_samples_leaf=1,
    max_features='auto',
    max_depth=50
)
best_model_hist.fit(X_hist, y_hist)

# Predict CO2 emissions for historical data
y_hist_pred = best_model_hist.predict(X_hist)

# Calculate the growth over the last 3 years
last_3_years_growth_rate = (y_hist.iloc[-1] / y_hist.iloc[-4]) ** (1/3) - 1

# Forecast CO2 emissions for the next 5 years
forecast_years = range(2021, 2026)
forecast_values = [y_hist.iloc[-1] * (1 + last_3_years_growth_rate) ** (i - 2021) for i in forecast_years]

# Predict CO2 emissions for the forecasted years
X_forecast = pd.DataFrame({'Year': forecast_years})
y_forecast_pred = best_model_hist.predict(X_forecast)

# Create DataFrames for actual, predicted, and forecasted values
actual_df = pd.DataFrame({'Year': X_hist['Year'], 'Actual': y_hist})
predicted_df = pd.DataFrame({'Year': X_hist['Year'], 'Predicted': y_hist_pred})
forecasted_df = pd.DataFrame({'Year': forecast_years, 'Forecasted': forecast_values})

# Merge DataFrames on 'Year' to create a single DataFrame
result_df = pd.merge(actual_df, predicted_df, on='Year', how='outer')
result_df = pd.merge(result_df, forecasted_df, on='Year', how='outer')

# Plot actual, predicted, and forecasted values
plt.plot(result_df['Year'], result_df['Actual'], label='Actual', marker='o', color='blue')
plt.plot(result_df['Year'], result_df['Predicted'], label='Predicted', marker='o', linestyle='dashed', color='green')
plt.plot(result_df['Year'], result_df['Forecasted'], label='Forecasted', marker='o', linestyle='dashed', color='orange')
plt.xlabel('Year')
plt.ylabel('CO2 Emission (kt)')
plt.title('World - Actual, Predicted, and Forecasted CO2 Emissions')
plt.legend()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from math import sqrt

# Assuming 'df' has columns 'Year' and 'Value_co2_emissions_kt_by_country'
# Make sure your 'Year' column is in integer format

# Aggregate data for world total CO2 emissions
world_data = df.groupby('Year')['Value_co2_emissions_kt_by_country'].sum().reset_index()

# Filter data for years up to 2020
historical_data = world_data[world_data['Year'] <= 2020]

# Prepare features and target variable for historical data
X_hist = historical_data[['Year']]
y_hist = historical_data['Value_co2_emissions_kt_by_country']

# Create and fit the model with the specified hyperparameters
best_model_hist = RandomForestRegressor(
    n_estimators=100,
    min_samples_split=2,
    min_samples_leaf=1,
    max_features='auto',
    max_depth=50
)
best_model_hist.fit(X_hist, y_hist)

# Predict CO2 emissions for historical data
y_hist_pred = best_model_hist.predict(X_hist)

# Calculate the 5-year growth rate
last_5_years_growth_rate = (y_hist.iloc[-1] / y_hist.iloc[-6]) ** (1/5) - 1

# Forecast CO2 emissions for the next 5 years
forecast_years = range(2021, 2026)
forecast_values = [y_hist.iloc[-1] * (1 + last_5_years_growth_rate) ** (i - 2021) for i in forecast_years]

# Predict CO2 emissions for the forecasted years
X_forecast = pd.DataFrame({'Year': forecast_years})
y_forecast_pred = best_model_hist.predict(X_forecast)

# Create DataFrames for actual, predicted, and forecasted values
actual_df = pd.DataFrame({'Year': X_hist['Year'], 'Actual': y_hist})
predicted_df = pd.DataFrame({'Year': X_hist['Year'], 'Predicted': y_hist_pred})
forecasted_df = pd.DataFrame({'Year': forecast_years, 'Forecasted': forecast_values})

# Merge DataFrames on 'Year' to create a single DataFrame
result_df = pd.merge(actual_df, predicted_df, on='Year', how='outer')
result_df = pd.merge(result_df, forecasted_df, on='Year', how='outer')

# Plot actual, predicted, and forecasted values
plt.plot(result_df['Year'], result_df['Actual'], label='Actual', marker='o', color='blue')
plt.plot(result_df['Year'], result_df['Predicted'], label='Predicted', marker='o', linestyle='dashed', color='green')
plt.plot(result_df['Year'], result_df['Forecasted'], label='Forecasted', marker='o', linestyle='dashed', color='orange')
plt.xlabel('Year')
plt.ylabel('CO2 Emission (kt)')
plt.title('World - Actual, Predicted, and Forecasted CO2 Emissions')
plt.legend()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from math import sqrt

# Assuming 'df' has columns 'Year' and 'Value_co2_emissions_kt_by_country'
# Make sure your 'Year' column is in integer format

# Aggregate data for world total CO2 emissions
world_data = df.groupby('Year')['Value_co2_emissions_kt_by_country'].sum().reset_index()

# Filter data for years up to 2020
historical_data = world_data[world_data['Year'] <= 2020]

# Prepare features and target variable for historical data
X_hist = historical_data[['Year']]
y_hist = historical_data['Value_co2_emissions_kt_by_country']

# Create and fit the model with the specified hyperparameters
best_model_hist = RandomForestRegressor(
    n_estimators=100,
    min_samples_split=2,
    min_samples_leaf=1,
    max_features='auto',
    max_depth=50
)
best_model_hist.fit(X_hist, y_hist)

# Predict CO2 emissions for historical data
y_hist_pred = best_model_hist.predict(X_hist)

# Calculate the 10-year growth rate
last_10_years_growth_rate = (y_hist.iloc[-1] / y_hist.iloc[-11]) ** (1/10) - 1

# Forecast CO2 emissions for the next 5 years
forecast_years = range(2021, 2026)
forecast_values = [y_hist.iloc[-1] * (1 + last_10_years_growth_rate) ** (i - 2021) for i in forecast_years]

# Predict CO2 emissions for the forecasted years
X_forecast = pd.DataFrame({'Year': forecast_years})
y_forecast_pred = best_model_hist.predict(X_forecast)

# Create DataFrames for actual, predicted, and forecasted values
actual_df = pd.DataFrame({'Year': X_hist['Year'], 'Actual': y_hist})
predicted_df = pd.DataFrame({'Year': X_hist['Year'], 'Predicted': y_hist_pred})
forecasted_df = pd.DataFrame({'Year': forecast_years, 'Forecasted': forecast_values})

# Merge DataFrames on 'Year' to create a single DataFrame
result_df = pd.merge(actual_df, predicted_df, on='Year', how='outer')
result_df = pd.merge(result_df, forecasted_df, on='Year', how='outer')

# Plot actual, predicted, and forecasted values
plt.plot(result_df['Year'], result_df['Actual'], label='Actual', marker='o', color='blue')
plt.plot(result_df['Year'], result_df['Predicted'], label='Predicted', marker='o', linestyle='dashed', color='green')
plt.plot(result_df['Year'], result_df['Forecasted'], label='Forecasted', marker='o', linestyle='dashed', color='orange')
plt.xlabel('Year')
plt.ylabel('CO2 Emission (kt)')
plt.title('World - Actual, Predicted, and Forecasted CO2 Emissions')
plt.legend()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from math import sqrt

# Assuming 'df' has columns 'Year' and 'Value_co2_emissions_kt_by_country'
# Make sure your 'Year' column is in integer format

# Aggregate data for world total CO2 emissions
world_data = df.groupby('Year')['Value_co2_emissions_kt_by_country'].sum().reset_index()

# Filter data for years up to 2020
historical_data = world_data[world_data['Year'] <= 2020]

# Prepare features and target variable for historical data
X_hist = historical_data[['Year']]
y_hist = historical_data['Value_co2_emissions_kt_by_country']

# Create and fit the model with the specified hyperparameters
best_model_hist = RandomForestRegressor(
    n_estimators=100,
    min_samples_split=2,
    min_samples_leaf=1,
    max_features='auto',
    max_depth=50
)
best_model_hist.fit(X_hist, y_hist)

# Predict CO2 emissions for historical data
y_hist_pred = best_model_hist.predict(X_hist)

# Calculate the 10-year growth rate
last_10_years_growth_rate = (y_hist.iloc[-1] / y_hist.iloc[-11]) ** (1/10) - 1

# Forecast CO2 emissions for the next 5 years with the growth rate
forecast_years = range(2021, 2026)
forecast_values = [y_hist.iloc[-1] * (1 + last_10_years_growth_rate) ** (i - 2020) for i in forecast_years]

# Predict CO2 emissions for the forecasted years
X_forecast = pd.DataFrame({'Year': forecast_years})
y_forecast_pred = best_model_hist.predict(X_forecast)

# Get the standard deviation of predictions
prediction_std = sqrt(mean_squared_error(y_hist, y_hist_pred))

# Set the confidence level (e.g., 95% confidence interval)
confidence_level = 0.95

# Calculate the margin of error for the forecasted values
margin_of_error_forecast = prediction_std * 1.96  # for a 95% confidence interval (z-score of 1.96)

# Create upper and lower bounds for the forecasted values
upper_bound_forecast = []
lower_bound_forecast = []

for i, val in enumerate(forecast_values, start=2020):
    upper_bound_forecast.append(val + margin_of_error_forecast)
    lower_bound_forecast.append(val - margin_of_error_forecast)

# Create DataFrames for actual, predicted, and forecasted values with bounds
actual_df = pd.DataFrame({'Year': X_hist['Year'], 'Actual': y_hist})
predicted_df = pd.DataFrame({'Year': X_hist['Year'], 'Predicted': y_hist_pred})
forecasted_df = pd.DataFrame({
    'Year': forecast_years,
    'Forecasted': forecast_values,
    'Upper Bound': upper_bound_forecast,
    'Lower Bound': lower_bound_forecast
})

# Merge DataFrames on 'Year' to create a single DataFrame
result_df = pd.merge(actual_df, predicted_df, on='Year', how='outer')
result_df = pd.merge(result_df, forecasted_df, on='Year', how='outer')

# Plot actual, predicted, and forecasted values with bounds
plt.plot(result_df['Year'], result_df['Actual'], label='Actual', marker='o', color='blue')
plt.plot(result_df['Year'], result_df['Predicted'], label='Predicted', marker='o', linestyle='dashed', color='green')
plt.fill_between(result_df['Year'], result_df['Lower Bound'], result_df['Upper Bound'], color='orange', alpha=0.2, label='95% Confidence Interval')
plt.plot(result_df['Year'], result_df['Forecasted'], label='Forecasted', marker='o', linestyle='dashed', color='orange')
plt.xlabel('Year')
plt.ylabel('CO2 Emission (kt)')
plt.title('World - Actual, Predicted, and Forecasted CO2 Emissions with Confidence Intervals')
plt.legend()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from math import sqrt

# Assuming 'df' has columns 'Year' and 'Value_co2_emissions_kt_by_country'
# Make sure your 'Year' column is in integer format

# Aggregate data for world total CO2 emissions
world_data = df.groupby('Year')['Value_co2_emissions_kt_by_country'].sum().reset_index()

# Filter data for years up to 2020
historical_data = world_data[world_data['Year'] <= 2020]

# Prepare features and target variable for historical data
X_hist = historical_data[['Year']]
y_hist = historical_data['Value_co2_emissions_kt_by_country']

# Create and fit the model with the specified hyperparameters
best_model_hist = RandomForestRegressor(
    n_estimators=100,
    min_samples_split=2,
    min_samples_leaf=1,
    max_features='auto',
    max_depth=50
)
best_model_hist.fit(X_hist, y_hist)

# Predict CO2 emissions for historical data
y_hist_pred = best_model_hist.predict(X_hist)

# Calculate the 10-year growth rate
last_10_years_growth_rate = (y_hist.iloc[-1] / y_hist.iloc[-11]) ** (1/10) - 1

# Forecast CO2 emissions for the next 5 years with the growth rate
forecast_years = range(2021, 2026)
forecast_values = [y_hist.iloc[-1] * (1 + last_10_years_growth_rate) ** (i - 2020) for i in forecast_years]

# Predict CO2 emissions for the forecasted years
X_forecast = pd.DataFrame({'Year': forecast_years})
y_forecast_pred = best_model_hist.predict(X_forecast)

# Get the standard deviation of predictions
prediction_std = sqrt(mean_squared_error(y_hist, y_hist_pred))

# Set the confidence level (e.g., 95% confidence interval)
confidence_level = 0.95

# Calculate the margin of error for the forecasted values
margin_of_error_forecast = prediction_std * 1.96  # for a 95% confidence interval (z-score of 1.96)

# Create upper and lower bounds for the forecasted values
upper_bound_forecast = []
lower_bound_forecast = []

for i, val in enumerate(forecast_values, start=2020):
    upper_bound_forecast.append(val + margin_of_error_forecast)
    lower_bound_forecast.append(val - margin_of_error_forecast)

# Create DataFrames for actual, predicted, and forecasted values with bounds
actual_df = pd.DataFrame({'Year': X_hist['Year'], 'Actual': y_hist})
predicted_df = pd.DataFrame({'Year': X_hist['Year'], 'Predicted': y_hist_pred})
forecasted_df = pd.DataFrame({
    'Year': forecast_years,
    'Forecasted': forecast_values,
    'Upper Bound': upper_bound_forecast,
    'Lower Bound': lower_bound_forecast
})

# Merge DataFrames on 'Year' to create a single DataFrame
result_df = pd.merge(actual_df, predicted_df, on='Year', how='outer')
result_df = pd.merge(result_df, forecasted_df, on='Year', how='outer')

# Plot actual, predicted, and forecasted values with bounds
plt.plot(result_df['Year'], result_df['Actual'], label='Actual', marker='o', color='blue')
plt.plot(result_df['Year'], result_df['Predicted'], label='Predicted', marker='o', linestyle='dashed', color='green')
plt.plot(result_df['Year'], result_df['Forecasted'], label='Forecasted', marker='o', linestyle='dashed', color='orange')

# Create a funnel-shaped confidence interval
plt.fill_between(result_df['Year'], result_df['Lower Bound'], result_df['Upper Bound'], color='purple', alpha=0.5, label='95% Confidence Interval')

plt.xlabel('Year')
plt.ylabel('CO2 Emission (kt)')
plt.title('World - Actual, Predicted, and Forecasted CO2 Emissions with Confidence Interval')
plt.legend()
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor

# Assuming 'df' has columns 'Year' and 'Value_co2_emissions_kt_by_country'
# Make sure your 'Year' column is in integer format

# Aggregate data for world total CO2 emissions
world_data = df.groupby('Year')['Value_co2_emissions_kt_by_country'].sum().reset_index()

# Filter data for years up to 2020
historical_data = world_data[world_data['Year'] <= 2020]

# Prepare features and target variable for historical data
X_hist = historical_data[['Year']]
y_hist = historical_data['Value_co2_emissions_kt_by_country']

# Create and fit the model with the specified hyperparameters
best_model_hist = RandomForestRegressor(
    n_estimators=100,
    min_samples_split=2,
    min_samples_leaf=1,
    max_features='auto',
    max_depth=50
)
best_model_hist.fit(X_hist, y_hist)

# Predict CO2 emissions for historical data
y_hist_pred = best_model_hist.predict(X_hist)

# Forecast CO2 emissions for the next 5 years
forecast_years = range(2021, 2026)
X_forecast = pd.DataFrame({'Year': forecast_years})

# Perform bootstrapping to estimate prediction intervals
n_bootstraps = 1000
bootstrap_predictions = np.zeros((n_bootstraps, len(forecast_years)))

for i in range(n_bootstraps):
    bootstrap_sample = np.random.choice(y_hist_pred, size=len(y_hist_pred), replace=True)
    best_model_hist.fit(X_hist, bootstrap_sample)
    bootstrap_predictions[i, :] = best_model_hist.predict(X_forecast)

# Calculate percentiles to get prediction intervals
lower_bound_forecast = np.percentile(bootstrap_predictions, 2.5, axis=0)
upper_bound_forecast = np.percentile(bootstrap_predictions, 97.5, axis=0)
y_forecast_pred = np.mean(bootstrap_predictions, axis=0)

# Create DataFrames for actual, predicted, and forecasted values with bounds
actual_df = pd.DataFrame({'Year': X_hist['Year'], 'Actual': y_hist})
predicted_df = pd.DataFrame({'Year': X_hist['Year'], 'Predicted': y_hist_pred})
forecasted_df = pd.DataFrame({
    'Year': forecast_years,
    'Forecasted': y_forecast_pred,
    'Lower Bound': lower_bound_forecast,
    'Upper Bound': upper_bound_forecast
})

# Merge DataFrames on 'Year' to create a single DataFrame
result_df = pd.merge(actual_df, predicted_df, on='Year', how='outer')
result_df = pd.merge(result_df, forecasted_df, on='Year', how='outer')

# Plot actual, predicted, and forecasted values with bounds
plt.plot(result_df['Year'], result_df['Actual'], label='Actual', marker='o', color='blue')
plt.plot(result_df['Year'], result_df['Predicted'], label='Predicted', marker='o', linestyle='dashed', color='green')
plt.plot(result_df['Year'], result_df['Forecasted'], label='Forecasted', marker='o', linestyle='dashed', color='orange')

# Create a cone-like confidence interval
plt.fill_between(result_df['Year'], result_df['Lower Bound'], result_df['Upper Bound'], color='purple', alpha=0.5, label='95% Confidence Interval')

plt.xlabel('Year')
plt.ylabel('CO2 Emission (kt)')
plt.title('World - Actual, Predicted, and Forecasted CO2 Emissions with Confidence Interval')
plt.legend()
plt.show()

"""## **China**"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import mean_squared_error
from math import sqrt

# Assuming 'df' has columns 'Year', 'Country', and 'Value_co2_emissions_kt_by_country'
# Make sure your 'Year' column is in integer format

# Filter data for China
china_data = df[df['Entity'] == 'China']

# Prepare features and target variable for China
X_china = china_data[['Year']]
y_china = china_data['Value_co2_emissions_kt_by_country']

# Define the parameter grid for Randomized Search
param_grid = {
    'n_estimators': [int(x) for x in range(100, 1001, 100)],
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_depth': [int(x) for x in range(10, 110, 10)],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create and fit the RandomizedSearchCV model for China
rf_model_china = RandomForestRegressor()
random_search_china = RandomizedSearchCV(estimator=rf_model_china, param_distributions=param_grid,
                                          n_iter=100, scoring='neg_mean_squared_error', cv=5,
                                          random_state=42, n_jobs=-1)
random_search_china.fit(X_china, y_china)

# Get the best model from the random search for China
best_model_china = random_search_china.best_estimator_

# Predict CO2 emissions for China
y_china_pred = best_model_china.predict(X_china)

# Calculate the growth over the last 3 years for China
last_3_years_growth_rate_china = (y_china.iloc[-1] / y_china.iloc[-4]) ** (1/3) - 1

# Forecast CO2 emissions for China for the next 5 years
forecast_years_china = range(2021, 2026)
forecast_values_china = [y_china.iloc[-1] * (1 + last_3_years_growth_rate_china) ** (i - 2021) for i in forecast_years_china]

# Predict CO2 emissions for China for the forecasted years
X_forecast_china = pd.DataFrame({'Year': forecast_years_china})
y_forecast_pred_china = best_model_china.predict(X_forecast_china)

# Create DataFrames for actual, predicted, and forecasted values for China
actual_df_china = pd.DataFrame({'Year': X_china['Year'], 'Actual': y_china})
predicted_df_china = pd.DataFrame({'Year': X_china['Year'], 'Predicted': y_china_pred})
forecasted_df_china = pd.DataFrame({'Year': forecast_years_china, 'Forecasted': forecast_values_china})

# Merge DataFrames on 'Year' to create a single DataFrame for China
result_df_china = pd.merge(actual_df_china, predicted_df_china, on='Year', how='outer')
result_df_china = pd.merge(result_df_china, forecasted_df_china, on='Year', how='outer')

# Plot actual, predicted, and forecasted values for China
plt.plot(result_df_china['Year'], result_df_china['Actual'], label='Actual', marker='o', color='blue')
plt.plot(result_df_china['Year'], result_df_china['Predicted'], label='Predicted', marker='o', linestyle='dashed', color='green')
plt.plot(result_df_china['Year'], result_df_china['Forecasted'], label='Forecasted', marker='o', linestyle='dashed', color='orange')
plt.xlabel('Year')
plt.ylabel('CO2 Emission (kt)')
plt.title('China - Actual, Predicted, and Forecasted CO2 Emissions')
plt.legend()
plt.show()

# Display the best hyperparameters found by Randomized Search for China
print("Best Hyperparameters for China:", random_search_china.best_params_)

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor

# Assuming 'df' has columns 'Year', 'Country', and 'Value_co2_emissions_kt_by_country'
# Make sure your 'Year' column is in integer format

# Filter data for China
china_data = df[df['Entity'] == 'China']

# Prepare features and target variable for China
X_china = china_data[['Year']]
y_china = china_data['Value_co2_emissions_kt_by_country']

# Use the best hyperparameters found by Randomized Search for China
best_hyperparameters_china = {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': 20}

# Create and fit the RandomForestRegressor model for China with the best hyperparameters
rf_model_china = RandomForestRegressor(**best_hyperparameters_china)
rf_model_china.fit(X_china, y_china)

# Predict CO2 emissions for China
y_china_pred = rf_model_china.predict(X_china)

# Calculate the growth over the last 3 years for China
last_3_years_growth_rate_china = (y_china.iloc[-1] / y_china.iloc[-4]) ** (1/3) - 1

# Forecast CO2 emissions for China for the next 5 years
forecast_years_china = range(2021, 2026)
forecast_values_china = [y_china.iloc[-1] * (1 + last_3_years_growth_rate_china) ** (i - 2021) for i in forecast_years_china]

# Predict CO2 emissions for China for the forecasted years
X_forecast_china = pd.DataFrame({'Year': forecast_years_china})
y_forecast_pred_china = rf_model_china.predict(X_forecast_china)

# Create DataFrames for actual, predicted, and forecasted values for China
actual_df_china = pd.DataFrame({'Year': X_china['Year'], 'Actual': y_china})
predicted_df_china = pd.DataFrame({'Year': X_china['Year'], 'Predicted': y_china_pred})
forecasted_df_china = pd.DataFrame({'Year': forecast_years_china, 'Forecasted': forecast_values_china})

# Merge DataFrames on 'Year' to create a single DataFrame for China
result_df_china = pd.merge(actual_df_china, predicted_df_china, on='Year', how='outer')
result_df_china = pd.merge(result_df_china, forecasted_df_china, on='Year', how='outer')

# Plot actual, predicted, and forecasted values for China
plt.plot(result_df_china['Year'], result_df_china['Actual'], label='Actual', marker='o', color='blue')
plt.plot(result_df_china['Year'], result_df_china['Predicted'], label='Predicted', marker='o', linestyle='dashed', color='green')
plt.plot(result_df_china['Year'], result_df_china['Forecasted'], label='Forecasted', marker='o', linestyle='dashed', color='orange')
plt.xlabel('Year')
plt.ylabel('CO2 Emission (kt)')
plt.title('China - Actual, Predicted, and Forecasted CO2 Emissions')
plt.legend()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor

# Assuming 'df' has columns 'Year', 'Country', and 'Value_co2_emissions_kt_by_country'
# Make sure your 'Year' column is in integer format

# Filter data for China
china_data = df[df['Entity'] == 'China']

# Prepare features and target variable for China
X_china = china_data[['Year']]
y_china = china_data['Value_co2_emissions_kt_by_country']

# Use the best hyperparameters found by Randomized Search for China
best_hyperparameters_china = {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': 20}

# Create and fit the RandomForestRegressor model for China with the best hyperparameters
rf_model_china = RandomForestRegressor(**best_hyperparameters_china)
rf_model_china.fit(X_china, y_china)

# Predict CO2 emissions for China
y_china_pred = rf_model_china.predict(X_china)

# Calculate the growth over the last 5 years for China
last_5_years_growth_rate_china = (y_china.iloc[-1] / y_china.iloc[-6]) ** (1/5) - 1

# Forecast CO2 emissions for China for the next 5 years
forecast_years_china = range(2021, 2026)
forecast_values_china = [y_china.iloc[-1] * (1 + last_5_years_growth_rate_china) ** (i - 2021) for i in forecast_years_china]

# Predict CO2 emissions for China for the forecasted years
X_forecast_china = pd.DataFrame({'Year': forecast_years_china})
y_forecast_pred_china = rf_model_china.predict(X_forecast_china)

# Create DataFrames for actual, predicted, and forecasted values for China
actual_df_china = pd.DataFrame({'Year': X_china['Year'], 'Actual': y_china})
predicted_df_china = pd.DataFrame({'Year': X_china['Year'], 'Predicted': y_china_pred})
forecasted_df_china = pd.DataFrame({'Year': forecast_years_china, 'Forecasted': forecast_values_china})

# Merge DataFrames on 'Year' to create a single DataFrame for China
result_df_china = pd.merge(actual_df_china, predicted_df_china, on='Year', how='outer')
result_df_china = pd.merge(result_df_china, forecasted_df_china, on='Year', how='outer')

# Plot actual, predicted, and forecasted values for China
plt.plot(result_df_china['Year'], result_df_china['Actual'], label='Actual', marker='o', color='blue')
plt.plot(result_df_china['Year'], result_df_china['Predicted'], label='Predicted', marker='o', linestyle='dashed', color='green')
plt.plot(result_df_china['Year'], result_df_china['Forecasted'], label='Forecasted', marker='o', linestyle='dashed', color='orange')
plt.xlabel('Year')
plt.ylabel('CO2 Emission (kt)')
plt.title('China - Actual, Predicted, and Forecasted CO2 Emissions')
plt.legend()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor

# Assuming 'df' has columns 'Year', 'Country', and 'Value_co2_emissions_kt_by_country'
# Make sure your 'Year' column is in integer format

# Filter data for China
china_data = df[df['Entity'] == 'China']

# Prepare features and target variable for China
X_china = china_data[['Year']]
y_china = china_data['Value_co2_emissions_kt_by_country']

# Use the best hyperparameters found by Randomized Search for China
best_hyperparameters_china = {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': 20}

# Create and fit the RandomForestRegressor model for China with the best hyperparameters
rf_model_china = RandomForestRegressor(**best_hyperparameters_china)
rf_model_china.fit(X_china, y_china)

# Predict CO2 emissions for China
y_china_pred = rf_model_china.predict(X_china)

# Calculate the growth over the last 10 years for China
last_10_years_growth_rate_china = (y_china.iloc[-1] / y_china.iloc[-11]) ** (1/10) - 1

# Forecast CO2 emissions for China for the next 10 years
forecast_years_china = range(2021, 2026)
forecast_values_china = [y_china.iloc[-1] * (1 + last_10_years_growth_rate_china) ** (i - 2021) for i in forecast_years_china]

# Predict CO2 emissions for China for the forecasted years
X_forecast_china = pd.DataFrame({'Year': forecast_years_china})
y_forecast_pred_china = rf_model_china.predict(X_forecast_china)

# Create DataFrames for actual, predicted, and forecasted values for China
actual_df_china = pd.DataFrame({'Year': X_china['Year'], 'Actual': y_china})
predicted_df_china = pd.DataFrame({'Year': X_china['Year'], 'Predicted': y_china_pred})
forecasted_df_china = pd.DataFrame({'Year': forecast_years_china, 'Forecasted': forecast_values_china})

# Merge DataFrames on 'Year' to create a single DataFrame for China
result_df_china = pd.merge(actual_df_china, predicted_df_china, on='Year', how='outer')
result_df_china = pd.merge(result_df_china, forecasted_df_china, on='Year', how='outer')

# Plot actual, predicted, and forecasted values for China
plt.plot(result_df_china['Year'], result_df_china['Actual'], label='Actual', marker='o', color='blue')
plt.plot(result_df_china['Year'], result_df_china['Predicted'], label='Predicted', marker='o', linestyle='dashed', color='green')
plt.plot(result_df_china['Year'], result_df_china['Forecasted'], label='Forecasted', marker='o', linestyle='dashed', color='orange')
plt.xlabel('Year')
plt.ylabel('CO2 Emission (kt)')
plt.title('China - Actual, Predicted, and Forecasted CO2 Emissions')
plt.legend()
plt.show()

china_data = df[df['Entity'] == 'China']

# Prepare features and target variable for historical data in China
X_hist_china = china_data[['Year']]
y_hist_china = china_data['Value_co2_emissions_kt_by_country']

# Create and fit the model with the specified hyperparameters for China
best_model_hist_china = RandomForestRegressor(
    n_estimators=100,
    min_samples_split=2,
    min_samples_leaf=1,
    max_features='auto',
    max_depth=20
)
best_model_hist_china.fit(X_hist_china, y_hist_china)

# Predict CO2 emissions for historical data in China
y_hist_pred_china = best_model_hist_china.predict(X_hist_china)

# Calculate the 10-year growth rate for China
last_10_years_growth_rate_china = (y_hist_china.iloc[-1] / y_hist_china.iloc[-11]) ** (1/10) - 1

# Forecast CO2 emissions for the next 5 years in China with the growth rate
forecast_years_china = range(2021, 2026)
forecast_values_china = [y_hist_china.iloc[-1] * (1 + last_10_years_growth_rate_china) ** (i - 2020) for i in forecast_years_china]

# Predict CO2 emissions for the forecasted years in China
X_forecast_china = pd.DataFrame({'Year': forecast_years_china})
y_forecast_pred_china = best_model_hist_china.predict(X_forecast_china)

# Get the standard deviation of predictions for China
prediction_std_china = sqrt(mean_squared_error(y_hist_china, y_hist_pred_china))

# Set the confidence level (e.g., 95% confidence interval) for China
confidence_level_china = 0.95

# Calculate the margin of error for the forecasted values in China
margin_of_error_forecast_china = prediction_std_china * 1.96  # for a 95% confidence interval (z-score of 1.96)

# Create upper and lower bounds for the forecasted values in China
upper_bound_forecast_china = [val + margin_of_error_forecast_china for val in forecast_values_china]
lower_bound_forecast_china = [val - margin_of_error_forecast_china for val in forecast_values_china]

# Create DataFrames for actual, predicted, and forecasted values with bounds for China
actual_df_china = pd.DataFrame({'Year': X_hist_china['Year'], 'Actual': y_hist_china})
predicted_df_china = pd.DataFrame({'Year': X_hist_china['Year'], 'Predicted': y_hist_pred_china})
forecasted_df_china = pd.DataFrame({
    'Year': forecast_years_china,
    'Forecasted': forecast_values_china,
    'Upper Bound': upper_bound_forecast_china,
    'Lower Bound': lower_bound_forecast_china
})

# Merge DataFrames on 'Year' to create a single DataFrame for China
result_df_china = pd.merge(actual_df_china, predicted_df_china, on='Year', how='outer')
result_df_china = pd.merge(result_df_china, forecasted_df_china, on='Year', how='outer')

# Plot actual, predicted, and forecasted values with bounds for China
plt.plot(result_df_china['Year'], result_df_china['Actual'], label='Actual', marker='o', color='blue')
plt.plot(result_df_china['Year'], result_df_china['Predicted'], label='Predicted', marker='o', linestyle='dashed', color='green')
plt.plot(result_df_china['Year'], result_df_china['Forecasted'], label='Forecasted', marker='o', linestyle='dashed', color='orange')

# Create a funnel-shaped confidence interval for China
plt.fill_between(result_df_china['Year'], result_df_china['Lower Bound'], result_df_china['Upper Bound'], color='purple', alpha=0.5, label='95% Confidence Interval')

plt.xlabel('Year')
plt.ylabel('CO2 Emission (kt)')
plt.title('China - Actual, Predicted, and Forecasted CO2 Emissions with Confidence Interval')
plt.legend()
plt.show()

"""## **USA**"""

us_data = df[df['Entity'] == 'United States']

# Prepare features and target variable for the United States
X_us = us_data[['Year']]
y_us = us_data['Value_co2_emissions_kt_by_country']

# Define the parameter grid for Randomized Search
param_grid = {
    'n_estimators': [int(x) for x in range(100, 1001, 100)],
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_depth': [int(x) for x in range(10, 110, 10)],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create and fit the RandomizedSearchCV model for the United States
rf_model_us = RandomForestRegressor()
random_search_us = RandomizedSearchCV(estimator=rf_model_us, param_distributions=param_grid,
                                       n_iter=100, scoring='neg_mean_squared_error', cv=5,
                                       random_state=42, n_jobs=-1)
random_search_us.fit(X_us, y_us)

# Get the best model from the random search for the United States
best_model_us = random_search_us.best_estimator_

# Predict CO2 emissions for the United States
y_us_pred = best_model_us.predict(X_us)

# Calculate the growth over the last 3 years for the United States
last_3_years_growth_rate_us = (y_us.iloc[-1] / y_us.iloc[-4]) ** (1/3) - 1

# Forecast CO2 emissions for the United States for the next 5 years
forecast_years_us = range(2021, 2026)
forecast_values_us = [y_us.iloc[-1] * (1 + last_3_years_growth_rate_us) ** (i - 2021) for i in forecast_years_us]

# Predict CO2 emissions for the United States for the forecasted years
X_forecast_us = pd.DataFrame({'Year': forecast_years_us})
y_forecast_pred_us = best_model_us.predict(X_forecast_us)

# Create DataFrames for actual, predicted, and forecasted values for the United States
actual_df_us = pd.DataFrame({'Year': X_us['Year'], 'Actual': y_us})
predicted_df_us = pd.DataFrame({'Year': X_us['Year'], 'Predicted': y_us_pred})
forecasted_df_us = pd.DataFrame({'Year': forecast_years_us, 'Forecasted': forecast_values_us})

# Merge DataFrames on 'Year' to create a single DataFrame for the United States
result_df_us = pd.merge(actual_df_us, predicted_df_us, on='Year', how='outer')
result_df_us = pd.merge(result_df_us, forecasted_df_us, on='Year', how='outer')

# Plot actual, predicted, and forecasted values for the United States
plt.plot(result_df_us['Year'], result_df_us['Actual'], label='Actual', marker='o', color='blue')
plt.plot(result_df_us['Year'], result_df_us['Predicted'], label='Predicted', marker='o', linestyle='dashed', color='green')
plt.plot(result_df_us['Year'], result_df_us['Forecasted'], label='Forecasted', marker='o', linestyle='dashed', color='orange')
plt.xlabel('Year')
plt.ylabel('CO2 Emission (kt)')
plt.title('United States - Actual, Predicted, and Forecasted CO2 Emissions')
plt.legend()
plt.show()

# Display the best hyperparameters found by Randomized Search for the United States
print("Best Hyperparameters for the United States:", random_search_us.best_params_)

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor

# Assuming 'df' has columns 'Year', 'Country', and 'Value_co2_emissions_kt_by_country'
# Make sure your 'Year' column is in integer format

# Filter data for the United States
us_data = df[df['Entity'] == 'United States']

# Prepare features and target variable for the United States
X_us = us_data[['Year']]
y_us = us_data['Value_co2_emissions_kt_by_country']

# Use the best hyperparameters found by Randomized Search for the United States
best_hyperparameters_us = {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': 60}

# Create and fit the RandomForestRegressor model for the United States with the best hyperparameters
rf_model_us = RandomForestRegressor(**best_hyperparameters_us)
rf_model_us.fit(X_us, y_us)

# Predict CO2 emissions for the United States
y_us_pred = rf_model_us.predict(X_us)

# Calculate the growth over the last 3 years for the United States
last_3_years_growth_rate_us = (y_us.iloc[-1] / y_us.iloc[-4]) ** (1/3) - 1

# Forecast CO2 emissions for the United States for the next 5 years
forecast_years_us = range(2021, 2026)
forecast_values_us = [y_us.iloc[-1] * (1 + last_3_years_growth_rate_us) ** (i - 2021) for i in forecast_years_us]

# Predict CO2 emissions for the United States for the forecasted years
X_forecast_us = pd.DataFrame({'Year': forecast_years_us})
y_forecast_pred_us = rf_model_us.predict(X_forecast_us)

# Create DataFrames for actual, predicted, and forecasted values for the United States
actual_df_us = pd.DataFrame({'Year': X_us['Year'], 'Actual': y_us})
predicted_df_us = pd.DataFrame({'Year': X_us['Year'], 'Predicted': y_us_pred})
forecasted_df_us = pd.DataFrame({'Year': forecast_years_us, 'Forecasted': forecast_values_us})

# Merge DataFrames on 'Year' to create a single DataFrame for the United States
result_df_us = pd.merge(actual_df_us, predicted_df_us, on='Year', how='outer')
result_df_us = pd.merge(result_df_us, forecasted_df_us, on='Year', how='outer')

# Plot actual, predicted, and forecasted values for the United States
plt.plot(result_df_us['Year'], result_df_us['Actual'], label='Actual', marker='o', color='blue')
plt.plot(result_df_us['Year'], result_df_us['Predicted'], label='Predicted', marker='o', linestyle='dashed', color='green')
plt.plot(result_df_us['Year'], result_df_us['Forecasted'], label='Forecasted', marker='o', linestyle='dashed', color='orange')
plt.xlabel('Year')
plt.ylabel('CO2 Emission (kt)')
plt.title('United States - Actual, Predicted, and Forecasted CO2 Emissions')
plt.legend()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor

# Assuming 'df' has columns 'Year', 'Country', and 'Value_co2_emissions_kt_by_country'
# Make sure your 'Year' column is in integer format

# Filter data for the United States
us_data = df[df['Entity'] == 'United States']

# Prepare features and target variable for the United States
X_us = us_data[['Year']]
y_us = us_data['Value_co2_emissions_kt_by_country']

# Use the best hyperparameters found by Randomized Search for the United States
best_hyperparameters_us = {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': 60}

# Create and fit the RandomForestRegressor model for the United States with the best hyperparameters
rf_model_us = RandomForestRegressor(**best_hyperparameters_us)
rf_model_us.fit(X_us, y_us)

# Predict CO2 emissions for the United States
y_us_pred = rf_model_us.predict(X_us)

# Calculate the growth over the last 5 years for the United States
last_5_years_growth_rate_us = (y_us.iloc[-1] / y_us.iloc[-6]) ** (1/5) - 1

# Forecast CO2 emissions for the United States for the next 5 years
forecast_years_us = range(2021, 2026)
forecast_values_us = [y_us.iloc[-1] * (1 + last_5_years_growth_rate_us) ** (i - 2021) for i in forecast_years_us]

# Predict CO2 emissions for the United States for the forecasted years
X_forecast_us = pd.DataFrame({'Year': forecast_years_us})
y_forecast_pred_us = rf_model_us.predict(X_forecast_us)

# Create DataFrames for actual, predicted, and forecasted values for the United States
actual_df_us = pd.DataFrame({'Year': X_us['Year'], 'Actual': y_us})
predicted_df_us = pd.DataFrame({'Year': X_us['Year'], 'Predicted': y_us_pred})
forecasted_df_us = pd.DataFrame({'Year': forecast_years_us, 'Forecasted': forecast_values_us})

# Merge DataFrames on 'Year' to create a single DataFrame for the United States
result_df_us = pd.merge(actual_df_us, predicted_df_us, on='Year', how='outer')
result_df_us = pd.merge(result_df_us, forecasted_df_us, on='Year', how='outer')

# Plot actual, predicted, and forecasted values for the United States
plt.plot(result_df_us['Year'], result_df_us['Actual'], label='Actual', marker='o', color='blue')
plt.plot(result_df_us['Year'], result_df_us['Predicted'], label='Predicted', marker='o', linestyle='dashed', color='green')
plt.plot(result_df_us['Year'], result_df_us['Forecasted'], label='Forecasted', marker='o', linestyle='dashed', color='orange')
plt.xlabel('Year')
plt.ylabel('CO2 Emission (kt)')
plt.title('United States - Actual, Predicted, and Forecasted CO2 Emissions')
plt.legend()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor

# Assuming 'df' has columns 'Year', 'Country', and 'Value_co2_emissions_kt_by_country'
# Make sure your 'Year' column is in integer format

# Filter data for the United States
us_data = df[df['Entity'] == 'United States']

# Prepare features and target variable for the United States
X_us = us_data[['Year']]
y_us = us_data['Value_co2_emissions_kt_by_country']

# Use the best hyperparameters found by Randomized Search for the United States
best_hyperparameters_us = {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': 60}

# Create and fit the RandomForestRegressor model for the United States with the best hyperparameters
rf_model_us = RandomForestRegressor(**best_hyperparameters_us)
rf_model_us.fit(X_us, y_us)

# Predict CO2 emissions for the United States
y_us_pred = rf_model_us.predict(X_us)

# Calculate the growth over the last 10 years for the United States
last_10_years_growth_rate_us = (y_us.iloc[-1] / y_us.iloc[-11]) ** (1/10) - 1

# Forecast CO2 emissions for the United States for the next 5 years
forecast_years_us = range(2021, 2026)  # Extend the range to cover the next 10 years
forecast_values_us = [y_us.iloc[-1] * (1 + last_10_years_growth_rate_us) ** (i - 2021) for i in forecast_years_us]

# Predict CO2 emissions for the United States for the forecasted years
X_forecast_us = pd.DataFrame({'Year': forecast_years_us})
y_forecast_pred_us = rf_model_us.predict(X_forecast_us)

# Create DataFrames for actual, predicted, and forecasted values for the United States
actual_df_us = pd.DataFrame({'Year': X_us['Year'], 'Actual': y_us})
predicted_df_us = pd.DataFrame({'Year': X_us['Year'], 'Predicted': y_us_pred})
forecasted_df_us = pd.DataFrame({'Year': forecast_years_us, 'Forecasted': forecast_values_us})

# Merge DataFrames on 'Year' to create a single DataFrame for the United States
result_df_us = pd.merge(actual_df_us, predicted_df_us, on='Year', how='outer')
result_df_us = pd.merge(result_df_us, forecasted_df_us, on='Year', how='outer')

# Plot actual, predicted, and forecasted values for the United States
plt.plot(result_df_us['Year'], result_df_us['Actual'], label='Actual', marker='o', color='blue')
plt.plot(result_df_us['Year'], result_df_us['Predicted'], label='Predicted', marker='o', linestyle='dashed', color='green')
plt.plot(result_df_us['Year'], result_df_us['Forecasted'], label='Forecasted', marker='o', linestyle='dashed', color='orange')
plt.xlabel('Year')
plt.ylabel('CO2 Emission (kt)')
plt.title('United States - Actual, Predicted, and Forecasted CO2 Emissions')
plt.legend()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from math import sqrt

# Assuming 'df' has columns 'Year' and 'Value_co2_emissions_kt_by_country'
# Make sure your 'Year' column is in integer format

# Filter data for the United States
us_data = df[df['Entity'] == 'United States']

# Prepare features and target variable for historical data in the United States
X_hist_us = us_data[['Year']]
y_hist_us = us_data['Value_co2_emissions_kt_by_country']

# Create and fit the model with the specified hyperparameters for the United States
best_model_hist_us = RandomForestRegressor(
    n_estimators=200,
    min_samples_split=5,
    min_samples_leaf=1,
    max_features='auto',
    max_depth=60
)
best_model_hist_us.fit(X_hist_us, y_hist_us)

# Predict CO2 emissions for historical data in the United States
y_hist_pred_us = best_model_hist_us.predict(X_hist_us)

# Calculate the 10-year growth rate for the United States
last_10_years_growth_rate_us = (y_hist_us.iloc[-1] / y_hist_us.iloc[-11]) ** (1/10) - 1

# Forecast CO2 emissions for the next 5 years in the United States with the growth rate
forecast_years_us = range(2021, 2026)
forecast_values_us = [y_hist_us.iloc[-1] * (1 + last_10_years_growth_rate_us) ** (i - 2020) for i in forecast_years_us]

# Predict CO2 emissions for the forecasted years in the United States
X_forecast_us = pd.DataFrame({'Year': forecast_years_us})
y_forecast_pred_us = best_model_hist_us.predict(X_forecast_us)

# Get the standard deviation of predictions for the United States
prediction_std_us = sqrt(mean_squared_error(y_hist_us, y_hist_pred_us))

# Set the confidence level (e.g., 95% confidence interval) for the United States
confidence_level_us = 0.95

# Calculate the margin of error for the forecasted values in the United States
margin_of_error_forecast_us = prediction_std_us * 1.96  # for a 95% confidence interval (z-score of 1.96)

# Create upper and lower bounds for the forecasted values in the United States
upper_bound_forecast_us = [val + margin_of_error_forecast_us for val in forecast_values_us]
lower_bound_forecast_us = [val - margin_of_error_forecast_us for val in forecast_values_us]

# Create DataFrames for actual, predicted, and forecasted values with bounds for the United States
actual_df_us = pd.DataFrame({'Year': X_hist_us['Year'], 'Actual': y_hist_us})
predicted_df_us = pd.DataFrame({'Year': X_hist_us['Year'], 'Predicted': y_hist_pred_us})
forecasted_df_us = pd.DataFrame({
    'Year': forecast_years_us,
    'Forecasted': forecast_values_us,
    'Upper Bound': upper_bound_forecast_us,
    'Lower Bound': lower_bound_forecast_us
})

# Merge DataFrames on 'Year' to create a single DataFrame for the United States
result_df_us = pd.merge(actual_df_us, predicted_df_us, on='Year', how='outer')
result_df_us = pd.merge(result_df_us, forecasted_df_us, on='Year', how='outer')

# Plot actual, predicted, and forecasted values with bounds for the United States
plt.plot(result_df_us['Year'], result_df_us['Actual'], label='Actual', marker='o', color='blue')
plt.plot(result_df_us['Year'], result_df_us['Predicted'], label='Predicted', marker='o', linestyle='dashed', color='green')
plt.plot(result_df_us['Year'], result_df_us['Forecasted'], label='Forecasted', marker='o', linestyle='dashed', color='orange')

# Create a funnel-shaped confidence interval for the United States
plt.fill_between(result_df_us['Year'], result_df_us['Lower Bound'], result_df_us['Upper Bound'], color='purple', alpha=0.5, label='95% Confidence Interval')

plt.xlabel('Year')
plt.ylabel('CO2 Emission (kt)')
plt.title('United States - Actual, Predicted, and Forecasted CO2 Emissions with Confidence Interval')
plt.legend()
plt.show()

"""## **India**"""

india_data = df[df['Entity'] == 'India']

# Prepare features and target variable for India
X_india = india_data[['Year']]
y_india = india_data['Value_co2_emissions_kt_by_country']

# Define the parameter grid for Randomized Search
param_grid = {
    'n_estimators': [int(x) for x in range(100, 1001, 100)],
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_depth': [int(x) for x in range(10, 110, 10)],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create and fit the RandomizedSearchCV model for India
rf_model_india = RandomForestRegressor()
random_search_india = RandomizedSearchCV(estimator=rf_model_india, param_distributions=param_grid,
                                          n_iter=100, scoring='neg_mean_squared_error', cv=5,
                                          random_state=42, n_jobs=-1)
random_search_india.fit(X_india, y_india)

# Get the best model from the random search for India
best_model_india = random_search_india.best_estimator_

# Predict CO2 emissions for India
y_india_pred = best_model_india.predict(X_india)

# Calculate the growth over the last 3 years for India
last_3_years_growth_rate_india = (y_india.iloc[-1] / y_india.iloc[-4]) ** (1/3) - 1

# Forecast CO2 emissions for India for the next 5 years
forecast_years_india = range(2021, 2026)
forecast_values_india = [y_india.iloc[-1] * (1 + last_3_years_growth_rate_india) ** (i - 2021) for i in forecast_years_india]

# Predict CO2 emissions for India for the forecasted years
X_forecast_india = pd.DataFrame({'Year': forecast_years_india})
y_forecast_pred_india = best_model_india.predict(X_forecast_india)

# Create DataFrames for actual, predicted, and forecasted values for India
actual_df_india = pd.DataFrame({'Year': X_india['Year'], 'Actual': y_india})
predicted_df_india = pd.DataFrame({'Year': X_india['Year'], 'Predicted': y_india_pred})
forecasted_df_india = pd.DataFrame({'Year': forecast_years_india, 'Forecasted': forecast_values_india})

# Merge df on 'Year' to create a single df for India
result_df_india = pd.merge(actual_df_india, predicted_df_india, on='Year', how='outer')
result_df_india = pd.merge(result_df_india, forecasted_df_india, on='Year', how='outer')

# Plot actual, predicted, and forecasted values for India
plt.plot(result_df_india['Year'], result_df_india['Actual'], label='Actual', marker='o', color='blue')
plt.plot(result_df_india['Year'], result_df_india['Predicted'], label='Predicted', marker='o', linestyle='dashed', color='green')
plt.plot(result_df_india['Year'], result_df_india['Forecasted'], label='Forecasted', marker='o', linestyle='dashed', color='orange')
plt.xlabel('Year')
plt.ylabel('CO2 Emission (kt)')
plt.title('India - Actual, Predicted, and Forecasted CO2 Emissions')
plt.legend()
plt.show()

print("Best Hyperparameters for India:", random_search_india.best_params_)

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV

# Assuming 'df' has columns 'Year', 'Country', and 'Value_co2_emissions_kt_by_country'
# Make sure your 'Year' column is in integer format

# Filter data for India
india_data = df[df['Entity'] == 'India']

# Prepare features and target variable for India
X_india = india_data[['Year']]
y_india = india_data['Value_co2_emissions_kt_by_country']

# Use the best hyperparameters found by Randomized Search for India
best_hyperparameters_india = {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 20}

# Create and fit the RandomForestRegressor model for India with the best hyperparameters
rf_model_india = RandomForestRegressor(**best_hyperparameters_india)
rf_model_india.fit(X_india, y_india)

# Predict CO2 emissions for India
y_india_pred = rf_model_india.predict(X_india)

# Calculate the growth over the last 3 years for India
last_3_years_growth_rate_india = (y_india.iloc[-1] / y_india.iloc[-4]) ** (1/3) - 1

# Forecast CO2 emissions for India for the next 5 years
forecast_years_india = range(2021, 2026)
forecast_values_india = [y_india.iloc[-1] * (1 + last_3_years_growth_rate_india) ** (i - 2021) for i in forecast_years_india]

# Predict CO2 emissions for India for the forecasted years
X_forecast_india = pd.DataFrame({'Year': forecast_years_india})
y_forecast_pred_india = rf_model_india.predict(X_forecast_india)

# Create DataFrames for actual, predicted, and forecasted values for India
actual_df_india = pd.DataFrame({'Year': X_india['Year'], 'Actual': y_india})
predicted_df_india = pd.DataFrame({'Year': X_india['Year'], 'Predicted': y_india_pred})
forecasted_df_india = pd.DataFrame({'Year': forecast_years_india, 'Forecasted': forecast_values_india})

# Merge DataFrames on 'Year' to create a single DataFrame for India
result_df_india = pd.merge(actual_df_india, predicted_df_india, on='Year', how='outer')
result_df_india = pd.merge(result_df_india, forecasted_df_india, on='Year', how='outer')

# Plot actual, predicted, and forecasted values for India
plt.plot(result_df_india['Year'], result_df_india['Actual'], label='Actual', marker='o', color='blue')
plt.plot(result_df_india['Year'], result_df_india['Predicted'], label='Predicted', marker='o', linestyle='dashed', color='green')
plt.plot(result_df_india['Year'], result_df_india['Forecasted'], label='Forecasted', marker='o', linestyle='dashed', color='orange')
plt.xlabel('Year')
plt.ylabel('CO2 Emission (kt)')
plt.title('India - Actual, Predicted, and Forecasted CO2 Emissions')
plt.legend()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor

# Assuming 'df' has columns 'Year', 'Country', and 'Value_co2_emissions_kt_by_country'
# Make sure your 'Year' column is in integer format

# Filter data for India
india_data = df[df['Entity'] == 'India']

# Prepare features and target variable for India
X_india = india_data[['Year']]
y_india = india_data['Value_co2_emissions_kt_by_country']

# Use the best hyperparameters found by Randomized Search for India
best_hyperparameters_india = {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 20}

# Create and fit the RandomForestRegressor model for India with the best hyperparameters
rf_model_india = RandomForestRegressor(**best_hyperparameters_india)
rf_model_india.fit(X_india, y_india)

# Predict CO2 emissions for India
y_india_pred = rf_model_india.predict(X_india)

# Calculate the growth over the last 5 years for India
last_5_years_growth_rate_india = (y_india.iloc[-1] / y_india.iloc[-6]) ** (1/5) - 1

# Forecast CO2 emissions for India for the next 5 years
forecast_years_india = range(2021, 2026)
forecast_values_india = [y_india.iloc[-1] * (1 + last_5_years_growth_rate_india) ** (i - 2021) for i in forecast_years_india]

# Predict CO2 emissions for India for the forecasted years
X_forecast_india = pd.DataFrame({'Year': forecast_years_india})
y_forecast_pred_india = rf_model_india.predict(X_forecast_india)

# Create DataFrames for actual, predicted, and forecasted values for India
actual_df_india = pd.DataFrame({'Year': X_india['Year'], 'Actual': y_india})
predicted_df_india = pd.DataFrame({'Year': X_india['Year'], 'Predicted': y_india_pred})
forecasted_df_india = pd.DataFrame({'Year': forecast_years_india, 'Forecasted': forecast_values_india})

# Merge DataFrames on 'Year' to create a single DataFrame for India
result_df_india = pd.merge(actual_df_india, predicted_df_india, on='Year', how='outer')
result_df_india = pd.merge(result_df_india, forecasted_df_india, on='Year', how='outer')

# Plot actual, predicted, and forecasted values for India
plt.plot(result_df_india['Year'], result_df_india['Actual'], label='Actual', marker='o', color='blue')
plt.plot(result_df_india['Year'], result_df_india['Predicted'], label='Predicted', marker='o', linestyle='dashed', color='green')
plt.plot(result_df_india['Year'], result_df_india['Forecasted'], label='Forecasted', marker='o', linestyle='dashed', color='orange')
plt.xlabel('Year')
plt.ylabel('CO2 Emission (kt)')
plt.title('India - Actual, Predicted, and Forecasted CO2 Emissions')
plt.legend()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor

# Assuming 'df' has columns 'Year', 'Country', and 'Value_co2_emissions_kt_by_country'
# Make sure your 'Year' column is in integer format

# Filter data for India
india_data = df[df['Entity'] == 'India']

# Prepare features and target variable for India
X_india = india_data[['Year']]
y_india = india_data['Value_co2_emissions_kt_by_country']

# Use the best hyperparameters found by Randomized Search for India
best_hyperparameters_india = {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 20}

# Create and fit the RandomForestRegressor model for India with the best hyperparameters
rf_model_india = RandomForestRegressor(**best_hyperparameters_india)
rf_model_india.fit(X_india, y_india)

# Predict CO2 emissions for India
y_india_pred = rf_model_india.predict(X_india)

# Calculate the growth over the last 10 years for India
last_10_years_growth_rate_india = (y_india.iloc[-1] / y_india.iloc[-11]) ** (1/10) - 1

# Forecast CO2 emissions for India for the next 10 years
forecast_years_india = range(2021, 2026)
forecast_values_india = [y_india.iloc[-1] * (1 + last_10_years_growth_rate_india) ** (i - 2021) for i in forecast_years_india]

# Predict CO2 emissions for India for the forecasted years
X_forecast_india = pd.DataFrame({'Year': forecast_years_india})
y_forecast_pred_india = rf_model_india.predict(X_forecast_india)

# Create DataFrames for actual, predicted, and forecasted values for India
actual_df_india = pd.DataFrame({'Year': X_india['Year'], 'Actual': y_india})
predicted_df_india = pd.DataFrame({'Year': X_india['Year'], 'Predicted': y_india_pred})
forecasted_df_india = pd.DataFrame({'Year': forecast_years_india, 'Forecasted': forecast_values_india})

# Merge DataFrames on 'Year' to create a single DataFrame for India
result_df_india = pd.merge(actual_df_india, predicted_df_india, on='Year', how='outer')
result_df_india = pd.merge(result_df_india, forecasted_df_india, on='Year', how='outer')

# Plot actual, predicted, and forecasted values for India
plt.plot(result_df_india['Year'], result_df_india['Actual'], label='Actual', marker='o', color='blue')
plt.plot(result_df_india['Year'], result_df_india['Predicted'], label='Predicted', marker='o', linestyle='dashed', color='green')
plt.plot(result_df_india['Year'], result_df_india['Forecasted'], label='Forecasted', marker='o', linestyle='dashed', color='orange')
plt.xlabel('Year')
plt.ylabel('CO2 Emission (kt)')
plt.title('India - Actual, Predicted, and Forecasted CO2 Emissions')
plt.legend()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from math import sqrt

# Assuming 'df' has columns 'Year' and 'Value_co2_emissions_kt_by_country'
# Make sure your 'Year' column is in integer format

# Filter data for India
india_data = df[df['Entity'] == 'India']

# Prepare features and target variable for historical data in India
X_hist_india = india_data[['Year']]
y_hist_india = india_data['Value_co2_emissions_kt_by_country']

# Create and fit the model with the specified hyperparameters for India
best_model_hist_india = RandomForestRegressor(
    n_estimators=100,
    min_samples_split=2,
    min_samples_leaf=1,
    max_features='log2',
    max_depth=20
)
best_model_hist_india.fit(X_hist_india, y_hist_india)

# Predict CO2 emissions for historical data in India
y_hist_pred_india = best_model_hist_india.predict(X_hist_india)

# Calculate the 10-year growth rate for India
last_10_years_growth_rate_india = (y_hist_india.iloc[-1] / y_hist_india.iloc[-11]) ** (1/10) - 1

# Forecast CO2 emissions for the next 5 years in India with the growth rate
forecast_years_india = range(2021, 2026)
forecast_values_india = [y_hist_india.iloc[-1] * (1 + last_10_years_growth_rate_india) ** (i - 2020) for i in forecast_years_india]

# Predict CO2 emissions for the forecasted years in India
X_forecast_india = pd.DataFrame({'Year': forecast_years_india})
y_forecast_pred_india = best_model_hist_india.predict(X_forecast_india)

# Get the standard deviation of predictions for India
prediction_std_india = sqrt(mean_squared_error(y_hist_india, y_hist_pred_india))

# Set the confidence level (e.g., 95% confidence interval) for India
confidence_level_india = 0.95

# Calculate the margin of error for the forecasted values in India
margin_of_error_forecast_india = prediction_std_india * 1.96  # for a 95% confidence interval (z-score of 1.96)

# Create upper and lower bounds for the forecasted values in India
upper_bound_forecast_india = [val + margin_of_error_forecast_india for val in forecast_values_india]
lower_bound_forecast_india = [val - margin_of_error_forecast_india for val in forecast_values_india]

# Create DataFrames for actual, predicted, and forecasted values with bounds for India
actual_df_india = pd.DataFrame({'Year': X_hist_india['Year'], 'Actual': y_hist_india})
predicted_df_india = pd.DataFrame({'Year': X_hist_india['Year'], 'Predicted': y_hist_pred_india})
forecasted_df_india = pd.DataFrame({
    'Year': forecast_years_india,
    'Forecasted': forecast_values_india,
    'Upper Bound': upper_bound_forecast_india,
    'Lower Bound': lower_bound_forecast_india
})

# Merge DataFrames on 'Year' to create a single DataFrame for India
result_df_india = pd.merge(actual_df_india, predicted_df_india, on='Year', how='outer')
result_df_india = pd.merge(result_df_india, forecasted_df_india, on='Year', how='outer')

# Plot actual, predicted, and forecasted values with bounds for India
plt.plot(result_df_india['Year'], result_df_india['Actual'], label='Actual', marker='o', color='blue')
plt.plot(result_df_india['Year'], result_df_india['Predicted'], label='Predicted', marker='o', linestyle='dashed', color='green')
plt.plot(result_df_india['Year'], result_df_india['Forecasted'], label='Forecasted', marker='o', linestyle='dashed', color='orange')

# Create a funnel-shaped confidence interval for India
plt.fill_between(result_df_india['Year'], result_df_india['Lower Bound'], result_df_india['Upper Bound'], color='purple', alpha=0.5, label='95% Confidence Interval')

plt.xlabel('Year')
plt.ylabel('CO2 Emission (kt)')
plt.title('India - Actual, Predicted, and Forecasted CO2 Emissions with Confidence Interval')
plt.legend()
plt.show()

"""## **UK**"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import mean_squared_error
from math import sqrt

# Assuming 'df' has columns 'Year', 'Country', and 'Value_co2_emissions_kt_by_country'
# Make sure your 'Year' column is in integer format

# Filter data for the United Kingdom
uk_data = df[df['Entity'] == 'United Kingdom']

# Prepare features and target variable for the United Kingdom
X_uk = uk_data[['Year']]
y_uk = uk_data['Value_co2_emissions_kt_by_country']

# Define the parameter grid for Randomized Search
param_grid = {
    'n_estimators': [int(x) for x in range(100, 1001, 100)],
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_depth': [int(x) for x in range(10, 110, 10)],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create and fit the RandomizedSearchCV model for the United Kingdom
rf_model_uk = RandomForestRegressor()
random_search_uk = RandomizedSearchCV(estimator=rf_model_uk, param_distributions=param_grid,
                                       n_iter=100, scoring='neg_mean_squared_error', cv=5,
                                       random_state=42, n_jobs=-1)
random_search_uk.fit(X_uk, y_uk)

# Get the best model from the random search for the United Kingdom
best_model_uk = random_search_uk.best_estimator_

# Predict CO2 emissions for the United Kingdom
y_uk_pred = best_model_uk.predict(X_uk)

# Calculate the growth over the last 3 years for the United Kingdom
last_3_years_growth_rate_uk = (y_uk.iloc[-1] / y_uk.iloc[-4]) ** (1/3) - 1

# Forecast CO2 emissions for the United Kingdom for the next 5 years
forecast_years_uk = range(2021, 2026)
forecast_values_uk = [y_uk.iloc[-1] * (1 + last_3_years_growth_rate_uk) ** (i - 2021) for i in forecast_years_uk]

# Predict CO2 emissions for the United Kingdom for the forecasted years
X_forecast_uk = pd.DataFrame({'Year': forecast_years_uk})
y_forecast_pred_uk = best_model_uk.predict(X_forecast_uk)

# Create DataFrames for actual, predicted, and forecasted values for the United Kingdom
actual_df_uk = pd.DataFrame({'Year': X_uk['Year'], 'Actual': y_uk})
predicted_df_uk = pd.DataFrame({'Year': X_uk['Year'], 'Predicted': y_uk_pred})
forecasted_df_uk = pd.DataFrame({'Year': forecast_years_uk, 'Forecasted': forecast_values_uk})

# Merge DataFrames on 'Year' to create a single DataFrame for the United Kingdom
result_df_uk = pd.merge(actual_df_uk, predicted_df_uk, on='Year', how='outer')
result_df_uk = pd.merge(result_df_uk, forecasted_df_uk, on='Year', how='outer')

# Plot actual, predicted, and forecasted values for the United Kingdom
plt.plot(result_df_uk['Year'], result_df_uk['Actual'], label='Actual', marker='o', color='blue')
plt.plot(result_df_uk['Year'], result_df_uk['Predicted'], label='Predicted', marker='o', linestyle='dashed', color='green')
plt.plot(result_df_uk['Year'], result_df_uk['Forecasted'], label='Forecasted', marker='o', linestyle='dashed', color='orange')
plt.xlabel('Year')
plt.ylabel('CO2 Emission (kt)')
plt.title('United Kingdom - Actual, Predicted, and Forecasted CO2 Emissions')
plt.legend()
plt.show()

# Display the best hyperparameters found by Randomized Search for the United Kingdom
print("Best Hyperparameters for the United Kingdom:", random_search_uk.best_params_)

uk_data = df[df['Entity'] == 'United Kingdom']

# Prepare features and target variable for the United Kingdom
X_uk = uk_data[['Year']]
y_uk = uk_data['Value_co2_emissions_kt_by_country']

# Use the provided hyperparameters for the United Kingdom
best_hyperparameters_uk = {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 40}

# Create and fit the RandomForestRegressor model for the United Kingdom with the provided hyperparameters
rf_model_uk = RandomForestRegressor(**best_hyperparameters_uk)
rf_model_uk.fit(X_uk, y_uk)

# Predict CO2 emissions for the United Kingdom
y_uk_pred = rf_model_uk.predict(X_uk)

# Calculate the growth over the last 3 years for the United Kingdom
last_3_years_growth_rate_uk = (y_uk.iloc[-1] / y_uk.iloc[-4]) ** (1/3) - 1

# Forecast CO2 emissions for the United Kingdom for the next 5 years
forecast_years_uk = range(2021, 2026)
forecast_values_uk = [y_uk.iloc[-1] * (1 + last_3_years_growth_rate_uk) ** (i - 2021) for i in forecast_years_uk]

# Predict CO2 emissions for the United Kingdom for the forecasted years
X_forecast_uk = pd.DataFrame({'Year': forecast_years_uk})
y_forecast_pred_uk = rf_model_uk.predict(X_forecast_uk)

# Create DataFrames for actual, predicted, and forecasted values for the United Kingdom
actual_df_uk = pd.DataFrame({'Year': X_uk['Year'], 'Actual': y_uk})
predicted_df_uk = pd.DataFrame({'Year': X_uk['Year'], 'Predicted': y_uk_pred})
forecasted_df_uk = pd.DataFrame({'Year': forecast_years_uk, 'Forecasted': forecast_values_uk})

# Merge DataFrames on 'Year' to create a single DataFrame for the United Kingdom
result_df_uk = pd.merge(actual_df_uk, predicted_df_uk, on='Year', how='outer')
result_df_uk = pd.merge(result_df_uk, forecasted_df_uk, on='Year', how='outer')

# Plot actual, predicted, and forecasted values for the United Kingdom
plt.plot(result_df_uk['Year'], result_df_uk['Actual'], label='Actual', marker='o', color='blue')
plt.plot(result_df_uk['Year'], result_df_uk['Predicted'], label='Predicted', marker='o', linestyle='dashed', color='green')
plt.plot(result_df_uk['Year'], result_df_uk['Forecasted'], label='Forecasted', marker='o', linestyle='dashed', color='orange')
plt.xlabel('Year')
plt.ylabel('CO2 Emission (kt)')
plt.title('United Kingdom - Actual, Predicted, and Forecasted CO2 Emissions')
plt.legend()
plt.show()

uk_data = df[df['Entity'] == 'United Kingdom']

# Prepare features and target variable for the United Kingdom
X_uk = uk_data[['Year']]
y_uk = uk_data['Value_co2_emissions_kt_by_country']

best_hyperparameters_uk = {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 40}

# Create and fit the RandomForestRegressor model for the United Kingdom the hyperparameters
rf_model_uk = RandomForestRegressor(**best_hyperparameters_uk)
rf_model_uk.fit(X_uk, y_uk)

# Predict CO2 emissions for the United Kingdom
y_uk_pred = rf_model_uk.predict(X_uk)

# Calculate the growth over the last 5 years for the United Kingdom
last_5_years_growth_rate_uk = (y_uk.iloc[-1] / y_uk.iloc[-6]) ** (1/5) - 1

# Forecast CO2 emissions for the United Kingdom for the next 5 years
forecast_years_uk = range(2021, 2026)
forecast_values_uk = [y_uk.iloc[-1] * (1 + last_5_years_growth_rate_uk) ** (i - 2021) for i in forecast_years_uk]

# Predict CO2 emissions for the United Kingdom for the forecasted years
X_forecast_uk = pd.DataFrame({'Year': forecast_years_uk})
y_forecast_pred_uk = rf_model_uk.predict(X_forecast_uk)

# Create DataFrames for actual, predicted, and forecasted values for the United Kingdom
actual_df_uk = pd.DataFrame({'Year': X_uk['Year'], 'Actual': y_uk})
predicted_df_uk = pd.DataFrame({'Year': X_uk['Year'], 'Predicted': y_uk_pred})
forecasted_df_uk = pd.DataFrame({'Year': forecast_years_uk, 'Forecasted': forecast_values_uk})

# Merge DataFrames on 'Year' to create a single DataFrame for the United Kingdom
result_df_uk = pd.merge(actual_df_uk, predicted_df_uk, on='Year', how='outer')
result_df_uk = pd.merge(result_df_uk, forecasted_df_uk, on='Year', how='outer')

# Plot actual, predicted, and forecasted values for the United Kingdom
plt.plot(result_df_uk['Year'], result_df_uk['Actual'], label='Actual', marker='o', color='blue')
plt.plot(result_df_uk['Year'], result_df_uk['Predicted'], label='Predicted', marker='o', linestyle='dashed', color='green')
plt.plot(result_df_uk['Year'], result_df_uk['Forecasted'], label='Forecasted', marker='o', linestyle='dashed', color='orange')
plt.xlabel('Year')
plt.ylabel('CO2 Emission (kt)')
plt.title('United Kingdom - Actual, Predicted, and Forecasted CO2 Emissions')
plt.legend()
plt.show()

uk_data = df[df['Entity'] == 'United Kingdom']

# Prepare features and target variable for the United Kingdom
X_uk = uk_data[['Year']]
y_uk = uk_data['Value_co2_emissions_kt_by_country']

best_hyperparameters_uk = {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 40}

# Create and fit the RandomForestRegressor model for the United Kingdom with hyperparameters
rf_model_uk = RandomForestRegressor(**best_hyperparameters_uk)
rf_model_uk.fit(X_uk, y_uk)

# Predict CO2 emissions for the United Kingdom
y_uk_pred = rf_model_uk.predict(X_uk)

# Calculate the growth over the last 10 years for the United Kingdom
last_10_years_growth_rate_uk = (y_uk.iloc[-1] / y_uk.iloc[-11]) ** (1/10) - 1

# Forecast CO2 emissions for the United Kingdom for the next 10 years
forecast_years_uk = range(2021, 2026)
forecast_values_uk = [y_uk.iloc[-1] * (1 + last_10_years_growth_rate_uk) ** (i - 2021) for i in forecast_years_uk]

# Predict CO2 emissions for the United Kingdom for the forecasted years
X_forecast_uk = pd.DataFrame({'Year': forecast_years_uk})
y_forecast_pred_uk = rf_model_uk.predict(X_forecast_uk)

# Create DataFrames for actual, predicted, and forecasted values for the United Kingdom
actual_df_uk = pd.DataFrame({'Year': X_uk['Year'], 'Actual': y_uk})
predicted_df_uk = pd.DataFrame({'Year': X_uk['Year'], 'Predicted': y_uk_pred})
forecasted_df_uk = pd.DataFrame({'Year': forecast_years_uk, 'Forecasted': forecast_values_uk})

# Merge DataFrames on 'Year' to create a single DataFrame for the United Kingdom
result_df_uk = pd.merge(actual_df_uk, predicted_df_uk, on='Year', how='outer')
result_df_uk = pd.merge(result_df_uk, forecasted_df_uk, on='Year', how='outer')

# Plot actual, predicted, and forecasted values for the United Kingdom
plt.plot(result_df_uk['Year'], result_df_uk['Actual'], label='Actual', marker='o', color='blue')
plt.plot(result_df_uk['Year'], result_df_uk['Predicted'], label='Predicted', marker='o', linestyle='dashed', color='green')
plt.plot(result_df_uk['Year'], result_df_uk['Forecasted'], label='Forecasted', marker='o', linestyle='dashed', color='orange')
plt.xlabel('Year')
plt.ylabel('CO2 Emission (kt)')
plt.title('United Kingdom - Actual, Predicted, and Forecasted CO2 Emissions')
plt.legend()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from math import sqrt

# Assuming 'df' has columns 'Year' and 'Value_co2_emissions_kt_by_country'
# Make sure your 'Year' column is in integer format

# Filter data for the United Kingdom
uk_data = df[df['Entity'] == 'United Kingdom']

# Prepare features and target variable for historical data in the United Kingdom
X_hist_uk = uk_data[['Year']]
y_hist_uk = uk_data['Value_co2_emissions_kt_by_country']

# Create and fit the model with the specified hyperparameters for the United Kingdom
best_model_hist_uk = RandomForestRegressor(
    n_estimators=100,
    min_samples_split=2,
    min_samples_leaf=1,
    max_features='sqrt',
    max_depth=40
)
best_model_hist_uk.fit(X_hist_uk, y_hist_uk)

# Predict CO2 emissions for historical data in the United Kingdom
y_hist_pred_uk = best_model_hist_uk.predict(X_hist_uk)

# Calculate the 10-year growth rate for the United Kingdom
last_10_years_growth_rate_uk = (y_hist_uk.iloc[-1] / y_hist_uk.iloc[-11]) ** (1/10) - 1

# Forecast CO2 emissions for the next 5 years in the United Kingdom with the growth rate
forecast_years_uk = range(2021, 2026)
forecast_values_uk = [y_hist_uk.iloc[-1] * (1 + last_10_years_growth_rate_uk) ** (i - 2020) for i in forecast_years_uk]

# Predict CO2 emissions for the forecasted years in the United Kingdom
X_forecast_uk = pd.DataFrame({'Year': forecast_years_uk})
y_forecast_pred_uk = best_model_hist_uk.predict(X_forecast_uk)

# Get the standard deviation of predictions for the United Kingdom
prediction_std_uk = sqrt(mean_squared_error(y_hist_uk, y_hist_pred_uk))

# Set the confidence level (e.g., 95% confidence interval) for the United Kingdom
confidence_level_uk = 0.95

# Calculate the margin of error for the forecasted values in the United Kingdom
margin_of_error_forecast_uk = prediction_std_uk * 1.96  # for a 95% confidence interval (z-score of 1.96)

# Create upper and lower bounds for the forecasted values in the United Kingdom
upper_bound_forecast_uk = [val + margin_of_error_forecast_uk for val in forecast_values_uk]
lower_bound_forecast_uk = [val - margin_of_error_forecast_uk for val in forecast_values_uk]

# Create DataFrames for actual, predicted, and forecasted values with bounds for the United Kingdom
actual_df_uk = pd.DataFrame({'Year': X_hist_uk['Year'], 'Actual': y_hist_uk})
predicted_df_uk = pd.DataFrame({'Year': X_hist_uk['Year'], 'Predicted': y_hist_pred_uk})
forecasted_df_uk = pd.DataFrame({
    'Year': forecast_years_uk,
    'Forecasted': forecast_values_uk,
    'Upper Bound': upper_bound_forecast_uk,
    'Lower Bound': lower_bound_forecast_uk
})

# Merge DataFrames on 'Year' to create a single DataFrame for the United Kingdom
result_df_uk = pd.merge(actual_df_uk, predicted_df_uk, on='Year', how='outer')
result_df_uk = pd.merge(result_df_uk, forecasted_df_uk, on='Year', how='outer')

# Plot actual, predicted, and forecasted values with bounds for the United Kingdom
plt.plot(result_df_uk['Year'], result_df_uk['Actual'], label='Actual', marker='o', color='blue')
plt.plot(result_df_uk['Year'], result_df_uk['Predicted'], label='Predicted', marker='o', linestyle='dashed', color='green')
plt.plot(result_df_uk['Year'], result_df_uk['Forecasted'], label='Forecasted', marker='o', linestyle='dashed', color='orange')

# Create a funnel-shaped confidence interval for the United Kingdom
plt.fill_between(result_df_uk['Year'], result_df_uk['Lower Bound'], result_df_uk['Upper Bound'], color='purple', alpha=0.5, label='95% Confidence Interval')

plt.xlabel('Year')
plt.ylabel('CO2 Emission (kt)')
plt.title('United Kingdom - Actual, Predicted, and Forecasted CO2 Emissions with Confidence Interval')
plt.legend()
plt.show()











"""## **Sarimax**"""

1

import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.holtwinters import Holt
from sklearn.metrics import mean_squared_error
from math import sqrt

# Assuming 'df' has columns 'Year' and 'Value_co2_emissions_kt_by_country'
# Make sure your 'Year' column is in integer format

# Aggregate data for world total CO2 emissions
world_data = df.groupby('Year')['Value_co2_emissions_kt_by_country'].sum().reset_index()

# Filter data for years up to 2020
historical_data = world_data[world_data['Year'] <= 2020]

# Prepare features and target variable for historical data
X_hist = historical_data[['Year']]
y_hist = historical_data['Value_co2_emissions_kt_by_country']

# Create and fit the Holt model
holt_model = Holt(y_hist, initialization_method="estimated")
holt_result = holt_model.fit()

# Forecast CO2 emissions for the next 5 years
forecast_years = range(2021, 2026)
y_forecast_pred = holt_result.forecast(steps=len(forecast_years))

# Plot actual, predicted, and forecasted values
plt.plot(historical_data['Year'], y_hist, label='Actual', marker='o', color='blue')
plt.plot(historical_data['Year'], holt_result.fittedvalues, label='Fitted', linestyle='dashed', color='green')
plt.plot(forecast_years, y_forecast_pred, label='Forecasted', linestyle='dashed', color='orange')
plt.xlabel('Year')
plt.ylabel('CO2 Emission (kt)')
plt.title('World - Actual, Fitted, and Forecasted CO2 Emissions (Holt)')
plt.legend()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.stattools import adfuller
from sklearn.metrics import mean_squared_error
from math import sqrt

# Assuming 'df' has columns 'Year' and 'Value_co2_emissions_kt_by_country'
# Make sure your 'Year' column is in integer format

# Aggregate data for world total CO2 emissions
world_data = df.groupby('Year')['Value_co2_emissions_kt_by_country'].sum().reset_index()

# Filter data for years up to 2020
historical_data = world_data[world_data['Year'] <= 2020]

# Prepare features and target variable for historical data
X_hist = historical_data[['Year']]
y_hist = historical_data['Value_co2_emissions_kt_by_country']

# Check for stationarity
result = adfuller(y_hist)
print('ADF Statistic:', result[0])
print('p-value:', result[1])
print('Critical Values:', result[4])

# If not stationary, perform differencing
if result[1] > 0.05:
    y_hist_diff = y_hist.diff().dropna()
else:
    y_hist_diff = y_hist.copy()

# Determine the order of the ARIMA model
# You may need to adjust these parameters based on the characteristics of your data
p, d, q = 1, 1, 1

# Fit ARIMA model
arima_model = SARIMAX(y_hist, order=(p, d, q))
arima_result = arima_model.fit()

# Forecast CO2 emissions for the next 5 years
forecast_years = range(2021, 2026)
y_forecast_pred = arima_result.get_forecast(steps=len(forecast_years)).predicted_mean

# Plot actual and forecasted values
plt.plot(historical_data['Year'], y_hist, label='Actual', marker='o', color='blue')
plt.plot(historical_data['Year'], arima_result.fittedvalues, label='Fitted', linestyle='dashed', color='green')
plt.plot(forecast_years, y_forecast_pred, label='Forecasted', linestyle='dashed', color='orange')
plt.xlabel('Year')
plt.ylabel('CO2 Emission (kt)')
plt.title('World - Actual, Fitted, and Forecasted CO2 Emissions (ARIMA)')
plt.legend()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.stattools import adfuller
from sklearn.metrics import mean_squared_error
from math import sqrt

# Assuming 'df' has columns 'Year' and 'Value_co2_emissions_kt_by_country'
# Make sure your 'Year' column is in integer format

# Aggregate data for world total CO2 emissions
world_data = df.groupby('Year')['Value_co2_emissions_kt_by_country'].sum().reset_index()

# Filter data for years up to 2020
historical_data = world_data[world_data['Year'] <= 2020]

# Prepare features and target variable for historical data
X_hist = historical_data[['Year']]
y_hist = historical_data['Value_co2_emissions_kt_by_country']

# Perform differencing to make the time series stationary
y_hist_diff = y_hist.diff().dropna()

# Check for stationarity again
result_diff = adfuller(y_hist_diff)
print('ADF Statistic (after differencing):', result_diff[0])
print('p-value:', result_diff[1])
print('Critical Values:', result_diff[4])

# Determine the order of the ARIMA model
# You may need to adjust these parameters based on the characteristics of your data
p, d, q = 1, 1, 1

# Fit ARIMA model with enforce_stationarity=False
arima_model = SARIMAX(y_hist, order=(p, d, q), enforce_stationarity=False)
arima_result = arima_model.fit()

# Forecast CO2 emissions for the next 5 years
forecast_years = range(2021, 2026)
y_forecast_pred = arima_result.get_forecast(steps=len(forecast_years)).predicted_mean

# Plot actual and forecasted values
plt.plot(historical_data['Year'], y_hist, label='Actual', marker='o', color='blue')
plt.plot(historical_data['Year'], arima_result.fittedvalues, label='Fitted', linestyle='dashed', color='green')
plt.plot(forecast_years, y_forecast_pred, label='Forecasted', linestyle='dashed', color='orange')
plt.xlabel('Year')
plt.ylabel('CO2 Emission (kt)')
plt.title('World - Actual, Fitted, and Forecasted CO2 Emissions (ARIMA)')
plt.legend()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.seasonal import seasonal_decompose

# Assuming 'df' has columns 'Year' and 'Value_co2_emissions_kt_by_country'
# Make sure your 'Year' column is in integer format

# Aggregate data for world total CO2 emissions
world_data = df.groupby('Year')['Value_co2_emissions_kt_by_country'].sum().reset_index()

# Filter data for years up to 2020
historical_data = world_data[world_data['Year'] <= 2020]

# Prepare features and target variable for historical data
X_hist = historical_data[['Year']]
y_hist = historical_data['Value_co2_emissions_kt_by_country']

# Create a time series decomposition object
result = seasonal_decompose(y_hist, model='additive', period=1)

# Plot the original time series, trend, seasonality, and residuals
plt.figure(figsize=(12, 8))
plt.subplot(4, 1, 1)
plt.plot(X_hist['Year'], y_hist, label='Original')
plt.legend(loc='upper left')
plt.title('Original Time Series')

plt.subplot(4, 1, 2)
plt.plot(X_hist['Year'], result.trend, label='Trend')
plt.legend(loc='upper left')
plt.title('Trend Component')

plt.subplot(4, 1, 3)
plt.plot(X_hist['Year'], result.seasonal, label='Seasonal')
plt.legend(loc='upper left')
plt.title('Seasonal Component')

plt.subplot(4, 1, 4)
plt.plot(X_hist['Year'], result.resid, label='Residuals')
plt.legend(loc='upper left')
plt.title('Residuals')

plt.tight_layout()
plt.show()

# Plot ACF and PACF
plt.figure(figsize=(12, 4))

# ACF plot
plt.subplot(1, 2, 1)
plot_acf(y_hist, lags=20, ax=plt.gca())
plt.title('Autocorrelation Function (ACF)')

# PACF plot
plt.subplot(1, 2, 2)
plot_pacf(y_hist, lags=9, ax=plt.gca())
plt.title('Partial Autocorrelation Function (PACF)')

plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm

# Assuming 'df' has columns 'Year' and 'Value_co2_emissions_kt_by_country'
# Make sure your 'Year' column is in integer format

# Aggregate data for world total CO2 emissions
world_data = df.groupby('Year')['Value_co2_emissions_kt_by_country'].sum().reset_index()

# Filter data for years up to 2020
historical_data = world_data[world_data['Year'] <= 2020]

# Prepare features and target variable for historical data
y_hist = historical_data['Value_co2_emissions_kt_by_country']

# Define SARIMA model with initial parameters
sarima_model = sm.tsa.SARIMAX(y_hist, order=(1, 1, 0), seasonal_order=(1, 1, 0, 5))

# Fit the model
sarima_results = sarima_model.fit()

# Print model summary
print(sarima_results.summary())

# Plot diagnostic plots
sarima_results.plot_diagnostics(figsize=(15, 8))
plt.show()

# Forecast CO2 emissions for the next 5 years
forecast_years = range(2021, 2026)
forecast_values = sarima_results.get_forecast(steps=5).predicted_mean

# Plot actual, predicted, and forecasted values
plt.plot(historical_data['Year'], y_hist, label='Actual', marker='o', color='blue')
plt.plot(historical_data['Year'], sarima_results.fittedvalues, label='Fitted', marker='o', linestyle='dashed', color='green')
plt.plot(forecast_years, forecast_values, label='Forecasted', marker='o', linestyle='dashed', color='orange')
plt.xlabel('Year')
plt.ylabel('CO2 Emission (kt)')
plt.title('World - Actual, Fitted, and Forecasted CO2 Emissions')
plt.legend()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm

# Assuming 'df' has columns 'Year' and 'Value_co2_emissions_kt_by_country'
# Make sure your 'Year' column is in integer format

# Aggregate data for world total CO2 emissions
world_data = df.groupby('Year')['Value_co2_emissions_kt_by_country'].sum().reset_index()

# Filter data for years up to 2020
historical_data = world_data[world_data['Year'] <= 2020]

# Prepare features and target variable for historical data
y_hist = historical_data['Value_co2_emissions_kt_by_country']

# Define SARIMA model with initial parameters
sarima_model = sm.tsa.SARIMAX(y_hist, order=(1, 1, 0), seasonal_order=(1, 1, 0, 5))

# Fit the model
sarima_results = sarima_model.fit()

# Print model summary
print(sarima_results.summary())

# Plot diagnostic plots
sarima_results.plot_diagnostics(figsize=(15, 8))
plt.show()

# Forecast CO2 emissions for the next 5 years
forecast_years = range(2021, 2026)
forecast_values = sarima_results.get_forecast(steps=5).predicted_mean

# Plot actual, fitted, and forecasted values
plt.plot(historical_data['Year'], y_hist, label='Actual', marker='o', color='blue')
plt.plot(historical_data['Year'], sarima_results.fittedvalues, label='Fitted', marker='o', linestyle='dashed', color='green')
plt.plot(forecast_years, forecast_values, label='Forecasted', marker='o', linestyle='dashed', color='orange')

# Plot predicted values for historical data
plt.plot(historical_data['Year'], sarima_results.predict(), label='Predicted', marker='o', linestyle='dashed', color='red')

plt.xlabel('Year')
plt.ylabel('CO2 Emission (kt)')
plt.title('World - Actual, Fitted, Predicted, and Forecasted CO2 Emissions')
plt.legend()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
from sklearn.metrics import mean_squared_error, mean_absolute_error

# Assuming 'world_data' is the DataFrame with 'Year' and 'Value_co2_emissions_kt_by_country'
# Filter data for years up to 2020
historical_data = world_data[world_data['Year'] <= 2020]

# Prepare the target variable for historical data
y_hist = historical_data['Value_co2_emissions_kt_by_country']

# SARIMA model
sarima_model = sm.tsa.SARIMAX(
    y_hist, order=(1, 1, 0), seasonal_order=(1, 1, 0, 5),
    initialization_method='approximate_diffuse'
)

# Fit the SARIMA model
sarima_results = sarima_model.fit()

# Predict CO2 emissions for historical data
y_hist_pred = sarima_results.predict()

# Forecast CO2 emissions for the next 5 years
forecast_years = range(2021, 2026)
y_forecast_pred = sarima_results.get_forecast(steps=len(forecast_years)).predicted_mean

# Create DataFrames for actual, predicted, and forecasted values
actual_df = pd.DataFrame({'Year': historical_data['Year'], 'Actual': y_hist})
predicted_df = pd.DataFrame({'Year': historical_data['Year'], 'Predicted': y_hist_pred})
forecasted_df = pd.DataFrame({'Year': forecast_years, 'Forecasted': y_forecast_pred})

# Merge DataFrames on 'Year' to create a single DataFrame
result_df = pd.merge(actual_df, predicted_df, on='Year', how='outer')
result_df = pd.merge(result_df, forecasted_df, on='Year', how='outer')

# Plot actual, predicted, and forecasted values
plt.plot(result_df['Year'], result_df['Actual'], label='Actual', marker='o', color='blue')
plt.plot(result_df['Year'], result_df['Predicted'], label='Predicted', marker='o', linestyle='dashed', color='green')
plt.plot(result_df['Year'], result_df['Forecasted'], label='Forecasted', marker='o', linestyle='dashed', color='orange')
plt.xlabel('Year')
plt.ylabel('CO2 Emission (kt)')
plt.title('World - Actual, Predicted, and Forecasted CO2 Emissions')
plt.legend()
plt.show()

# Evaluation metrics
aic = sarima_results.aic
bic = sarima_results.bic
mse = mean_squared_error(y_hist, y_hist_pred)
mae = mean_absolute_error(y_hist, y_hist_pred)

print(f'AIC: {aic:.2f}')
print(f'BIC: {bic:.2f}')
print(f'Mean Squared Error (MSE): {mse:.2f}')
print(f'Mean Absolute Error (MAE): {mae:.2f}')

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from statsmodels.tsa.holtwinters import ExponentialSmoothing
from statsmodels.tsa.statespace.sarimax import SARIMAX
import numpy as np

# Assuming 'df' has columns 'Year' and 'Value_co2_emissions_kt_by_country'
# Make sure your 'Year' column is in integer format

# Aggregate data for world total CO2 emissions
world_data = df.groupby('Year')['Value_co2_emissions_kt_by_country'].sum().reset_index()

# Filter data for years up to 2020
historical_data = world_data[world_data['Year'] <= 2020]

# Prepare features and target variable for historical data
X_hist = historical_data[['Year']]
y_hist = historical_data['Value_co2_emissions_kt_by_country']

# Create and fit the model with the specified hyperparameters
best_model_hist = RandomForestRegressor(
    n_estimators=100,
    min_samples_split=2,
    min_samples_leaf=1,
    max_features='auto',
    max_depth=50
)
best_model_hist.fit(X_hist, y_hist)

# Predict CO2 emissions for historical data
y_hist_pred = best_model_hist.predict(X_hist)

# Calculate the growth over the last 3 years
last_3_years_growth_rate = (y_hist.iloc[-1] / y_hist.iloc[-4]) ** (1/3) - 1

# Forecast CO2 emissions for the next 5 years
forecast_years = range(2021, 2026)
forecast_values = [y_hist.iloc[-1] * (1 + last_3_years_growth_rate) ** (i - 2021) for i in forecast_years]

# Predict CO2 emissions for the forecasted years
X_forecast = pd.DataFrame({'Year': forecast_years})
y_forecast_pred = best_model_hist.predict(X_forecast)

# Calculate RMSE for historical data
rmse = sqrt(mean_squared_error(y_hist, y_hist_pred))

# Calculate RMSE as a percentage of the mean of the actual values
rmse_percentage = (rmse / np.mean(y_hist)) * 100

# Create DataFrames for actual, predicted, and forecasted values
actual_df = pd.DataFrame({'Year': X_hist['Year'], 'Actual': y_hist})
predicted_df = pd.DataFrame({'Year': X_hist['Year'], 'Predicted': y_hist_pred})
forecasted_df = pd.DataFrame({'Year': forecast_years, 'Forecasted': forecast_values})

# Merge DataFrames on 'Year' to create a single DataFrame
result_df = pd.merge(actual_df, predicted_df, on='Year', how='outer')
result_df = pd.merge(result_df, forecasted_df, on='Year', how='outer')

# Plot actual, predicted, and forecasted values
plt.plot(result_df['Year'], result_df['Actual'], label='Actual', marker='o', color='blue')
plt.plot(result_df['Year'], result_df['Predicted'], label='Predicted', marker='o', linestyle='dashed', color='green')
plt.plot(result_df['Year'], result_df['Forecasted'], label='Forecasted', marker='o', linestyle='dashed', color='orange')
plt.xlabel('Year')
plt.ylabel('CO2 Emission (kt)')
plt.title('World - Actual, Predicted, and Forecasted CO2 Emissions')
plt.legend()
plt.show()

# Display RMSE and RMSE as a percentage
print(f'Root Mean Squared Error (RMSE): {rmse:.2f}')
print(f'Root Mean Squared Error (RMSE) as a percentage of the mean of actual values: {rmse_percentage:.2f}%')



df['Year'] = df.index.year  # Assuming the 'Year' was set as the index

# If 'Year' was not the index, you can use the following:
# df['Year'] = df.index
# Or if 'Year' is a column in your DataFrame, you can use:
# df['Year'] = df['Year'].dt.year

df.reset_index(drop=True, inplace=True)

df

import pandas as pd
import numpy as np
from statsmodels.tsa.statespace.sarimax import SARIMAX
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error
from math import sqrt

# Assuming 'df' is your DataFrame with the relevant columns and 'Year' as the index

# Select features and target variable
features = ['Electricity from fossil fuels (TWh)', 'Electricity from renewables (TWh)', 'Electricity from nuclear (TWh)']
target_variable = 'Value_co2_emissions_kt_by_country'

# Create exogenous variable X
X = df[features]

# Create endogenous variable y
y = df[target_variable]

# Initialize standard scaler
scaler = StandardScaler()

# Standardize the data
X_scaled = scaler.fit_transform(X)

# Set up TimeSeriesSplit for cross-validation
tscv = TimeSeriesSplit(n_splits=5)

# Initialize arrays to store evaluation metrics
mae_scores = []
rmse_scores = []
rmse_percentage_scores = []
mape_scores = []

# Loop through each fold in TimeSeriesSplit
for train_index, test_index in tscv.split(X_scaled):
    X_train, X_test = X_scaled[train_index], X_scaled[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    # Fit SARIMAX model
    sarimax_model = SARIMAX(endog=y_train, exog=X_train, order=(1, 1, 1), seasonal_order=(0, 1, 1, 12))
    sarimax_fit = sarimax_model.fit()

    # Make predictions
    y_pred = sarimax_fit.get_forecast(steps=len(X_test), exog=X_test)
    y_pred_mean = y_pred.predicted_mean

    # Calculate evaluation metrics
    mae = mean_absolute_error(y_test, y_pred_mean)
    rmse = sqrt(mean_squared_error(y_test, y_pred_mean))
    rmse_percentage = rmse / y.mean() * 100  # RMSE as a percentage of the mean
    mape = np.mean(np.abs((y_test - y_pred_mean) / y_test)) * 100

    # Append scores to arrays
    mae_scores.append(mae)
    rmse_scores.append(rmse)
    rmse_percentage_scores.append(rmse_percentage)
    mape_scores.append(mape)

# Calculate mean scores across folds
mean_mae = np.mean(mae_scores)
mean_rmse = np.mean(rmse_scores)
mean_rmse_percentage = np.mean(rmse_percentage_scores)
mean_mape = np.mean(mape_scores)

# Print mean scores
print(f'Mean MAE: {mean_mae}')
print(f'Mean RMSE: {mean_rmse}')
print(f'Mean RMSE (% of mean): {mean_rmse_percentage}')
print(f'Mean MAPE: {mean_mape}')

import pandas as pd
import numpy as np
from statsmodels.tsa.statespace.sarimax import SARIMAX
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error

# Assuming 'df' is your DataFrame with the relevant columns and 'Year' as the index

# Select features and target variable
features = ['Electricity from fossil fuels (TWh)', 'Electricity from renewables (TWh)', 'Electricity from nuclear (TWh)']
target_variable = 'Value_co2_emissions_kt_by_country'

# Create exogenous variable X
X = df[features]

# Create endogenous variable y
y = df[target_variable]

# Set up TimeSeriesSplit for cross-validation
tscv = TimeSeriesSplit(n_splits=5)

# Define hyperparameter grid
p_values = range(0, 3)
d_values = range(0, 2)
q_values = range(0, 3)
P_values = range(0, 3)
D_values = range(0, 2)
Q_values = range(0, 3)
s_values = [12]  # Seasonal period (assuming monthly data)

# Initialize arrays to store evaluation metrics
best_params = None
best_rmse = float('inf')

# Loop through hyperparameter combinations
for p in p_values:
    for d in d_values:
        for q in q_values:
            for P in P_values:
                for D in D_values:
                    for Q in Q_values:
                        for s in s_values:
                            rmse_scores = []  # Store RMSE for each fold

                            # Loop through each fold in TimeSeriesSplit
                            for train_index, test_index in tscv.split(X):
                                X_train, X_test = X.iloc[train_index], X.iloc[test_index]
                                y_train, y_test = y.iloc[train_index], y.iloc[test_index]

                                # Fit SARIMAX model
                                sarimax_model = SARIMAX(endog=y_train, exog=X_train, order=(p, d, q), seasonal_order=(P, D, Q, s))
                                sarimax_fit = sarimax_model.fit()

                                # Make predictions
                                y_pred = sarimax_fit.get_forecast(steps=len(X_test), exog=X_test)
                                y_pred_mean = y_pred.predicted_mean

                                # Calculate RMSE for the current fold
                                rmse = np.sqrt(mean_squared_error(y_test, y_pred_mean))
                                rmse_scores.append(rmse)

                            # Calculate average RMSE across folds
                            avg_rmse = np.mean(rmse_scores)

                            # Update best hyperparameters if current combination performs better
                            if avg_rmse < best_rmse:
                                best_rmse = avg_rmse
                                best_params = {'p': p, 'd': d, 'q': q, 'P': P, 'D': D, 'Q': Q, 's': s}

# Print the best hyperparameters
print("Best Hyperparameters:", best_params)
print("Best RMSE:", best_rmse)